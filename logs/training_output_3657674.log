Training set erstellt: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251103_1640.csv (1955 Patienten)
Test set erstellt: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/test_metadataHC_0.720251103_1640.csv (2213 Patienten, davon 838 HC und 1375 andere)
Only one CUDA device found. Using cuda:0

####################################################################################################

Starting Catatonia CPA training session NormativeVAE20_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_20251103_1640_HC_columnwise at 2025-11-03_18-40
----------------------------------------------------------------------------------------------------------------------------------------------------------------

Using Device:        cuda:0
Queued Epochs:       250
Batch Size:          32
Data Summary:        ['/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251103_1640.csv']
MRI Data Directory:  /net/data.isilon/ag-cherrmann/lduttenhoefer/project/CAT12_newvals/QC/CAT12_results_final.csv
Loading Model:       False
Output Directory:    /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640

More details in log file: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/logs/2025-11-03_18-40_NormativeVAE20_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_20251103_1640_HC_columnwise_training_log_.txt

####################################################################################################
Starting normative modeling with atlas: lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux, epochs: 250, bootstraps: 80
Normalization method: columnwise
Configuration saved to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/config.csv
Using device: cuda
Loading NORM control data...
================================================================================
[INFO] Loading PRE-NORMALIZED MRI data
================================================================================
[INFO] Selected atlases: ['lpba40', 'neuromorphometrics', 'ibsr', 'aparc_dk40', 'aparc_destrieux']
[INFO] Target volume types: ['Vgm', 'G', 'T']
[INFO] Loading metadata from: ['/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251103_1640.csv']
[INFO] Loaded metadata with 1955 subjects
[INFO] Filtered to 1955 subjects with diagnoses: ['HC']
[INFO] Loading pre-normalized MRI data from: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/CAT12_results_NORMALIZED_columnwise_HC_separate_TRAIN.csv
[INFO] Loaded MRI data with shape: (1954, 1211)
[INFO] Found 1194 total ROI columns in MRI data
[INFO] Filtering columns by selected atlases and volume types...
[INFO] Processing atlas: lpba40 (prefix: lpba40)
[INFO] Found 56 columns for atlas lpba40
[INFO] Processing atlas: neuromorphometrics (prefix: Neurom)
[INFO] Found 123 columns for atlas neuromorphometrics
[INFO] Processing atlas: ibsr (prefix: IBSR)
[INFO] Found 32 columns for atlas ibsr
[INFO] Processing atlas: aparc_dk40 (prefix: DK40)
[INFO] Found 137 columns for atlas aparc_dk40
[INFO] Processing atlas: aparc_destrieux (prefix: Destrieux)
[INFO] Found 296 columns for atlas aparc_destrieux
[INFO] Selected 644 feature columns total after filtering
[INFO]   - Vgm: 211 features
[INFO]   - G: 216 features
[INFO]   - T: 217 features
[INFO] Matching metadata subjects with MRI data...
[INFO] Successfully matched 1954/1955 subjects
[WARNING] 1 subjects not found in MRI data
[WARNING] Unmatched: ['sub-031461_T1w']
[INFO] Total subjects processed: 1954
[INFO] Total ROI features per subject: 644
[INFO] Data loading complete!
================================================================================
[DEBUG] Data shape: torch.Size([1954, 644])
[DEBUG] Data min: -7.208443641662598
[DEBUG] Data max: 8.865362167358398
[DEBUG] Data mean: -1.9401245887618046e-10
[DEBUG] Data std: 0.9997444748878479
[DEBUG] Has NaN: False
[DEBUG] Has Inf: False
[DEBUG] Sample values: tensor([0.3622, 0.4168, 0.2031, 0.1700, 0.1494, 1.0642, 1.0130, 0.6701, 0.8466,
        0.3796])
Number of ROIs in atlas: 644
[INFO] 1560 subjects in training set
[INFO] 390 subjects in validation set
[WARNING] 4 subjects not found in annotations:
  - sub-031704_T1w
  - sub-NDARINVEY033HCZ_run01_T1w
  - sub-031705_T1w
  - sub-NDARINVKC627BAV_run01_T1w
                        Filename Data_Type  ... NSS_Motor NSS_Total
0                       sub-0001     train  ...       NaN       NaN
1                       sub-0004     train  ...       NaN       NaN
2                       sub-0011     train  ...       NaN       NaN
3                       sub-0013     train  ...       NaN       NaN
4                       sub-0016     train  ...       NaN       NaN
...                          ...       ...  ...       ...       ...
1946           sub-NM5946_0_MPR1     valid  ...       NaN       NaN
1947           sub-NM7365_0_MPR1     valid  ...       NaN       NaN
1948           sub-NM8356_0_MPR1     valid  ...       NaN       NaN
1949  sub-whiteCAT191_ses-01_T1w     valid  ...       0.0       3.0
1950  sub-whiteCAT200_ses-01_T1w     valid  ...       1.0       9.0

[1951 rows x 21 columns]
Using atlas: ['lpba40', 'neuromorphometrics', 'ibsr', 'aparc_dk40', 'aparc_destrieux']
Number of ROIs: 644
---------------
Data Processing
---------------
Loading Data
  Training Data:       1560 subjects loaded
  Validation Data:      390 subjects loaded

Creating Model
--------------
Training data shape: torch.Size([1560, 644])
Validation data shape: torch.Size([390, 644])
/net/data.isilon/ag-cherrmann/lduttenhoefer/project/miniconda3/envs/LISA_ba_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/src/models/ContrastVAE_2D.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler()
Model Archtecture: 
NormativeVAE_2D(
  (encoder): Sequential(
    (0): Linear(in_features=644, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=100, out_features=100, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=100, out_features=20, bias=True)
  )
  (fc_mu): Linear(in_features=20, out_features=20, bias=True)
  (fc_var): Linear(in_features=20, out_features=20, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=20, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=100, out_features=100, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=100, out_features=644, bias=True)
  )
)

    latent_dim:          20
    optimizer:           Adam (
    scheduler:           <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2ab1ac23c450>
    scaler:              <torch.cuda.amp.grad_scaler.GradScaler object at 0x2ab1af7b95d0>
    recon_loss_weight:   16.6449
    kldiv_loss_weight:   1.2
    contr_loss_weight:   0.0
    schedule_on_validation: True
    scheduler_patience:  10
    scheduler_factor:    0.5
    learning_rate:       0.000559
    weight_decay:        1e-05
    dropout_prob:        0.1
    device:              cuda
    Total Parameters:    154,704
    Trainable Params:    154,704
Training baseline model before bootstrap training...
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9466, Val Loss: 0.7143, Recon: 0.9466, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7031, Val Loss: 4.6497, Recon: 0.8276, KL: 3.8755, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4640, Val Loss: 9.4163, Recon: 0.8047, KL: 8.6592, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2321, Val Loss: 14.2023, Recon: 0.7805, KL: 13.4516, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0376, Val Loss: 18.9900, Recon: 0.7848, KL: 18.2528, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8385, Val Loss: 23.7942, Recon: 0.7775, KL: 23.0610, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6170, Val Loss: 28.5874, Recon: 0.7644, KL: 27.8526, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4015, Val Loss: 33.3825, Recon: 0.7559, KL: 32.6457, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2104, Val Loss: 38.2021, Recon: 0.7578, KL: 37.4526, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0154, Val Loss: 42.9776, Recon: 0.7580, KL: 42.2574, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8095, Val Loss: 47.7930, Recon: 0.7600, KL: 47.0495, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7631, Val Loss: 48.7263, Recon: 0.7556, KL: 48.0075, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7535, Val Loss: 48.7149, Recon: 0.7495, KL: 48.0039, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7577, Val Loss: 48.7361, Recon: 0.7522, KL: 48.0055, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7601, Val Loss: 48.7287, Recon: 0.7477, KL: 48.0124, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7551, Val Loss: 48.7278, Recon: 0.7490, KL: 48.0061, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7518, Val Loss: 48.7379, Recon: 0.7387, KL: 48.0130, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7465, Val Loss: 48.7224, Recon: 0.7379, KL: 48.0086, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7374, Val Loss: 48.7181, Recon: 0.7327, KL: 48.0048, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7420, Val Loss: 48.7088, Recon: 0.7369, KL: 48.0051, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7304, Val Loss: 48.7104, Recon: 0.7278, KL: 48.0026, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7480, Val Loss: 48.7134, Recon: 0.7434, KL: 48.0046, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7420, Val Loss: 48.7169, Recon: 0.7374, KL: 48.0047, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7268, Val Loss: 48.7038, Recon: 0.7237, KL: 48.0030, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7505, Val Loss: 48.7215, Recon: 0.7413, KL: 48.0093, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7364, Val Loss: 48.7146, Recon: 0.7310, KL: 48.0054, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7330, Val Loss: 48.7039, Recon: 0.7303, KL: 48.0027, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7319, Val Loss: 48.7115, Recon: 0.7278, KL: 48.0040, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7384, Val Loss: 48.7104, Recon: 0.7316, KL: 48.0068, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7278, Val Loss: 48.7016, Recon: 0.7252, KL: 48.0026, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7225, Val Loss: 48.7106, Recon: 0.7189, KL: 48.0036, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7198, Val Loss: 48.7092, Recon: 0.7168, KL: 48.0030, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7402, Val Loss: 48.7236, Recon: 0.7299, KL: 48.0102, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7293, Val Loss: 48.7066, Recon: 0.7267, KL: 48.0026, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7178, Val Loss: 48.7006, Recon: 0.7161, KL: 48.0017, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7231, Val Loss: 48.7009, Recon: 0.7168, KL: 48.0063, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7351, Val Loss: 48.7007, Recon: 0.7327, KL: 48.0024, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7170, Val Loss: 48.6949, Recon: 0.7136, KL: 48.0034, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7217, Val Loss: 48.7094, Recon: 0.7168, KL: 48.0049, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7236, Val Loss: 48.7061, Recon: 0.7167, KL: 48.0069, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7215, Val Loss: 48.7075, Recon: 0.7093, KL: 48.0121, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7091, Val Loss: 48.6889, Recon: 0.7065, KL: 48.0027, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7051, Val Loss: 48.6889, Recon: 0.7018, KL: 48.0033, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7034, Val Loss: 48.6729, Recon: 0.6997, KL: 48.0037, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7127, Val Loss: 48.6862, Recon: 0.7082, KL: 48.0045, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7006, Val Loss: 48.6794, Recon: 0.6982, KL: 48.0024, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7162, Val Loss: 48.6996, Recon: 0.7101, KL: 48.0061, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6960, Val Loss: 48.6802, Recon: 0.6925, KL: 48.0036, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6933, Val Loss: 48.6815, Recon: 0.6899, KL: 48.0034, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6978, Val Loss: 48.6787, Recon: 0.6944, KL: 48.0034, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6951, Val Loss: 48.6766, Recon: 0.6905, KL: 48.0046, KL_weight: 4.8000
Training bootstrap models...
Starting bootstrap training with 80 iterations
KL warmup will occur over first 50 epochs
Training bootstrap model 1/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9565, Val Loss: 0.7663, Recon: 0.9565, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6770, Val Loss: 4.6462, Recon: 0.8114, KL: 3.8656, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4381, Val Loss: 9.4233, Recon: 0.7830, KL: 8.6550, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2320, Val Loss: 14.2149, Recon: 0.7744, KL: 13.4576, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0080, Val Loss: 18.9887, Recon: 0.7611, KL: 18.2469, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8197, Val Loss: 23.7959, Recon: 0.7589, KL: 23.0607, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6084, Val Loss: 28.5670, Recon: 0.7599, KL: 27.8484, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4060, Val Loss: 33.3824, Recon: 0.7514, KL: 32.6546, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1906, Val Loss: 38.1693, Recon: 0.7441, KL: 37.4465, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9900, Val Loss: 42.9911, Recon: 0.7466, KL: 42.2433, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7808, Val Loss: 47.7675, Recon: 0.7349, KL: 47.0459, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7403, Val Loss: 48.7318, Recon: 0.7303, KL: 48.0100, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7322, Val Loss: 48.7406, Recon: 0.7279, KL: 48.0043, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7442, Val Loss: 48.7194, Recon: 0.7366, KL: 48.0077, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7332, Val Loss: 48.7335, Recon: 0.7282, KL: 48.0051, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7323, Val Loss: 48.7274, Recon: 0.7252, KL: 48.0072, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7268, Val Loss: 48.7250, Recon: 0.7219, KL: 48.0049, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7243, Val Loss: 48.7247, Recon: 0.7195, KL: 48.0047, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7284, Val Loss: 48.7284, Recon: 0.7227, KL: 48.0057, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7400, Val Loss: 48.7253, Recon: 0.7257, KL: 48.0143, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7174, Val Loss: 48.7238, Recon: 0.7153, KL: 48.0021, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7282, Val Loss: 48.7126, Recon: 0.7229, KL: 48.0053, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7255, Val Loss: 48.7200, Recon: 0.7221, KL: 48.0034, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7255, Val Loss: 48.7275, Recon: 0.7188, KL: 48.0067, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7333, Val Loss: 48.7311, Recon: 0.7237, KL: 48.0097, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7175, Val Loss: 48.7176, Recon: 0.7151, KL: 48.0024, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7125, Val Loss: 48.7178, Recon: 0.7089, KL: 48.0036, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7111, Val Loss: 48.7063, Recon: 0.7087, KL: 48.0024, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7211, Val Loss: 48.7082, Recon: 0.7196, KL: 48.0014, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7072, Val Loss: 48.7041, Recon: 0.7045, KL: 48.0027, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7401, Val Loss: 48.7313, Recon: 0.7289, KL: 48.0113, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7170, Val Loss: 48.7112, Recon: 0.7146, KL: 48.0024, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7194, Val Loss: 48.7173, Recon: 0.7150, KL: 48.0044, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7197, Val Loss: 48.7156, Recon: 0.7182, KL: 48.0015, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7125, Val Loss: 48.7058, Recon: 0.7098, KL: 48.0027, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7134, Val Loss: 48.7093, Recon: 0.7101, KL: 48.0033, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7062, Val Loss: 48.7053, Recon: 0.7014, KL: 48.0047, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7098, Val Loss: 48.7022, Recon: 0.7062, KL: 48.0036, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7087, Val Loss: 48.6985, Recon: 0.7061, KL: 48.0027, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7169, Val Loss: 48.7051, Recon: 0.7116, KL: 48.0052, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7096, Val Loss: 48.7064, Recon: 0.7039, KL: 48.0056, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7059, Val Loss: 48.7141, Recon: 0.6992, KL: 48.0067, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7155, Val Loss: 48.7083, Recon: 0.7098, KL: 48.0057, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7066, Val Loss: 48.7052, Recon: 0.7039, KL: 48.0026, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7060, Val Loss: 48.7014, Recon: 0.7043, KL: 48.0017, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7050, Val Loss: 48.6941, Recon: 0.7023, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7048, Val Loss: 48.7040, Recon: 0.7031, KL: 48.0017, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7057, Val Loss: 48.7047, Recon: 0.7015, KL: 48.0042, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7073, Val Loss: 48.7046, Recon: 0.7043, KL: 48.0030, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7030, Val Loss: 48.7095, Recon: 0.7002, KL: 48.0029, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7051, Val Loss: 48.7001, Recon: 0.7014, KL: 48.0037, KL_weight: 4.8000
Saved model 1 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_0.pt
Training bootstrap model 2/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9845, Val Loss: 0.7204, Recon: 0.9845, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7136, Val Loss: 4.6396, Recon: 0.8438, KL: 3.8698, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4656, Val Loss: 9.4070, Recon: 0.8080, KL: 8.6576, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2496, Val Loss: 14.1985, Recon: 0.7911, KL: 13.4584, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0580, Val Loss: 18.9870, Recon: 0.7938, KL: 18.2643, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8248, Val Loss: 23.7809, Recon: 0.7766, KL: 23.0482, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6256, Val Loss: 28.5948, Recon: 0.7747, KL: 27.8510, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4296, Val Loss: 33.3717, Recon: 0.7750, KL: 32.6546, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2091, Val Loss: 38.1721, Recon: 0.7595, KL: 37.4496, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0286, Val Loss: 42.9670, Recon: 0.7785, KL: 42.2502, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8066, Val Loss: 47.7521, Recon: 0.7588, KL: 47.0479, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7673, Val Loss: 48.7244, Recon: 0.7609, KL: 48.0065, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7641, Val Loss: 48.7253, Recon: 0.7539, KL: 48.0103, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7514, Val Loss: 48.7081, Recon: 0.7467, KL: 48.0047, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7459, Val Loss: 48.7179, Recon: 0.7435, KL: 48.0024, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7677, Val Loss: 48.7165, Recon: 0.7549, KL: 48.0128, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7399, Val Loss: 48.7060, Recon: 0.7365, KL: 48.0034, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7683, Val Loss: 48.7190, Recon: 0.7591, KL: 48.0092, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7465, Val Loss: 48.7200, Recon: 0.7390, KL: 48.0075, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7431, Val Loss: 48.7300, Recon: 0.7352, KL: 48.0078, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7302, Val Loss: 48.7089, Recon: 0.7255, KL: 48.0047, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7226, Val Loss: 48.7116, Recon: 0.7167, KL: 48.0059, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7513, Val Loss: 48.7074, Recon: 0.7447, KL: 48.0066, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7244, Val Loss: 48.7152, Recon: 0.7206, KL: 48.0038, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7242, Val Loss: 48.7170, Recon: 0.7207, KL: 48.0035, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7183, Val Loss: 48.6894, Recon: 0.7134, KL: 48.0049, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7211, Val Loss: 48.7134, Recon: 0.7149, KL: 48.0062, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7189, Val Loss: 48.7009, Recon: 0.7160, KL: 48.0029, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7079, Val Loss: 48.6801, Recon: 0.7027, KL: 48.0052, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7086, Val Loss: 48.6906, Recon: 0.7025, KL: 48.0061, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7089, Val Loss: 48.6939, Recon: 0.7051, KL: 48.0039, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7106, Val Loss: 48.6935, Recon: 0.7034, KL: 48.0071, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6992, Val Loss: 48.6899, Recon: 0.6959, KL: 48.0033, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6987, Val Loss: 48.6870, Recon: 0.6950, KL: 48.0037, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7042, Val Loss: 48.6898, Recon: 0.7006, KL: 48.0036, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6967, Val Loss: 48.6842, Recon: 0.6919, KL: 48.0048, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6978, Val Loss: 48.6907, Recon: 0.6935, KL: 48.0043, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7133, Val Loss: 48.7120, Recon: 0.7042, KL: 48.0091, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7018, Val Loss: 48.6850, Recon: 0.6984, KL: 48.0035, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6982, Val Loss: 48.6741, Recon: 0.6931, KL: 48.0050, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6917, Val Loss: 48.6961, Recon: 0.6880, KL: 48.0036, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6982, Val Loss: 48.6812, Recon: 0.6943, KL: 48.0039, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7008, Val Loss: 48.6870, Recon: 0.6950, KL: 48.0058, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6951, Val Loss: 48.6738, Recon: 0.6916, KL: 48.0035, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6881, Val Loss: 48.6725, Recon: 0.6838, KL: 48.0043, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7027, Val Loss: 48.6869, Recon: 0.6957, KL: 48.0070, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7126, Val Loss: 48.6920, Recon: 0.7011, KL: 48.0115, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6904, Val Loss: 48.6882, Recon: 0.6858, KL: 48.0046, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6914, Val Loss: 48.6763, Recon: 0.6875, KL: 48.0039, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6886, Val Loss: 48.6805, Recon: 0.6853, KL: 48.0033, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6932, Val Loss: 48.6765, Recon: 0.6898, KL: 48.0033, KL_weight: 4.8000
Saved model 2 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_1.pt
Training bootstrap model 3/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9364, Val Loss: 0.7148, Recon: 0.9364, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6761, Val Loss: 4.6278, Recon: 0.8066, KL: 3.8695, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4438, Val Loss: 9.3956, Recon: 0.7855, KL: 8.6582, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2174, Val Loss: 14.2117, Recon: 0.7627, KL: 13.4546, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0104, Val Loss: 19.0097, Recon: 0.7579, KL: 18.2525, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8093, Val Loss: 23.7792, Recon: 0.7594, KL: 23.0499, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5941, Val Loss: 28.5867, Recon: 0.7471, KL: 27.8470, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3843, Val Loss: 33.3663, Recon: 0.7397, KL: 32.6446, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2073, Val Loss: 38.1889, Recon: 0.7549, KL: 37.4525, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9865, Val Loss: 42.9644, Recon: 0.7409, KL: 42.2456, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7856, Val Loss: 47.7594, Recon: 0.7363, KL: 47.0493, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7446, Val Loss: 48.7380, Recon: 0.7343, KL: 48.0103, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7489, Val Loss: 48.7445, Recon: 0.7385, KL: 48.0104, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7489, Val Loss: 48.7225, Recon: 0.7444, KL: 48.0045, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7359, Val Loss: 48.7234, Recon: 0.7312, KL: 48.0047, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7238, Val Loss: 48.7208, Recon: 0.7200, KL: 48.0038, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7228, Val Loss: 48.7202, Recon: 0.7216, KL: 48.0012, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7307, Val Loss: 48.7400, Recon: 0.7239, KL: 48.0068, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7326, Val Loss: 48.7284, Recon: 0.7221, KL: 48.0105, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7219, Val Loss: 48.7054, Recon: 0.7165, KL: 48.0054, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7294, Val Loss: 48.7370, Recon: 0.7232, KL: 48.0062, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7215, Val Loss: 48.7192, Recon: 0.7195, KL: 48.0019, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7321, Val Loss: 48.7301, Recon: 0.7274, KL: 48.0046, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7295, Val Loss: 48.7061, Recon: 0.7248, KL: 48.0047, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7162, Val Loss: 48.7091, Recon: 0.7136, KL: 48.0026, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7089, Val Loss: 48.7009, Recon: 0.7070, KL: 48.0019, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7168, Val Loss: 48.6969, Recon: 0.7134, KL: 48.0034, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7134, Val Loss: 48.7080, Recon: 0.7084, KL: 48.0050, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7114, Val Loss: 48.7131, Recon: 0.7086, KL: 48.0028, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7201, Val Loss: 48.7035, Recon: 0.7169, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7123, Val Loss: 48.7089, Recon: 0.7105, KL: 48.0018, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7069, Val Loss: 48.7090, Recon: 0.7027, KL: 48.0042, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7117, Val Loss: 48.7037, Recon: 0.7070, KL: 48.0047, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7101, Val Loss: 48.6980, Recon: 0.7049, KL: 48.0052, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7084, Val Loss: 48.7024, Recon: 0.7071, KL: 48.0013, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7049, Val Loss: 48.6960, Recon: 0.7024, KL: 48.0025, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7096, Val Loss: 48.6967, Recon: 0.7084, KL: 48.0012, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7232, Val Loss: 48.7190, Recon: 0.7109, KL: 48.0123, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7075, Val Loss: 48.7017, Recon: 0.7060, KL: 48.0015, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7147, Val Loss: 48.6972, Recon: 0.7097, KL: 48.0050, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7054, Val Loss: 48.7006, Recon: 0.7033, KL: 48.0021, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7083, Val Loss: 48.6978, Recon: 0.7051, KL: 48.0032, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7011, Val Loss: 48.6978, Recon: 0.6998, KL: 48.0013, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7076, Val Loss: 48.7353, Recon: 0.7030, KL: 48.0046, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7191, Val Loss: 48.7024, Recon: 0.7138, KL: 48.0052, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7057, Val Loss: 48.7039, Recon: 0.7038, KL: 48.0020, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7098, Val Loss: 48.7017, Recon: 0.7058, KL: 48.0039, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7035, Val Loss: 48.7023, Recon: 0.7004, KL: 48.0031, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7105, Val Loss: 48.7036, Recon: 0.7049, KL: 48.0055, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6933, Val Loss: 48.6940, Recon: 0.6919, KL: 48.0014, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6966, Val Loss: 48.6959, Recon: 0.6943, KL: 48.0023, KL_weight: 4.8000
Saved model 3 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_2.pt
Training bootstrap model 4/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 1.0144, Val Loss: 0.7664, Recon: 1.0144, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7427, Val Loss: 4.6800, Recon: 0.8539, KL: 3.8888, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4919, Val Loss: 9.4342, Recon: 0.8321, KL: 8.6598, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2699, Val Loss: 14.2286, Recon: 0.8142, KL: 13.4558, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0566, Val Loss: 19.0326, Recon: 0.7983, KL: 18.2583, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8324, Val Loss: 23.8072, Recon: 0.7859, KL: 23.0465, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6372, Val Loss: 28.5933, Recon: 0.7836, KL: 27.8535, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4134, Val Loss: 33.3840, Recon: 0.7681, KL: 32.6453, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2164, Val Loss: 38.1759, Recon: 0.7691, KL: 37.4473, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0177, Val Loss: 42.9673, Recon: 0.7688, KL: 42.2489, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8214, Val Loss: 47.7960, Recon: 0.7705, KL: 47.0509, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.8185, Val Loss: 48.7713, Recon: 0.7750, KL: 48.0435, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7638, Val Loss: 48.7366, Recon: 0.7604, KL: 48.0034, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7759, Val Loss: 48.7342, Recon: 0.7697, KL: 48.0062, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7594, Val Loss: 48.7322, Recon: 0.7542, KL: 48.0052, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7470, Val Loss: 48.7222, Recon: 0.7436, KL: 48.0033, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7504, Val Loss: 48.7169, Recon: 0.7430, KL: 48.0074, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7475, Val Loss: 48.7141, Recon: 0.7426, KL: 48.0049, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7410, Val Loss: 48.7120, Recon: 0.7336, KL: 48.0074, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7372, Val Loss: 48.7276, Recon: 0.7335, KL: 48.0037, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7427, Val Loss: 48.7226, Recon: 0.7376, KL: 48.0051, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7368, Val Loss: 48.7201, Recon: 0.7322, KL: 48.0046, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7428, Val Loss: 48.7247, Recon: 0.7377, KL: 48.0051, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7277, Val Loss: 48.7075, Recon: 0.7252, KL: 48.0025, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7338, Val Loss: 48.7359, Recon: 0.7292, KL: 48.0046, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7316, Val Loss: 48.7157, Recon: 0.7277, KL: 48.0038, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7414, Val Loss: 48.7352, Recon: 0.7311, KL: 48.0103, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7334, Val Loss: 48.7272, Recon: 0.7263, KL: 48.0071, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7280, Val Loss: 48.7247, Recon: 0.7245, KL: 48.0035, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7135, Val Loss: 48.7037, Recon: 0.7108, KL: 48.0026, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7302, Val Loss: 48.7125, Recon: 0.7257, KL: 48.0045, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7227, Val Loss: 48.7047, Recon: 0.7167, KL: 48.0060, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7243, Val Loss: 48.7066, Recon: 0.7185, KL: 48.0058, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7150, Val Loss: 48.7354, Recon: 0.7101, KL: 48.0049, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7150, Val Loss: 48.7121, Recon: 0.7083, KL: 48.0067, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7172, Val Loss: 48.7059, Recon: 0.7127, KL: 48.0045, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7121, Val Loss: 48.7105, Recon: 0.7058, KL: 48.0063, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7055, Val Loss: 48.7107, Recon: 0.7019, KL: 48.0036, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7018, Val Loss: 48.6925, Recon: 0.6961, KL: 48.0058, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7166, Val Loss: 48.6917, Recon: 0.7126, KL: 48.0040, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7170, Val Loss: 48.7099, Recon: 0.7028, KL: 48.0143, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7001, Val Loss: 48.7122, Recon: 0.6964, KL: 48.0037, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6945, Val Loss: 48.6920, Recon: 0.6906, KL: 48.0039, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7005, Val Loss: 48.7187, Recon: 0.6945, KL: 48.0060, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7033, Val Loss: 48.6874, Recon: 0.7003, KL: 48.0030, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6953, Val Loss: 48.6963, Recon: 0.6924, KL: 48.0029, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7048, Val Loss: 48.6811, Recon: 0.7010, KL: 48.0038, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7012, Val Loss: 48.6811, Recon: 0.6942, KL: 48.0069, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7100, Val Loss: 48.6994, Recon: 0.7036, KL: 48.0064, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6944, Val Loss: 48.6963, Recon: 0.6896, KL: 48.0048, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6931, Val Loss: 48.7021, Recon: 0.6891, KL: 48.0040, KL_weight: 4.8000
Saved model 4 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_3.pt
Training bootstrap model 5/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9600, Val Loss: 0.7386, Recon: 0.9600, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7005, Val Loss: 4.6387, Recon: 0.8281, KL: 3.8724, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4516, Val Loss: 9.4175, Recon: 0.7927, KL: 8.6589, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2227, Val Loss: 14.1918, Recon: 0.7730, KL: 13.4497, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0133, Val Loss: 18.9870, Recon: 0.7640, KL: 18.2493, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8062, Val Loss: 23.8022, Recon: 0.7587, KL: 23.0474, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6092, Val Loss: 28.5827, Recon: 0.7591, KL: 27.8501, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4090, Val Loss: 33.3838, Recon: 0.7585, KL: 32.6505, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1925, Val Loss: 38.1598, Recon: 0.7460, KL: 37.4465, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9955, Val Loss: 42.9721, Recon: 0.7471, KL: 42.2485, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7959, Val Loss: 47.7615, Recon: 0.7453, KL: 47.0506, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7524, Val Loss: 48.7221, Recon: 0.7443, KL: 48.0081, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7458, Val Loss: 48.7244, Recon: 0.7392, KL: 48.0066, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7420, Val Loss: 48.7360, Recon: 0.7360, KL: 48.0060, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7343, Val Loss: 48.7319, Recon: 0.7266, KL: 48.0077, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7348, Val Loss: 48.7157, Recon: 0.7278, KL: 48.0070, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7231, Val Loss: 48.7157, Recon: 0.7196, KL: 48.0036, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7488, Val Loss: 48.7508, Recon: 0.7430, KL: 48.0059, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7298, Val Loss: 48.7134, Recon: 0.7249, KL: 48.0049, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7319, Val Loss: 48.7217, Recon: 0.7253, KL: 48.0066, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7278, Val Loss: 48.7206, Recon: 0.7213, KL: 48.0065, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7215, Val Loss: 48.7087, Recon: 0.7185, KL: 48.0030, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7247, Val Loss: 48.7156, Recon: 0.7189, KL: 48.0057, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7334, Val Loss: 48.7339, Recon: 0.7262, KL: 48.0072, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7205, Val Loss: 48.7058, Recon: 0.7156, KL: 48.0049, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7107, Val Loss: 48.6923, Recon: 0.7082, KL: 48.0024, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7041, Val Loss: 48.6971, Recon: 0.7018, KL: 48.0024, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7107, Val Loss: 48.7029, Recon: 0.7079, KL: 48.0028, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7288, Val Loss: 48.7068, Recon: 0.7165, KL: 48.0124, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7065, Val Loss: 48.6894, Recon: 0.7033, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7026, Val Loss: 48.6865, Recon: 0.6993, KL: 48.0033, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6983, Val Loss: 48.6934, Recon: 0.6960, KL: 48.0023, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7071, Val Loss: 48.6997, Recon: 0.7023, KL: 48.0048, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6986, Val Loss: 48.6839, Recon: 0.6962, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6963, Val Loss: 48.6824, Recon: 0.6924, KL: 48.0039, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7159, Val Loss: 48.7132, Recon: 0.6970, KL: 48.0190, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6962, Val Loss: 48.6835, Recon: 0.6939, KL: 48.0023, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6971, Val Loss: 48.6939, Recon: 0.6911, KL: 48.0060, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6997, Val Loss: 48.6845, Recon: 0.6964, KL: 48.0033, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6880, Val Loss: 48.6966, Recon: 0.6858, KL: 48.0021, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6882, Val Loss: 48.6826, Recon: 0.6849, KL: 48.0033, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6945, Val Loss: 48.6786, Recon: 0.6888, KL: 48.0056, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6912, Val Loss: 48.6766, Recon: 0.6891, KL: 48.0021, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6938, Val Loss: 48.6838, Recon: 0.6877, KL: 48.0061, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6840, Val Loss: 48.6729, Recon: 0.6804, KL: 48.0036, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6817, Val Loss: 48.6808, Recon: 0.6790, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6734, Val Loss: 48.6789, Recon: 0.6706, KL: 48.0028, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6899, Val Loss: 48.6816, Recon: 0.6867, KL: 48.0031, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6877, Val Loss: 48.6721, Recon: 0.6729, KL: 48.0148, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6805, Val Loss: 48.6774, Recon: 0.6767, KL: 48.0038, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6769, Val Loss: 48.6872, Recon: 0.6734, KL: 48.0034, KL_weight: 4.8000
Saved model 5 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_4.pt
Training bootstrap model 6/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9689, Val Loss: 0.7429, Recon: 0.9689, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7101, Val Loss: 4.6411, Recon: 0.8348, KL: 3.8753, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4794, Val Loss: 9.4291, Recon: 0.8189, KL: 8.6605, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2471, Val Loss: 14.2070, Recon: 0.7943, KL: 13.4528, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0493, Val Loss: 18.9926, Recon: 0.7935, KL: 18.2558, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8292, Val Loss: 23.7837, Recon: 0.7793, KL: 23.0499, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6160, Val Loss: 28.6004, Recon: 0.7662, KL: 27.8498, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4144, Val Loss: 33.3789, Recon: 0.7661, KL: 32.6482, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2035, Val Loss: 38.1779, Recon: 0.7587, KL: 37.4448, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0111, Val Loss: 43.0036, Recon: 0.7652, KL: 42.2459, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8176, Val Loss: 47.7964, Recon: 0.7600, KL: 47.0576, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7625, Val Loss: 48.7404, Recon: 0.7572, KL: 48.0053, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7682, Val Loss: 48.7334, Recon: 0.7574, KL: 48.0107, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7591, Val Loss: 48.7339, Recon: 0.7515, KL: 48.0076, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7553, Val Loss: 48.7281, Recon: 0.7467, KL: 48.0086, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7532, Val Loss: 48.7303, Recon: 0.7466, KL: 48.0066, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7469, Val Loss: 48.7329, Recon: 0.7403, KL: 48.0066, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7622, Val Loss: 48.7423, Recon: 0.7523, KL: 48.0099, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7450, Val Loss: 48.7312, Recon: 0.7378, KL: 48.0072, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7391, Val Loss: 48.7239, Recon: 0.7336, KL: 48.0055, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7570, Val Loss: 48.7300, Recon: 0.7450, KL: 48.0120, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7484, Val Loss: 48.7427, Recon: 0.7345, KL: 48.0140, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7322, Val Loss: 48.7282, Recon: 0.7303, KL: 48.0019, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7328, Val Loss: 48.7132, Recon: 0.7248, KL: 48.0080, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7560, Val Loss: 48.7245, Recon: 0.7366, KL: 48.0194, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7363, Val Loss: 48.7245, Recon: 0.7295, KL: 48.0068, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7264, Val Loss: 48.7212, Recon: 0.7217, KL: 48.0047, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7277, Val Loss: 48.7147, Recon: 0.7215, KL: 48.0062, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7231, Val Loss: 48.7081, Recon: 0.7194, KL: 48.0037, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7336, Val Loss: 48.7245, Recon: 0.7293, KL: 48.0042, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7195, Val Loss: 48.7152, Recon: 0.7157, KL: 48.0038, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7199, Val Loss: 48.7126, Recon: 0.7175, KL: 48.0024, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7054, Val Loss: 48.7019, Recon: 0.7028, KL: 48.0025, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7130, Val Loss: 48.6943, Recon: 0.7107, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7254, Val Loss: 48.7086, Recon: 0.7205, KL: 48.0048, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7126, Val Loss: 48.7163, Recon: 0.7076, KL: 48.0050, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7117, Val Loss: 48.7076, Recon: 0.7063, KL: 48.0054, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7186, Val Loss: 48.6965, Recon: 0.7113, KL: 48.0073, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7307, Val Loss: 48.7258, Recon: 0.7216, KL: 48.0091, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6990, Val Loss: 48.6971, Recon: 0.6967, KL: 48.0023, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7052, Val Loss: 48.6937, Recon: 0.6994, KL: 48.0057, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6983, Val Loss: 48.6972, Recon: 0.6943, KL: 48.0039, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6952, Val Loss: 48.6967, Recon: 0.6931, KL: 48.0020, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7068, Val Loss: 48.7064, Recon: 0.7034, KL: 48.0034, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7052, Val Loss: 48.6987, Recon: 0.7001, KL: 48.0051, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7086, Val Loss: 48.6854, Recon: 0.7062, KL: 48.0024, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6949, Val Loss: 48.6935, Recon: 0.6885, KL: 48.0064, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6977, Val Loss: 48.6892, Recon: 0.6917, KL: 48.0060, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6809, Val Loss: 48.6836, Recon: 0.6780, KL: 48.0029, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6865, Val Loss: 48.6943, Recon: 0.6834, KL: 48.0031, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6988, Val Loss: 48.6959, Recon: 0.6948, KL: 48.0040, KL_weight: 4.8000
Saved model 6 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_5.pt
Training bootstrap model 7/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9725, Val Loss: 0.7329, Recon: 0.9725, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7049, Val Loss: 4.6502, Recon: 0.8350, KL: 3.8699, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4591, Val Loss: 9.4206, Recon: 0.8038, KL: 8.6553, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2487, Val Loss: 14.1905, Recon: 0.7925, KL: 13.4562, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0366, Val Loss: 19.0008, Recon: 0.7816, KL: 18.2549, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8226, Val Loss: 23.7800, Recon: 0.7658, KL: 23.0568, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6243, Val Loss: 28.6137, Recon: 0.7735, KL: 27.8508, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4078, Val Loss: 33.3653, Recon: 0.7601, KL: 32.6476, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2103, Val Loss: 38.1904, Recon: 0.7600, KL: 37.4503, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9937, Val Loss: 42.9633, Recon: 0.7474, KL: 42.2462, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7992, Val Loss: 47.7579, Recon: 0.7538, KL: 47.0454, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7544, Val Loss: 48.7123, Recon: 0.7469, KL: 48.0075, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7617, Val Loss: 48.7150, Recon: 0.7505, KL: 48.0112, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7463, Val Loss: 48.7249, Recon: 0.7411, KL: 48.0052, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7381, Val Loss: 48.7274, Recon: 0.7345, KL: 48.0036, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7485, Val Loss: 48.7215, Recon: 0.7449, KL: 48.0036, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7409, Val Loss: 48.7256, Recon: 0.7365, KL: 48.0044, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7401, Val Loss: 48.7108, Recon: 0.7337, KL: 48.0064, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7381, Val Loss: 48.7231, Recon: 0.7325, KL: 48.0057, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7340, Val Loss: 48.7216, Recon: 0.7316, KL: 48.0025, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7299, Val Loss: 48.7114, Recon: 0.7268, KL: 48.0031, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7269, Val Loss: 48.7038, Recon: 0.7237, KL: 48.0032, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7361, Val Loss: 48.7438, Recon: 0.7311, KL: 48.0050, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7304, Val Loss: 48.7072, Recon: 0.7230, KL: 48.0074, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7244, Val Loss: 48.7015, Recon: 0.7198, KL: 48.0046, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7224, Val Loss: 48.7103, Recon: 0.7194, KL: 48.0030, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7233, Val Loss: 48.7046, Recon: 0.7184, KL: 48.0048, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7247, Val Loss: 48.7196, Recon: 0.7211, KL: 48.0036, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7154, Val Loss: 48.6932, Recon: 0.7118, KL: 48.0036, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7071, Val Loss: 48.7039, Recon: 0.7047, KL: 48.0024, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7199, Val Loss: 48.7003, Recon: 0.7155, KL: 48.0044, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7216, Val Loss: 48.7064, Recon: 0.7141, KL: 48.0074, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7256, Val Loss: 48.7077, Recon: 0.7129, KL: 48.0127, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7052, Val Loss: 48.6957, Recon: 0.7023, KL: 48.0030, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7119, Val Loss: 48.6999, Recon: 0.7088, KL: 48.0031, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7212, Val Loss: 48.7168, Recon: 0.7129, KL: 48.0083, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7306, Val Loss: 48.7176, Recon: 0.7142, KL: 48.0164, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7212, Val Loss: 48.7147, Recon: 0.7151, KL: 48.0060, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6979, Val Loss: 48.6832, Recon: 0.6935, KL: 48.0044, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7025, Val Loss: 48.6736, Recon: 0.7001, KL: 48.0024, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6978, Val Loss: 48.6913, Recon: 0.6929, KL: 48.0048, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6924, Val Loss: 48.6835, Recon: 0.6885, KL: 48.0039, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7019, Val Loss: 48.6806, Recon: 0.6920, KL: 48.0099, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6942, Val Loss: 48.6841, Recon: 0.6911, KL: 48.0031, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6951, Val Loss: 48.6929, Recon: 0.6902, KL: 48.0049, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7028, Val Loss: 48.6753, Recon: 0.6989, KL: 48.0040, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6940, Val Loss: 48.6917, Recon: 0.6902, KL: 48.0038, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6920, Val Loss: 48.6698, Recon: 0.6884, KL: 48.0036, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6906, Val Loss: 48.6849, Recon: 0.6879, KL: 48.0027, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6909, Val Loss: 48.6778, Recon: 0.6861, KL: 48.0048, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6868, Val Loss: 48.6925, Recon: 0.6819, KL: 48.0049, KL_weight: 4.8000
Saved model 7 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_6.pt
Training bootstrap model 8/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9641, Val Loss: 0.7592, Recon: 0.9641, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6922, Val Loss: 4.6404, Recon: 0.8173, KL: 3.8749, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4421, Val Loss: 9.4244, Recon: 0.7848, KL: 8.6573, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2315, Val Loss: 14.2109, Recon: 0.7781, KL: 13.4534, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0093, Val Loss: 18.9877, Recon: 0.7608, KL: 18.2486, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8059, Val Loss: 23.8035, Recon: 0.7550, KL: 23.0508, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6030, Val Loss: 28.6024, Recon: 0.7535, KL: 27.8495, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4095, Val Loss: 33.3718, Recon: 0.7574, KL: 32.6522, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1952, Val Loss: 38.1802, Recon: 0.7465, KL: 37.4487, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9988, Val Loss: 42.9814, Recon: 0.7518, KL: 42.2470, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8066, Val Loss: 47.8072, Recon: 0.7565, KL: 47.0501, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7471, Val Loss: 48.7292, Recon: 0.7406, KL: 48.0065, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7418, Val Loss: 48.7231, Recon: 0.7377, KL: 48.0042, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7425, Val Loss: 48.7481, Recon: 0.7368, KL: 48.0056, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7335, Val Loss: 48.7227, Recon: 0.7285, KL: 48.0050, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7322, Val Loss: 48.7334, Recon: 0.7292, KL: 48.0030, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7328, Val Loss: 48.7380, Recon: 0.7266, KL: 48.0062, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7235, Val Loss: 48.7524, Recon: 0.7180, KL: 48.0055, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7180, Val Loss: 48.7226, Recon: 0.7140, KL: 48.0040, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7126, Val Loss: 48.7121, Recon: 0.7094, KL: 48.0032, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7118, Val Loss: 48.7285, Recon: 0.7082, KL: 48.0036, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7159, Val Loss: 48.7075, Recon: 0.7121, KL: 48.0038, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7188, Val Loss: 48.7377, Recon: 0.7103, KL: 48.0085, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7171, Val Loss: 48.7147, Recon: 0.7095, KL: 48.0076, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7090, Val Loss: 48.7018, Recon: 0.7053, KL: 48.0038, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7072, Val Loss: 48.7040, Recon: 0.7023, KL: 48.0049, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7012, Val Loss: 48.7317, Recon: 0.6964, KL: 48.0048, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6979, Val Loss: 48.6958, Recon: 0.6958, KL: 48.0021, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7114, Val Loss: 48.7085, Recon: 0.7056, KL: 48.0059, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7180, Val Loss: 48.7501, Recon: 0.7053, KL: 48.0127, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7113, Val Loss: 48.7462, Recon: 0.7042, KL: 48.0071, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6964, Val Loss: 48.7082, Recon: 0.6923, KL: 48.0041, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6898, Val Loss: 48.7093, Recon: 0.6871, KL: 48.0027, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6923, Val Loss: 48.6853, Recon: 0.6882, KL: 48.0041, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6898, Val Loss: 48.6961, Recon: 0.6866, KL: 48.0032, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6926, Val Loss: 48.7037, Recon: 0.6890, KL: 48.0036, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6854, Val Loss: 48.7027, Recon: 0.6831, KL: 48.0022, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6946, Val Loss: 48.7064, Recon: 0.6917, KL: 48.0029, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6870, Val Loss: 48.6758, Recon: 0.6846, KL: 48.0024, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6814, Val Loss: 48.6755, Recon: 0.6778, KL: 48.0036, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6877, Val Loss: 48.7025, Recon: 0.6858, KL: 48.0019, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6896, Val Loss: 48.6743, Recon: 0.6841, KL: 48.0055, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6805, Val Loss: 48.6801, Recon: 0.6783, KL: 48.0023, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6952, Val Loss: 48.6793, Recon: 0.6835, KL: 48.0116, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6884, Val Loss: 48.6870, Recon: 0.6849, KL: 48.0036, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6832, Val Loss: 48.6799, Recon: 0.6778, KL: 48.0055, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6779, Val Loss: 48.6774, Recon: 0.6757, KL: 48.0021, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6845, Val Loss: 48.6845, Recon: 0.6799, KL: 48.0046, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6849, Val Loss: 48.6867, Recon: 0.6807, KL: 48.0042, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6788, Val Loss: 48.7202, Recon: 0.6760, KL: 48.0028, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6775, Val Loss: 48.6791, Recon: 0.6746, KL: 48.0029, KL_weight: 4.8000
Saved model 8 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_7.pt
Training bootstrap model 9/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9620, Val Loss: 0.7335, Recon: 0.9620, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6899, Val Loss: 4.6245, Recon: 0.8194, KL: 3.8705, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4563, Val Loss: 9.3951, Recon: 0.7987, KL: 8.6575, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2362, Val Loss: 14.2092, Recon: 0.7821, KL: 13.4541, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0209, Val Loss: 18.9954, Recon: 0.7687, KL: 18.2522, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8002, Val Loss: 23.7707, Recon: 0.7534, KL: 23.0468, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6182, Val Loss: 28.5723, Recon: 0.7696, KL: 27.8486, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4101, Val Loss: 33.3903, Recon: 0.7585, KL: 32.6515, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1980, Val Loss: 38.1890, Recon: 0.7511, KL: 37.4469, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9863, Val Loss: 42.9576, Recon: 0.7407, KL: 42.2457, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7940, Val Loss: 47.7971, Recon: 0.7464, KL: 47.0476, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7551, Val Loss: 48.7797, Recon: 0.7441, KL: 48.0111, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7489, Val Loss: 48.7425, Recon: 0.7442, KL: 48.0046, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7445, Val Loss: 48.7352, Recon: 0.7370, KL: 48.0075, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7322, Val Loss: 48.7435, Recon: 0.7286, KL: 48.0036, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7353, Val Loss: 48.7251, Recon: 0.7250, KL: 48.0103, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7389, Val Loss: 48.7433, Recon: 0.7335, KL: 48.0053, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7305, Val Loss: 48.7258, Recon: 0.7237, KL: 48.0068, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7170, Val Loss: 48.7155, Recon: 0.7149, KL: 48.0022, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7241, Val Loss: 48.7005, Recon: 0.7174, KL: 48.0067, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7158, Val Loss: 48.7127, Recon: 0.7081, KL: 48.0077, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7241, Val Loss: 48.7171, Recon: 0.7166, KL: 48.0075, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7314, Val Loss: 48.7212, Recon: 0.7222, KL: 48.0092, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7179, Val Loss: 48.7002, Recon: 0.7138, KL: 48.0041, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7137, Val Loss: 48.7081, Recon: 0.7111, KL: 48.0027, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7194, Val Loss: 48.7174, Recon: 0.7101, KL: 48.0094, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7056, Val Loss: 48.7372, Recon: 0.6956, KL: 48.0100, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7059, Val Loss: 48.7026, Recon: 0.7002, KL: 48.0057, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7086, Val Loss: 48.6979, Recon: 0.7033, KL: 48.0053, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7067, Val Loss: 48.7054, Recon: 0.7023, KL: 48.0044, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6945, Val Loss: 48.6840, Recon: 0.6897, KL: 48.0047, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6893, Val Loss: 48.6866, Recon: 0.6854, KL: 48.0038, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6874, Val Loss: 48.6882, Recon: 0.6841, KL: 48.0033, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6922, Val Loss: 48.6956, Recon: 0.6880, KL: 48.0042, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6902, Val Loss: 48.6853, Recon: 0.6879, KL: 48.0023, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7014, Val Loss: 48.7024, Recon: 0.6929, KL: 48.0085, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6873, Val Loss: 48.7003, Recon: 0.6837, KL: 48.0036, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6856, Val Loss: 48.6801, Recon: 0.6814, KL: 48.0043, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6890, Val Loss: 48.6840, Recon: 0.6860, KL: 48.0030, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6925, Val Loss: 48.6887, Recon: 0.6895, KL: 48.0030, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6928, Val Loss: 48.6879, Recon: 0.6884, KL: 48.0044, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6980, Val Loss: 48.6893, Recon: 0.6928, KL: 48.0052, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6812, Val Loss: 48.6812, Recon: 0.6789, KL: 48.0023, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6915, Val Loss: 48.7080, Recon: 0.6798, KL: 48.0118, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6832, Val Loss: 48.6686, Recon: 0.6808, KL: 48.0023, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6782, Val Loss: 48.6762, Recon: 0.6740, KL: 48.0043, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6775, Val Loss: 48.6827, Recon: 0.6740, KL: 48.0035, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6813, Val Loss: 48.6824, Recon: 0.6784, KL: 48.0029, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6751, Val Loss: 48.6813, Recon: 0.6727, KL: 48.0024, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6712, Val Loss: 48.6755, Recon: 0.6673, KL: 48.0039, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6882, Val Loss: 48.7001, Recon: 0.6811, KL: 48.0072, KL_weight: 4.8000
Saved model 9 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_8.pt
Training bootstrap model 10/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9339, Val Loss: 0.7136, Recon: 0.9339, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6830, Val Loss: 4.6315, Recon: 0.8159, KL: 3.8671, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4508, Val Loss: 9.4076, Recon: 0.7941, KL: 8.6567, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2272, Val Loss: 14.2043, Recon: 0.7740, KL: 13.4532, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0032, Val Loss: 18.9870, Recon: 0.7582, KL: 18.2450, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8156, Val Loss: 23.7855, Recon: 0.7628, KL: 23.0528, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5978, Val Loss: 28.5697, Recon: 0.7504, KL: 27.8474, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3975, Val Loss: 33.3821, Recon: 0.7516, KL: 32.6460, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2239, Val Loss: 38.1819, Recon: 0.7684, KL: 37.4554, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9789, Val Loss: 42.9627, Recon: 0.7335, KL: 42.2454, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7903, Val Loss: 47.7812, Recon: 0.7448, KL: 47.0456, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7442, Val Loss: 48.7493, Recon: 0.7388, KL: 48.0055, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7536, Val Loss: 48.7381, Recon: 0.7455, KL: 48.0082, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7354, Val Loss: 48.7182, Recon: 0.7288, KL: 48.0065, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7276, Val Loss: 48.7316, Recon: 0.7234, KL: 48.0042, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7362, Val Loss: 48.7291, Recon: 0.7297, KL: 48.0065, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7186, Val Loss: 48.7156, Recon: 0.7158, KL: 48.0028, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7265, Val Loss: 48.7341, Recon: 0.7201, KL: 48.0063, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7170, Val Loss: 48.7145, Recon: 0.7123, KL: 48.0047, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7176, Val Loss: 48.7192, Recon: 0.7134, KL: 48.0042, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7155, Val Loss: 48.7217, Recon: 0.7108, KL: 48.0047, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7080, Val Loss: 48.7002, Recon: 0.7049, KL: 48.0031, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7146, Val Loss: 48.7079, Recon: 0.7067, KL: 48.0079, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7085, Val Loss: 48.7064, Recon: 0.7046, KL: 48.0039, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7080, Val Loss: 48.7156, Recon: 0.7031, KL: 48.0049, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7055, Val Loss: 48.6991, Recon: 0.7019, KL: 48.0036, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7027, Val Loss: 48.6994, Recon: 0.6991, KL: 48.0035, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7022, Val Loss: 48.7029, Recon: 0.6987, KL: 48.0035, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6968, Val Loss: 48.6946, Recon: 0.6927, KL: 48.0041, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6972, Val Loss: 48.6917, Recon: 0.6940, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6949, Val Loss: 48.7259, Recon: 0.6904, KL: 48.0045, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6859, Val Loss: 48.6887, Recon: 0.6828, KL: 48.0031, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6879, Val Loss: 48.7001, Recon: 0.6839, KL: 48.0040, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6869, Val Loss: 48.6839, Recon: 0.6841, KL: 48.0029, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6845, Val Loss: 48.6890, Recon: 0.6816, KL: 48.0028, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6867, Val Loss: 48.6807, Recon: 0.6829, KL: 48.0038, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6883, Val Loss: 48.6921, Recon: 0.6847, KL: 48.0036, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6895, Val Loss: 48.6942, Recon: 0.6858, KL: 48.0036, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6777, Val Loss: 48.6832, Recon: 0.6759, KL: 48.0018, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6810, Val Loss: 48.6901, Recon: 0.6763, KL: 48.0047, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6888, Val Loss: 48.6955, Recon: 0.6837, KL: 48.0051, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6823, Val Loss: 48.6698, Recon: 0.6779, KL: 48.0044, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6925, Val Loss: 48.6821, Recon: 0.6849, KL: 48.0075, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6834, Val Loss: 48.6743, Recon: 0.6811, KL: 48.0023, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6951, Val Loss: 48.7051, Recon: 0.6898, KL: 48.0053, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6797, Val Loss: 48.6762, Recon: 0.6758, KL: 48.0039, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6737, Val Loss: 48.6834, Recon: 0.6711, KL: 48.0026, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6785, Val Loss: 48.6900, Recon: 0.6757, KL: 48.0028, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6748, Val Loss: 48.6841, Recon: 0.6733, KL: 48.0015, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6772, Val Loss: 48.6664, Recon: 0.6737, KL: 48.0035, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6790, Val Loss: 48.6963, Recon: 0.6767, KL: 48.0022, KL_weight: 4.8000
Saved model 10 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_9.pt
Training bootstrap model 11/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 1.0130, Val Loss: 0.8243, Recon: 1.0130, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6960, Val Loss: 4.6460, Recon: 0.8256, KL: 3.8704, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4618, Val Loss: 9.4127, Recon: 0.8059, KL: 8.6560, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2468, Val Loss: 14.2166, Recon: 0.7886, KL: 13.4582, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0296, Val Loss: 18.9904, Recon: 0.7747, KL: 18.2549, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8267, Val Loss: 23.7979, Recon: 0.7776, KL: 23.0491, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6342, Val Loss: 28.6122, Recon: 0.7812, KL: 27.8531, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4141, Val Loss: 33.3762, Recon: 0.7681, KL: 32.6460, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2248, Val Loss: 38.1810, Recon: 0.7730, KL: 37.4518, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0116, Val Loss: 42.9933, Recon: 0.7643, KL: 42.2473, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8111, Val Loss: 47.7958, Recon: 0.7641, KL: 47.0471, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7663, Val Loss: 48.7367, Recon: 0.7597, KL: 48.0065, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7603, Val Loss: 48.7215, Recon: 0.7558, KL: 48.0046, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7519, Val Loss: 48.7299, Recon: 0.7477, KL: 48.0042, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7641, Val Loss: 48.7397, Recon: 0.7561, KL: 48.0080, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7427, Val Loss: 48.7371, Recon: 0.7395, KL: 48.0032, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7447, Val Loss: 48.7310, Recon: 0.7413, KL: 48.0034, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7425, Val Loss: 48.7200, Recon: 0.7389, KL: 48.0036, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7418, Val Loss: 48.7162, Recon: 0.7391, KL: 48.0027, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7483, Val Loss: 48.7312, Recon: 0.7401, KL: 48.0082, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7403, Val Loss: 48.7321, Recon: 0.7347, KL: 48.0056, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7366, Val Loss: 48.7147, Recon: 0.7343, KL: 48.0023, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7283, Val Loss: 48.7168, Recon: 0.7265, KL: 48.0018, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7463, Val Loss: 48.7223, Recon: 0.7386, KL: 48.0077, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7402, Val Loss: 48.7171, Recon: 0.7327, KL: 48.0074, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7413, Val Loss: 48.7226, Recon: 0.7362, KL: 48.0052, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7346, Val Loss: 48.7163, Recon: 0.7305, KL: 48.0041, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7386, Val Loss: 48.7124, Recon: 0.7369, KL: 48.0018, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7391, Val Loss: 48.7179, Recon: 0.7349, KL: 48.0042, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7347, Val Loss: 48.7233, Recon: 0.7295, KL: 48.0052, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7330, Val Loss: 48.6977, Recon: 0.7306, KL: 48.0024, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7294, Val Loss: 48.7057, Recon: 0.7274, KL: 48.0020, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7307, Val Loss: 48.7104, Recon: 0.7280, KL: 48.0027, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7303, Val Loss: 48.7043, Recon: 0.7258, KL: 48.0045, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7225, Val Loss: 48.6921, Recon: 0.7210, KL: 48.0015, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7188, Val Loss: 48.7065, Recon: 0.7174, KL: 48.0014, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7395, Val Loss: 48.7233, Recon: 0.7323, KL: 48.0071, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7286, Val Loss: 48.7169, Recon: 0.7246, KL: 48.0040, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7218, Val Loss: 48.7037, Recon: 0.7181, KL: 48.0037, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7220, Val Loss: 48.7032, Recon: 0.7196, KL: 48.0024, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7199, Val Loss: 48.7065, Recon: 0.7175, KL: 48.0024, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7229, Val Loss: 48.6979, Recon: 0.7206, KL: 48.0024, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7263, Val Loss: 48.7048, Recon: 0.7182, KL: 48.0081, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7311, Val Loss: 48.7023, Recon: 0.7272, KL: 48.0039, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7215, Val Loss: 48.7189, Recon: 0.7157, KL: 48.0059, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7148, Val Loss: 48.7083, Recon: 0.7117, KL: 48.0031, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7171, Val Loss: 48.6991, Recon: 0.7149, KL: 48.0022, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7216, Val Loss: 48.7009, Recon: 0.7184, KL: 48.0032, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7263, Val Loss: 48.7031, Recon: 0.7199, KL: 48.0064, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7103, Val Loss: 48.7017, Recon: 0.7070, KL: 48.0033, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7160, Val Loss: 48.6996, Recon: 0.7133, KL: 48.0027, KL_weight: 4.8000
Saved model 11 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_10.pt
Training bootstrap model 12/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9679, Val Loss: 0.7708, Recon: 0.9679, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7137, Val Loss: 4.6528, Recon: 0.8378, KL: 3.8759, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4802, Val Loss: 9.4345, Recon: 0.8163, KL: 8.6639, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2530, Val Loss: 14.2223, Recon: 0.8000, KL: 13.4530, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0458, Val Loss: 19.0264, Recon: 0.7834, KL: 18.2624, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8165, Val Loss: 23.7915, Recon: 0.7679, KL: 23.0486, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6196, Val Loss: 28.5811, Recon: 0.7687, KL: 27.8509, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4229, Val Loss: 33.4004, Recon: 0.7740, KL: 32.6489, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2070, Val Loss: 38.1880, Recon: 0.7614, KL: 37.4457, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9998, Val Loss: 42.9790, Recon: 0.7539, KL: 42.2460, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8032, Val Loss: 47.7752, Recon: 0.7570, KL: 47.0462, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7656, Val Loss: 48.7291, Recon: 0.7593, KL: 48.0064, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7760, Val Loss: 48.7340, Recon: 0.7645, KL: 48.0115, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7548, Val Loss: 48.7453, Recon: 0.7510, KL: 48.0038, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7429, Val Loss: 48.7258, Recon: 0.7383, KL: 48.0046, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7321, Val Loss: 48.7317, Recon: 0.7292, KL: 48.0029, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7491, Val Loss: 48.7143, Recon: 0.7421, KL: 48.0070, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7386, Val Loss: 48.7138, Recon: 0.7345, KL: 48.0041, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7384, Val Loss: 48.7519, Recon: 0.7345, KL: 48.0039, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7457, Val Loss: 48.7486, Recon: 0.7355, KL: 48.0101, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7543, Val Loss: 48.7463, Recon: 0.7450, KL: 48.0093, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7418, Val Loss: 48.7143, Recon: 0.7323, KL: 48.0095, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7591, Val Loss: 48.7366, Recon: 0.7444, KL: 48.0147, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7350, Val Loss: 48.7262, Recon: 0.7316, KL: 48.0034, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7364, Val Loss: 48.7226, Recon: 0.7255, KL: 48.0109, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7253, Val Loss: 48.7219, Recon: 0.7194, KL: 48.0059, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7403, Val Loss: 48.7205, Recon: 0.7374, KL: 48.0030, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7321, Val Loss: 48.7136, Recon: 0.7269, KL: 48.0051, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7201, Val Loss: 48.7105, Recon: 0.7152, KL: 48.0049, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7236, Val Loss: 48.7111, Recon: 0.7198, KL: 48.0038, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7261, Val Loss: 48.7193, Recon: 0.7191, KL: 48.0070, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7077, Val Loss: 48.7111, Recon: 0.7026, KL: 48.0051, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7191, Val Loss: 48.6922, Recon: 0.7103, KL: 48.0089, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7082, Val Loss: 48.6875, Recon: 0.7044, KL: 48.0037, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7024, Val Loss: 48.6900, Recon: 0.6994, KL: 48.0031, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7018, Val Loss: 48.6896, Recon: 0.6977, KL: 48.0041, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6984, Val Loss: 48.7052, Recon: 0.6951, KL: 48.0033, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7298, Val Loss: 48.6952, Recon: 0.7271, KL: 48.0027, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7149, Val Loss: 48.7042, Recon: 0.7074, KL: 48.0074, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7051, Val Loss: 48.7085, Recon: 0.7012, KL: 48.0040, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7124, Val Loss: 48.7021, Recon: 0.7084, KL: 48.0039, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6935, Val Loss: 48.6906, Recon: 0.6903, KL: 48.0032, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6952, Val Loss: 48.6918, Recon: 0.6928, KL: 48.0024, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6895, Val Loss: 48.6872, Recon: 0.6858, KL: 48.0037, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6938, Val Loss: 48.6849, Recon: 0.6901, KL: 48.0037, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6919, Val Loss: 48.6916, Recon: 0.6893, KL: 48.0026, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6905, Val Loss: 48.6745, Recon: 0.6871, KL: 48.0035, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7042, Val Loss: 48.6945, Recon: 0.6897, KL: 48.0145, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6930, Val Loss: 48.6820, Recon: 0.6899, KL: 48.0031, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6893, Val Loss: 48.6828, Recon: 0.6873, KL: 48.0021, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6881, Val Loss: 48.6823, Recon: 0.6843, KL: 48.0038, KL_weight: 4.8000
Saved model 12 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_11.pt
Training bootstrap model 13/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9733, Val Loss: 0.7258, Recon: 0.9733, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7157, Val Loss: 4.6258, Recon: 0.8395, KL: 3.8762, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4628, Val Loss: 9.3878, Recon: 0.8080, KL: 8.6547, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2525, Val Loss: 14.1969, Recon: 0.7948, KL: 13.4577, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0377, Val Loss: 19.0403, Recon: 0.7836, KL: 18.2541, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8174, Val Loss: 23.7761, Recon: 0.7683, KL: 23.0491, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6091, Val Loss: 28.5827, Recon: 0.7635, KL: 27.8456, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4124, Val Loss: 33.3964, Recon: 0.7648, KL: 32.6477, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2017, Val Loss: 38.1742, Recon: 0.7557, KL: 37.4460, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0096, Val Loss: 42.9647, Recon: 0.7635, KL: 42.2461, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8077, Val Loss: 47.7970, Recon: 0.7605, KL: 47.0472, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7659, Val Loss: 48.7192, Recon: 0.7593, KL: 48.0066, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7573, Val Loss: 48.7239, Recon: 0.7522, KL: 48.0052, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7493, Val Loss: 48.7227, Recon: 0.7463, KL: 48.0030, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7472, Val Loss: 48.7238, Recon: 0.7439, KL: 48.0034, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7479, Val Loss: 48.7334, Recon: 0.7439, KL: 48.0040, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7793, Val Loss: 48.7419, Recon: 0.7650, KL: 48.0143, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7594, Val Loss: 48.7241, Recon: 0.7476, KL: 48.0118, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7426, Val Loss: 48.7263, Recon: 0.7408, KL: 48.0018, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7476, Val Loss: 48.7142, Recon: 0.7355, KL: 48.0121, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7457, Val Loss: 48.7204, Recon: 0.7418, KL: 48.0039, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7397, Val Loss: 48.7215, Recon: 0.7339, KL: 48.0058, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7485, Val Loss: 48.7217, Recon: 0.7444, KL: 48.0041, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7337, Val Loss: 48.7162, Recon: 0.7316, KL: 48.0021, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7511, Val Loss: 48.7288, Recon: 0.7433, KL: 48.0078, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7306, Val Loss: 48.7098, Recon: 0.7278, KL: 48.0029, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7378, Val Loss: 48.7154, Recon: 0.7351, KL: 48.0026, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7421, Val Loss: 48.7309, Recon: 0.7373, KL: 48.0048, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7226, Val Loss: 48.7233, Recon: 0.7207, KL: 48.0019, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7214, Val Loss: 48.7162, Recon: 0.7190, KL: 48.0024, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7309, Val Loss: 48.7237, Recon: 0.7285, KL: 48.0025, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7285, Val Loss: 48.6932, Recon: 0.7254, KL: 48.0031, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7197, Val Loss: 48.6947, Recon: 0.7149, KL: 48.0048, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7163, Val Loss: 48.6942, Recon: 0.7140, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7095, Val Loss: 48.6952, Recon: 0.7067, KL: 48.0027, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7086, Val Loss: 48.6899, Recon: 0.7053, KL: 48.0033, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7103, Val Loss: 48.6921, Recon: 0.7077, KL: 48.0026, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7016, Val Loss: 48.7008, Recon: 0.6992, KL: 48.0024, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7046, Val Loss: 48.6838, Recon: 0.7020, KL: 48.0026, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7176, Val Loss: 48.6912, Recon: 0.7066, KL: 48.0110, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7079, Val Loss: 48.6849, Recon: 0.7040, KL: 48.0038, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7031, Val Loss: 48.6776, Recon: 0.7011, KL: 48.0021, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7122, Val Loss: 48.6862, Recon: 0.7085, KL: 48.0037, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7008, Val Loss: 48.6733, Recon: 0.6964, KL: 48.0044, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7079, Val Loss: 48.6885, Recon: 0.7053, KL: 48.0026, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7198, Val Loss: 48.6844, Recon: 0.7172, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6998, Val Loss: 48.6848, Recon: 0.6969, KL: 48.0029, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7076, Val Loss: 48.6895, Recon: 0.6984, KL: 48.0092, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6972, Val Loss: 48.6849, Recon: 0.6943, KL: 48.0029, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6916, Val Loss: 48.6702, Recon: 0.6885, KL: 48.0031, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7166, Val Loss: 48.6995, Recon: 0.7074, KL: 48.0092, KL_weight: 4.8000
Saved model 13 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_12.pt
Training bootstrap model 14/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9675, Val Loss: 0.7355, Recon: 0.9675, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6927, Val Loss: 4.6434, Recon: 0.8181, KL: 3.8746, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4480, Val Loss: 9.4177, Recon: 0.7909, KL: 8.6571, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2383, Val Loss: 14.2129, Recon: 0.7809, KL: 13.4574, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0307, Val Loss: 18.9890, Recon: 0.7766, KL: 18.2541, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8110, Val Loss: 23.7872, Recon: 0.7607, KL: 23.0503, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6267, Val Loss: 28.5821, Recon: 0.7758, KL: 27.8509, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4052, Val Loss: 33.3756, Recon: 0.7572, KL: 32.6480, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2083, Val Loss: 38.1881, Recon: 0.7560, KL: 37.4523, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9999, Val Loss: 42.9644, Recon: 0.7510, KL: 42.2489, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8042, Val Loss: 47.7708, Recon: 0.7576, KL: 47.0466, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7565, Val Loss: 48.7177, Recon: 0.7492, KL: 48.0073, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7590, Val Loss: 48.7398, Recon: 0.7500, KL: 48.0090, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7755, Val Loss: 48.7534, Recon: 0.7549, KL: 48.0206, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7570, Val Loss: 48.7173, Recon: 0.7466, KL: 48.0104, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7420, Val Loss: 48.7186, Recon: 0.7374, KL: 48.0046, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7312, Val Loss: 48.7145, Recon: 0.7271, KL: 48.0041, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7202, Val Loss: 48.7098, Recon: 0.7179, KL: 48.0023, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7403, Val Loss: 48.7458, Recon: 0.7339, KL: 48.0064, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7305, Val Loss: 48.7312, Recon: 0.7262, KL: 48.0043, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7337, Val Loss: 48.7248, Recon: 0.7264, KL: 48.0074, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7227, Val Loss: 48.7147, Recon: 0.7177, KL: 48.0050, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7344, Val Loss: 48.7111, Recon: 0.7237, KL: 48.0107, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7246, Val Loss: 48.7134, Recon: 0.7130, KL: 48.0116, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7070, Val Loss: 48.6969, Recon: 0.7024, KL: 48.0046, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7074, Val Loss: 48.6967, Recon: 0.7028, KL: 48.0046, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7108, Val Loss: 48.6944, Recon: 0.7053, KL: 48.0055, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7071, Val Loss: 48.7003, Recon: 0.7006, KL: 48.0065, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7024, Val Loss: 48.7003, Recon: 0.6944, KL: 48.0079, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7055, Val Loss: 48.6932, Recon: 0.7003, KL: 48.0052, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7135, Val Loss: 48.7034, Recon: 0.7065, KL: 48.0070, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7008, Val Loss: 48.6829, Recon: 0.6973, KL: 48.0035, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6991, Val Loss: 48.6773, Recon: 0.6948, KL: 48.0043, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6939, Val Loss: 48.6901, Recon: 0.6907, KL: 48.0032, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6955, Val Loss: 48.6986, Recon: 0.6883, KL: 48.0072, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6905, Val Loss: 48.6856, Recon: 0.6857, KL: 48.0047, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6919, Val Loss: 48.6821, Recon: 0.6885, KL: 48.0034, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6851, Val Loss: 48.6810, Recon: 0.6819, KL: 48.0032, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6910, Val Loss: 48.6851, Recon: 0.6829, KL: 48.0081, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7064, Val Loss: 48.6827, Recon: 0.6918, KL: 48.0147, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6921, Val Loss: 48.6884, Recon: 0.6875, KL: 48.0046, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6909, Val Loss: 48.6910, Recon: 0.6864, KL: 48.0045, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6893, Val Loss: 48.6756, Recon: 0.6845, KL: 48.0047, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6847, Val Loss: 48.6718, Recon: 0.6805, KL: 48.0041, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6881, Val Loss: 48.6733, Recon: 0.6824, KL: 48.0057, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6872, Val Loss: 48.6672, Recon: 0.6811, KL: 48.0060, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6807, Val Loss: 48.6880, Recon: 0.6759, KL: 48.0048, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6886, Val Loss: 48.6781, Recon: 0.6819, KL: 48.0067, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6804, Val Loss: 48.6784, Recon: 0.6752, KL: 48.0052, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6808, Val Loss: 48.6667, Recon: 0.6758, KL: 48.0051, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6971, Val Loss: 48.6749, Recon: 0.6916, KL: 48.0055, KL_weight: 4.8000
Saved model 14 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_13.pt
Training bootstrap model 15/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 1.0064, Val Loss: 0.8365, Recon: 1.0064, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7006, Val Loss: 4.6526, Recon: 0.8312, KL: 3.8694, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4652, Val Loss: 9.4359, Recon: 0.8070, KL: 8.6583, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2311, Val Loss: 14.2070, Recon: 0.7805, KL: 13.4506, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0324, Val Loss: 19.0054, Recon: 0.7804, KL: 18.2521, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8196, Val Loss: 23.7837, Recon: 0.7700, KL: 23.0496, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6283, Val Loss: 28.6070, Recon: 0.7775, KL: 27.8508, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4166, Val Loss: 33.4093, Recon: 0.7681, KL: 32.6485, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2084, Val Loss: 38.1794, Recon: 0.7621, KL: 37.4463, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9977, Val Loss: 42.9869, Recon: 0.7509, KL: 42.2468, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7951, Val Loss: 47.7730, Recon: 0.7471, KL: 47.0480, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7655, Val Loss: 48.7471, Recon: 0.7540, KL: 48.0115, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7497, Val Loss: 48.7490, Recon: 0.7434, KL: 48.0064, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7348, Val Loss: 48.7250, Recon: 0.7304, KL: 48.0043, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7450, Val Loss: 48.7329, Recon: 0.7395, KL: 48.0054, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7417, Val Loss: 48.7170, Recon: 0.7352, KL: 48.0065, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7306, Val Loss: 48.7308, Recon: 0.7248, KL: 48.0059, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7208, Val Loss: 48.7279, Recon: 0.7185, KL: 48.0023, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7266, Val Loss: 48.7189, Recon: 0.7228, KL: 48.0037, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7334, Val Loss: 48.7228, Recon: 0.7257, KL: 48.0077, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7239, Val Loss: 48.7352, Recon: 0.7172, KL: 48.0067, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7272, Val Loss: 48.7274, Recon: 0.7236, KL: 48.0036, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7291, Val Loss: 48.7038, Recon: 0.7235, KL: 48.0055, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7274, Val Loss: 48.7206, Recon: 0.7213, KL: 48.0061, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7263, Val Loss: 48.7186, Recon: 0.7226, KL: 48.0037, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7162, Val Loss: 48.7123, Recon: 0.7101, KL: 48.0061, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7107, Val Loss: 48.6963, Recon: 0.7080, KL: 48.0027, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7101, Val Loss: 48.7129, Recon: 0.7065, KL: 48.0036, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7059, Val Loss: 48.7020, Recon: 0.7021, KL: 48.0038, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6992, Val Loss: 48.6963, Recon: 0.6969, KL: 48.0023, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7195, Val Loss: 48.7187, Recon: 0.7067, KL: 48.0127, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7029, Val Loss: 48.6962, Recon: 0.6989, KL: 48.0040, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7080, Val Loss: 48.6970, Recon: 0.7031, KL: 48.0049, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7040, Val Loss: 48.6999, Recon: 0.6946, KL: 48.0094, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6963, Val Loss: 48.6822, Recon: 0.6934, KL: 48.0029, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6972, Val Loss: 48.6976, Recon: 0.6918, KL: 48.0054, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6889, Val Loss: 48.6830, Recon: 0.6848, KL: 48.0041, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6868, Val Loss: 48.6782, Recon: 0.6827, KL: 48.0041, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7112, Val Loss: 48.7061, Recon: 0.6961, KL: 48.0150, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6907, Val Loss: 48.6864, Recon: 0.6875, KL: 48.0031, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6899, Val Loss: 48.6742, Recon: 0.6866, KL: 48.0032, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6862, Val Loss: 48.6834, Recon: 0.6827, KL: 48.0035, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6842, Val Loss: 48.6770, Recon: 0.6821, KL: 48.0021, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6817, Val Loss: 48.6768, Recon: 0.6804, KL: 48.0014, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6894, Val Loss: 48.6919, Recon: 0.6851, KL: 48.0043, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6770, Val Loss: 48.6832, Recon: 0.6743, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7019, Val Loss: 48.6977, Recon: 0.6925, KL: 48.0094, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7151, Val Loss: 48.6943, Recon: 0.6981, KL: 48.0170, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6773, Val Loss: 48.6717, Recon: 0.6738, KL: 48.0035, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6825, Val Loss: 48.6747, Recon: 0.6806, KL: 48.0019, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6778, Val Loss: 48.6811, Recon: 0.6734, KL: 48.0044, KL_weight: 4.8000
Saved model 15 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_14.pt
Training bootstrap model 16/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9796, Val Loss: 0.7613, Recon: 0.9796, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7000, Val Loss: 4.6299, Recon: 0.8295, KL: 3.8704, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4440, Val Loss: 9.4279, Recon: 0.7880, KL: 8.6561, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2283, Val Loss: 14.1886, Recon: 0.7760, KL: 13.4523, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0100, Val Loss: 18.9943, Recon: 0.7620, KL: 18.2480, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8125, Val Loss: 23.7981, Recon: 0.7619, KL: 23.0507, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6130, Val Loss: 28.5783, Recon: 0.7634, KL: 27.8497, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4039, Val Loss: 33.3838, Recon: 0.7553, KL: 32.6486, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2001, Val Loss: 38.1683, Recon: 0.7548, KL: 37.4453, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0068, Val Loss: 42.9805, Recon: 0.7514, KL: 42.2554, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7938, Val Loss: 47.7741, Recon: 0.7495, KL: 47.0443, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7366, Val Loss: 48.7321, Recon: 0.7314, KL: 48.0052, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7428, Val Loss: 48.7385, Recon: 0.7356, KL: 48.0071, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7452, Val Loss: 48.7249, Recon: 0.7389, KL: 48.0063, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7422, Val Loss: 48.7261, Recon: 0.7343, KL: 48.0079, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7408, Val Loss: 48.7220, Recon: 0.7341, KL: 48.0067, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7401, Val Loss: 48.7343, Recon: 0.7330, KL: 48.0071, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7238, Val Loss: 48.7304, Recon: 0.7212, KL: 48.0025, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7333, Val Loss: 48.7200, Recon: 0.7285, KL: 48.0048, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7313, Val Loss: 48.7197, Recon: 0.7250, KL: 48.0063, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7360, Val Loss: 48.7161, Recon: 0.7321, KL: 48.0040, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7368, Val Loss: 48.7327, Recon: 0.7295, KL: 48.0073, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7155, Val Loss: 48.7091, Recon: 0.7134, KL: 48.0021, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7255, Val Loss: 48.7042, Recon: 0.7210, KL: 48.0045, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7197, Val Loss: 48.7035, Recon: 0.7169, KL: 48.0028, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7219, Val Loss: 48.7062, Recon: 0.7202, KL: 48.0018, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7334, Val Loss: 48.7236, Recon: 0.7265, KL: 48.0069, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7161, Val Loss: 48.7075, Recon: 0.7135, KL: 48.0026, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7172, Val Loss: 48.7250, Recon: 0.7151, KL: 48.0021, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7152, Val Loss: 48.7175, Recon: 0.7123, KL: 48.0029, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7186, Val Loss: 48.7276, Recon: 0.7144, KL: 48.0042, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7137, Val Loss: 48.6989, Recon: 0.7074, KL: 48.0062, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7172, Val Loss: 48.7104, Recon: 0.7110, KL: 48.0062, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7140, Val Loss: 48.7087, Recon: 0.7115, KL: 48.0025, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7258, Val Loss: 48.7163, Recon: 0.7162, KL: 48.0096, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7333, Val Loss: 48.7119, Recon: 0.7271, KL: 48.0061, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7092, Val Loss: 48.7109, Recon: 0.7076, KL: 48.0016, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7164, Val Loss: 48.7248, Recon: 0.7089, KL: 48.0075, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7097, Val Loss: 48.6998, Recon: 0.7066, KL: 48.0031, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7123, Val Loss: 48.7082, Recon: 0.7070, KL: 48.0053, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7035, Val Loss: 48.7098, Recon: 0.7001, KL: 48.0034, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7011, Val Loss: 48.6966, Recon: 0.6981, KL: 48.0030, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7036, Val Loss: 48.6916, Recon: 0.7012, KL: 48.0024, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7161, Val Loss: 48.6926, Recon: 0.7090, KL: 48.0071, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6989, Val Loss: 48.6936, Recon: 0.6963, KL: 48.0026, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7116, Val Loss: 48.7183, Recon: 0.7059, KL: 48.0057, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7031, Val Loss: 48.6946, Recon: 0.7004, KL: 48.0027, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6979, Val Loss: 48.6887, Recon: 0.6940, KL: 48.0039, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7034, Val Loss: 48.6930, Recon: 0.6932, KL: 48.0102, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6951, Val Loss: 48.6902, Recon: 0.6913, KL: 48.0038, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6927, Val Loss: 48.6851, Recon: 0.6913, KL: 48.0014, KL_weight: 4.8000
Saved model 16 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_lpba40_neuromorphometrics_ibsr_aparc_dk40_aparc_destrieux_columnwise_20251103_1640/models/bootstrap_model_15.pt
Training bootstrap model 17/80
Training data shape IM MODEL: torch.Size([1560, 644])
Epoch 1/250, Train Loss: 0.9918, Val Loss: 0.7241, Recon: 0.9918, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7123, Val Loss: 4.6338, Recon: 0.8396, KL: 3.8727, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4892, Val Loss: 9.4289, Recon: 0.8152, KL: 8.6740, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2526, Val Loss: 14.2038, Recon: 0.8009, KL: 13.4517, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0313, Val Loss: 18.9784, Recon: 0.7837, KL: 18.2475, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8328, Val Loss: 23.7942, Recon: 0.7782, KL: 23.0546, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6382, Val Loss: 28.5834, Recon: 0.7867, KL: 27.8516, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4127, Val Loss: 33.3766, Recon: 0.7652, KL: 32.6475, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2134, Val Loss: 38.1862, Recon: 0.7676, KL: 37.4457, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0149, Val Loss: 42.9825, Recon: 0.7651, KL: 42.2498, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7957, Val Loss: 47.7960, Recon: 0.7521, KL: 47.0436, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7608, Val Loss: 48.7227, Recon: 0.7542, KL: 48.0066, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7771, Val Loss: 48.7398, Recon: 0.7645, KL: 48.0126, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7677, Val Loss: 48.7652, Recon: 0.7601, KL: 48.0077, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7639, Val Loss: 48.7571, Recon: 0.7462, KL: 48.0178, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7548, Val Loss: 48.7684, Recon: 0.7478, KL: 48.0070, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7429, Val Loss: 48.7386, Recon: 0.7388, KL: 48.0041, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7461, Val Loss: 48.7300, Recon: 0.7409, KL: 48.0053, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7403, Val Loss: 48.7158, Recon: 0.7354, KL: 48.0049, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7471, Val Loss: 48.7053, Recon: 0.7406, KL: 48.0065, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7549, Val Loss: 48.7011, Recon: 0.7486, KL: 48.0064, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7333, Val Loss: 48.7302, Recon: 0.7297, KL: 48.0035, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7593, Val Loss: 48.7216, Recon: 0.7526, KL: 48.0068, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7432, Val Loss: 48.7133, Recon: 0.7394, KL: 48.0037, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7382, Val Loss: 48.7261, Recon: 0.7338, KL: 48.0044, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7318, Val Loss: 48.7172, Recon: 0.7282, KL: 48.0036, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7248, Val Loss: 48.7121, Recon: 0.7200, KL: 48.0049, KL_weight: 4.8000
