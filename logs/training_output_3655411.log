Training set erstellt: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251101_1303.csv (1955 Patienten)
Test set erstellt: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/test_metadataHC_0.720251101_1303.csv (2213 Patienten, davon 838 HC und 1375 andere)
Only one CUDA device found. Using cuda:0

####################################################################################################

Starting Catatonia CPA training session NormativeVAE20_all_20251101_1303_HC_columnwise at 2025-11-01_15-03
----------------------------------------------------------------------------------------------------------

Using Device:        cuda:0
Queued Epochs:       250
Batch Size:          32
Data Summary:        ['/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251101_1303.csv']
MRI Data Directory:  /net/data.isilon/ag-cherrmann/lduttenhoefer/project/CAT12_newvals/QC/CAT12_results_final.csv
Loading Model:       False
Output Directory:    /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303

More details in log file: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/logs/2025-11-01_15-03_NormativeVAE20_all_20251101_1303_HC_columnwise_training_log_.txt

####################################################################################################
Starting normative modeling with atlas: all, epochs: 250, bootstraps: 50
Normalization method: columnwise
Configuration saved to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/config.csv
Using device: cuda
Loading NORM control data...
================================================================================
[INFO] Loading PRE-NORMALIZED MRI data
================================================================================
[INFO] Loading metadata from: ['/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/train_metadataHC_0.720251101_1303.csv']
[INFO] Loaded metadata with 1955 subjects
[INFO] Filtered to 1955 subjects with diagnoses: ['HC']
[INFO] Loading pre-normalized MRI data from: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/data_training/CAT12_results_NORMALIZED_columnwise_HC.csv
[INFO] Loaded MRI data with shape: (4163, 1211)
[INFO] Found 1194 ROI columns
[INFO]   - Vgm: 761 features
[INFO]   - G: 216 features
[INFO]   - T: 217 features
[INFO]   - Total: 1194 features
[INFO] Matching metadata subjects with MRI data...
[INFO] Successfully matched 1954/1955 subjects
[WARNING] 1 subjects not found in MRI data
[WARNING] Unmatched: ['sub-031461_T1w']
[INFO] Total subjects processed: 1954
[INFO] Total ROI features per subject: 1194
[INFO] Data loading complete!
================================================================================
[DEBUG] Data shape: torch.Size([1954, 1194])
[DEBUG] Data min: -7.208443641662598
[DEBUG] Data max: 12.389341354370117
[DEBUG] Data mean: -6.27859431112654e-10
[DEBUG] Data std: 0.9968094229698181
[DEBUG] Has NaN: False
[DEBUG] Has Inf: False
[DEBUG] Sample values: tensor([ 0.5064, -0.5589,  0.1566,  0.5520, -0.6191, -0.1211, -1.0259, -0.3292,
        -0.2522,  0.1287])
Number of ROIs in atlas: 1194
[INFO] 1560 subjects in training set
[INFO] 390 subjects in validation set
[WARNING] 4 subjects not found in annotations:
  - sub-031704_T1w
  - sub-NDARINVEY033HCZ_run01_T1w
  - sub-031705_T1w
  - sub-NDARINVKC627BAV_run01_T1w
                        Filename Data_Type  ... NSS_Motor NSS_Total
0                       sub-0001     train  ...       NaN       NaN
1                       sub-0004     train  ...       NaN       NaN
2                       sub-0011     train  ...       NaN       NaN
3                       sub-0013     train  ...       NaN       NaN
4                       sub-0016     train  ...       NaN       NaN
...                          ...       ...  ...       ...       ...
1946           sub-NM5946_0_MPR1     valid  ...       NaN       NaN
1947           sub-NM7365_0_MPR1     valid  ...       NaN       NaN
1948           sub-NM8356_0_MPR1     valid  ...       NaN       NaN
1949  sub-whiteCAT191_ses-01_T1w     valid  ...       0.0       3.0
1950  sub-whiteCAT200_ses-01_T1w     valid  ...       1.0       9.0

[1951 rows x 21 columns]
Using atlas: ['all']
Number of ROIs: 1194
---------------
Data Processing
---------------
Loading Data
  Training Data:       1560 subjects loaded
  Validation Data:      390 subjects loaded

Creating Model
--------------
Training data shape: torch.Size([1560, 1194])
Validation data shape: torch.Size([390, 1194])
/net/data.isilon/ag-cherrmann/lduttenhoefer/project/miniconda3/envs/LISA_ba_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
/net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/src/models/ContrastVAE_2D.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self.scaler = GradScaler()
Model Archtecture: 
NormativeVAE_2D(
  (encoder): Sequential(
    (0): Linear(in_features=1194, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=100, out_features=100, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=100, out_features=20, bias=True)
  )
  (fc_mu): Linear(in_features=20, out_features=20, bias=True)
  (fc_var): Linear(in_features=20, out_features=20, bias=True)
  (decoder): Sequential(
    (0): Linear(in_features=20, out_features=100, bias=True)
    (1): LeakyReLU(negative_slope=0.01)
    (2): Dropout(p=0.1, inplace=False)
    (3): Linear(in_features=100, out_features=100, bias=True)
    (4): LeakyReLU(negative_slope=0.01)
    (5): Dropout(p=0.1, inplace=False)
    (6): Linear(in_features=100, out_features=1194, bias=True)
  )
)

    latent_dim:          20
    optimizer:           Adam (
    scheduler:           <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x2afcda452dd0>
    scaler:              <torch.cuda.amp.grad_scaler.GradScaler object at 0x2afcd9e4d990>
    recon_loss_weight:   16.6449
    kldiv_loss_weight:   1.2
    contr_loss_weight:   0.0
    schedule_on_validation: True
    scheduler_patience:  10
    scheduler_factor:    0.5
    learning_rate:       0.000559
    weight_decay:        1e-05
    dropout_prob:        0.1
    device:              cuda
    Total Parameters:    265,254
    Trainable Params:    265,254
Training baseline model before bootstrap training...
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9057, Val Loss: 0.6751, Recon: 0.9057, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6789, Val Loss: 4.6366, Recon: 0.8064, KL: 3.8725, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4715, Val Loss: 9.4064, Recon: 0.8028, KL: 8.6687, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2394, Val Loss: 14.2030, Recon: 0.7770, KL: 13.4624, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0392, Val Loss: 18.9804, Recon: 0.7833, KL: 18.2558, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8280, Val Loss: 23.8029, Recon: 0.7772, KL: 23.0508, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6051, Val Loss: 28.5772, Recon: 0.7532, KL: 27.8519, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4060, Val Loss: 33.3828, Recon: 0.7469, KL: 32.6591, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1877, Val Loss: 38.1624, Recon: 0.7431, KL: 37.4446, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9871, Val Loss: 42.9573, Recon: 0.7363, KL: 42.2508, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8004, Val Loss: 47.7559, Recon: 0.7506, KL: 47.0498, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7393, Val Loss: 48.7234, Recon: 0.7340, KL: 48.0053, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7483, Val Loss: 48.7521, Recon: 0.7289, KL: 48.0194, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7305, Val Loss: 48.7147, Recon: 0.7238, KL: 48.0066, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7375, Val Loss: 48.7207, Recon: 0.7324, KL: 48.0050, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7399, Val Loss: 48.7196, Recon: 0.7301, KL: 48.0098, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7269, Val Loss: 48.7173, Recon: 0.7225, KL: 48.0044, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7177, Val Loss: 48.6986, Recon: 0.7130, KL: 48.0046, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7137, Val Loss: 48.7043, Recon: 0.7088, KL: 48.0049, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7134, Val Loss: 48.6935, Recon: 0.7112, KL: 48.0022, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7434, Val Loss: 48.7297, Recon: 0.7326, KL: 48.0108, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7261, Val Loss: 48.7103, Recon: 0.7171, KL: 48.0090, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7136, Val Loss: 48.6907, Recon: 0.7097, KL: 48.0039, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7148, Val Loss: 48.6888, Recon: 0.7092, KL: 48.0056, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7043, Val Loss: 48.6929, Recon: 0.7002, KL: 48.0042, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7063, Val Loss: 48.6837, Recon: 0.7037, KL: 48.0026, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7024, Val Loss: 48.6907, Recon: 0.7013, KL: 48.0012, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7034, Val Loss: 48.6913, Recon: 0.7003, KL: 48.0030, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7023, Val Loss: 48.6927, Recon: 0.6982, KL: 48.0041, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6929, Val Loss: 48.6822, Recon: 0.6920, KL: 48.0010, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7047, Val Loss: 48.6832, Recon: 0.7022, KL: 48.0025, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7109, Val Loss: 48.6902, Recon: 0.7004, KL: 48.0105, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7048, Val Loss: 48.6936, Recon: 0.7024, KL: 48.0024, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7564, Val Loss: 48.7077, Recon: 0.7138, KL: 48.0426, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6977, Val Loss: 48.6914, Recon: 0.6951, KL: 48.0025, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7227, Val Loss: 48.6985, Recon: 0.7143, KL: 48.0084, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7007, Val Loss: 48.6817, Recon: 0.6968, KL: 48.0039, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6946, Val Loss: 48.6954, Recon: 0.6916, KL: 48.0030, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6962, Val Loss: 48.6819, Recon: 0.6932, KL: 48.0029, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6978, Val Loss: 48.6827, Recon: 0.6943, KL: 48.0035, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7025, Val Loss: 48.6858, Recon: 0.6991, KL: 48.0033, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7046, Val Loss: 48.6928, Recon: 0.7002, KL: 48.0044, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6920, Val Loss: 48.6787, Recon: 0.6902, KL: 48.0019, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7876, Val Loss: 48.6970, Recon: 0.7038, KL: 48.0839, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7196, Val Loss: 48.6899, Recon: 0.7054, KL: 48.0142, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7026, Val Loss: 48.6848, Recon: 0.6995, KL: 48.0031, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7060, Val Loss: 48.6925, Recon: 0.7023, KL: 48.0037, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7022, Val Loss: 48.6815, Recon: 0.6968, KL: 48.0054, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6899, Val Loss: 48.6902, Recon: 0.6881, KL: 48.0018, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6955, Val Loss: 48.6805, Recon: 0.6937, KL: 48.0018, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6934, Val Loss: 48.6767, Recon: 0.6927, KL: 48.0007, KL_weight: 4.8000
Training bootstrap models...
Starting bootstrap training with 50 iterations
KL warmup will occur over first 50 epochs
Training bootstrap model 1/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9223, Val Loss: 0.7100, Recon: 0.9223, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7010, Val Loss: 4.6460, Recon: 0.8257, KL: 3.8753, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4676, Val Loss: 9.4345, Recon: 0.7966, KL: 8.6710, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2344, Val Loss: 14.2146, Recon: 0.7786, KL: 13.4558, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0052, Val Loss: 19.0143, Recon: 0.7518, KL: 18.2534, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8299, Val Loss: 23.8019, Recon: 0.7735, KL: 23.0564, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6005, Val Loss: 28.6094, Recon: 0.7520, KL: 27.8484, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4070, Val Loss: 33.3839, Recon: 0.7563, KL: 32.6508, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1833, Val Loss: 38.1853, Recon: 0.7344, KL: 37.4489, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0368, Val Loss: 42.9917, Recon: 0.7533, KL: 42.2835, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7832, Val Loss: 47.7721, Recon: 0.7348, KL: 47.0484, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7382, Val Loss: 48.7227, Recon: 0.7344, KL: 48.0037, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7333, Val Loss: 48.7390, Recon: 0.7279, KL: 48.0053, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7389, Val Loss: 48.7316, Recon: 0.7302, KL: 48.0087, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7255, Val Loss: 48.7198, Recon: 0.7181, KL: 48.0074, KL_weight: 4.8000
Epoch 75/250, Train Loss: 49.1517, Val Loss: 48.8435, Recon: 0.8079, KL: 48.3437, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7616, Val Loss: 48.7483, Recon: 0.7463, KL: 48.0153, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7583, Val Loss: 48.7194, Recon: 0.7452, KL: 48.0131, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7383, Val Loss: 48.7096, Recon: 0.7343, KL: 48.0040, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7363, Val Loss: 48.7242, Recon: 0.7295, KL: 48.0069, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7234, Val Loss: 48.7112, Recon: 0.7180, KL: 48.0054, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7235, Val Loss: 48.7114, Recon: 0.7168, KL: 48.0067, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7198, Val Loss: 48.7174, Recon: 0.7126, KL: 48.0072, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7201, Val Loss: 48.7092, Recon: 0.7151, KL: 48.0050, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7139, Val Loss: 48.7025, Recon: 0.7108, KL: 48.0031, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7097, Val Loss: 48.7061, Recon: 0.7072, KL: 48.0025, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7078, Val Loss: 48.6918, Recon: 0.7048, KL: 48.0030, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7085, Val Loss: 48.6945, Recon: 0.7022, KL: 48.0063, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7040, Val Loss: 48.6982, Recon: 0.6967, KL: 48.0073, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6983, Val Loss: 48.6898, Recon: 0.6957, KL: 48.0027, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6975, Val Loss: 48.6940, Recon: 0.6947, KL: 48.0029, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7044, Val Loss: 48.6924, Recon: 0.7011, KL: 48.0032, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7049, Val Loss: 48.6989, Recon: 0.7020, KL: 48.0030, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7000, Val Loss: 48.6926, Recon: 0.6966, KL: 48.0034, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7039, Val Loss: 48.7202, Recon: 0.6940, KL: 48.0098, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7059, Val Loss: 48.6944, Recon: 0.7021, KL: 48.0038, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6982, Val Loss: 48.7051, Recon: 0.6909, KL: 48.0073, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6953, Val Loss: 48.6838, Recon: 0.6930, KL: 48.0023, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7635, Val Loss: 48.7196, Recon: 0.7366, KL: 48.0269, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7195, Val Loss: 48.7442, Recon: 0.7098, KL: 48.0097, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6926, Val Loss: 48.6944, Recon: 0.6884, KL: 48.0042, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7007, Val Loss: 48.7040, Recon: 0.6942, KL: 48.0065, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6980, Val Loss: 48.6860, Recon: 0.6949, KL: 48.0031, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6938, Val Loss: 48.6900, Recon: 0.6909, KL: 48.0029, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7031, Val Loss: 48.6994, Recon: 0.6858, KL: 48.0173, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6860, Val Loss: 48.6871, Recon: 0.6849, KL: 48.0011, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6842, Val Loss: 48.6803, Recon: 0.6824, KL: 48.0018, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6856, Val Loss: 48.6943, Recon: 0.6842, KL: 48.0015, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7106, Val Loss: 48.6883, Recon: 0.7049, KL: 48.0057, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6949, Val Loss: 48.6890, Recon: 0.6910, KL: 48.0039, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6969, Val Loss: 48.6961, Recon: 0.6933, KL: 48.0035, KL_weight: 4.8000
Saved model 1 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_0.pt
Training bootstrap model 2/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8957, Val Loss: 0.6877, Recon: 0.8957, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6965, Val Loss: 4.6510, Recon: 0.8179, KL: 3.8786, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4688, Val Loss: 9.4738, Recon: 0.7898, KL: 8.6789, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2181, Val Loss: 14.1965, Recon: 0.7651, KL: 13.4529, KL_weight: 1.3440
Epoch 20/250, Train Loss: 18.9974, Val Loss: 18.9940, Recon: 0.7468, KL: 18.2505, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8069, Val Loss: 23.7886, Recon: 0.7546, KL: 23.0523, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5826, Val Loss: 28.5630, Recon: 0.7353, KL: 27.8473, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3800, Val Loss: 33.3726, Recon: 0.7339, KL: 32.6462, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1796, Val Loss: 38.1839, Recon: 0.7327, KL: 37.4470, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9733, Val Loss: 42.9474, Recon: 0.7286, KL: 42.2447, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7803, Val Loss: 47.7674, Recon: 0.7335, KL: 47.0468, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7293, Val Loss: 48.7191, Recon: 0.7222, KL: 48.0072, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7264, Val Loss: 48.7216, Recon: 0.7224, KL: 48.0039, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7343, Val Loss: 48.7380, Recon: 0.7248, KL: 48.0095, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7331, Val Loss: 48.7206, Recon: 0.7270, KL: 48.0061, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7221, Val Loss: 48.7195, Recon: 0.7177, KL: 48.0044, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7170, Val Loss: 48.7208, Recon: 0.7134, KL: 48.0036, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7113, Val Loss: 48.7159, Recon: 0.7071, KL: 48.0042, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7116, Val Loss: 48.7151, Recon: 0.7031, KL: 48.0084, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7050, Val Loss: 48.7139, Recon: 0.7020, KL: 48.0030, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7033, Val Loss: 48.7074, Recon: 0.7006, KL: 48.0027, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7033, Val Loss: 48.7041, Recon: 0.6984, KL: 48.0048, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7074, Val Loss: 48.7225, Recon: 0.7007, KL: 48.0067, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7044, Val Loss: 48.7198, Recon: 0.7024, KL: 48.0020, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.6996, Val Loss: 48.7000, Recon: 0.6973, KL: 48.0023, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.8084, Val Loss: 48.7273, Recon: 0.7367, KL: 48.0717, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7040, Val Loss: 48.7187, Recon: 0.6980, KL: 48.0060, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7510, Val Loss: 48.7159, Recon: 0.7094, KL: 48.0416, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7007, Val Loss: 48.6990, Recon: 0.6971, KL: 48.0036, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7016, Val Loss: 48.7054, Recon: 0.6983, KL: 48.0033, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7023, Val Loss: 48.6993, Recon: 0.6995, KL: 48.0028, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6928, Val Loss: 48.6975, Recon: 0.6887, KL: 48.0041, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7033, Val Loss: 48.6953, Recon: 0.6949, KL: 48.0084, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6882, Val Loss: 48.6966, Recon: 0.6874, KL: 48.0009, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6852, Val Loss: 48.6931, Recon: 0.6831, KL: 48.0021, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6954, Val Loss: 48.6880, Recon: 0.6920, KL: 48.0034, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6907, Val Loss: 48.7005, Recon: 0.6852, KL: 48.0055, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7173, Val Loss: 48.7055, Recon: 0.7117, KL: 48.0055, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7039, Val Loss: 48.6935, Recon: 0.6956, KL: 48.0083, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7027, Val Loss: 48.6972, Recon: 0.6978, KL: 48.0049, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6930, Val Loss: 48.6991, Recon: 0.6878, KL: 48.0052, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6877, Val Loss: 48.6897, Recon: 0.6854, KL: 48.0023, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6895, Val Loss: 48.6890, Recon: 0.6876, KL: 48.0019, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7146, Val Loss: 48.7169, Recon: 0.7064, KL: 48.0082, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6880, Val Loss: 48.6842, Recon: 0.6855, KL: 48.0025, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6933, Val Loss: 48.6944, Recon: 0.6850, KL: 48.0083, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6833, Val Loss: 48.6839, Recon: 0.6814, KL: 48.0018, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6831, Val Loss: 48.7096, Recon: 0.6809, KL: 48.0022, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6839, Val Loss: 48.6881, Recon: 0.6812, KL: 48.0027, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6770, Val Loss: 48.6738, Recon: 0.6759, KL: 48.0011, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6825, Val Loss: 48.6821, Recon: 0.6807, KL: 48.0019, KL_weight: 4.8000
Saved model 2 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_1.pt
Training bootstrap model 3/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8986, Val Loss: 0.6694, Recon: 0.8986, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6871, Val Loss: 4.6276, Recon: 0.8144, KL: 3.8727, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4513, Val Loss: 9.3943, Recon: 0.7887, KL: 8.6626, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2442, Val Loss: 14.2074, Recon: 0.7804, KL: 13.4638, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0119, Val Loss: 18.9825, Recon: 0.7606, KL: 18.2512, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8071, Val Loss: 23.7794, Recon: 0.7545, KL: 23.0526, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5895, Val Loss: 28.5732, Recon: 0.7441, KL: 27.8454, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4042, Val Loss: 33.3915, Recon: 0.7504, KL: 32.6538, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1760, Val Loss: 38.1618, Recon: 0.7280, KL: 37.4480, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9784, Val Loss: 42.9654, Recon: 0.7350, KL: 42.2434, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7703, Val Loss: 47.7628, Recon: 0.7269, KL: 47.0434, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7628, Val Loss: 48.7466, Recon: 0.7434, KL: 48.0193, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7532, Val Loss: 48.7267, Recon: 0.7425, KL: 48.0107, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7395, Val Loss: 48.7202, Recon: 0.7300, KL: 48.0094, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7534, Val Loss: 48.7232, Recon: 0.7350, KL: 48.0184, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7301, Val Loss: 48.7254, Recon: 0.7204, KL: 48.0097, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7329, Val Loss: 48.7281, Recon: 0.7245, KL: 48.0084, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7180, Val Loss: 48.7114, Recon: 0.7099, KL: 48.0081, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7256, Val Loss: 48.7065, Recon: 0.7140, KL: 48.0115, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7063, Val Loss: 48.7159, Recon: 0.7020, KL: 48.0042, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7082, Val Loss: 48.7116, Recon: 0.7027, KL: 48.0055, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.6994, Val Loss: 48.7112, Recon: 0.6968, KL: 48.0026, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7073, Val Loss: 48.7040, Recon: 0.7015, KL: 48.0059, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7022, Val Loss: 48.7100, Recon: 0.6993, KL: 48.0029, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7395, Val Loss: 48.7105, Recon: 0.7280, KL: 48.0115, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7185, Val Loss: 48.7262, Recon: 0.7101, KL: 48.0084, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7177, Val Loss: 48.7172, Recon: 0.7105, KL: 48.0072, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7105, Val Loss: 48.7034, Recon: 0.7046, KL: 48.0059, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7120, Val Loss: 48.7075, Recon: 0.7064, KL: 48.0056, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6947, Val Loss: 48.6944, Recon: 0.6902, KL: 48.0045, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6903, Val Loss: 48.6895, Recon: 0.6890, KL: 48.0014, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6860, Val Loss: 48.6931, Recon: 0.6834, KL: 48.0026, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6893, Val Loss: 48.6854, Recon: 0.6876, KL: 48.0017, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7056, Val Loss: 48.6913, Recon: 0.6986, KL: 48.0070, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6956, Val Loss: 48.6883, Recon: 0.6881, KL: 48.0075, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7405, Val Loss: 48.6927, Recon: 0.7260, KL: 48.0145, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7019, Val Loss: 48.6925, Recon: 0.6976, KL: 48.0043, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6930, Val Loss: 48.6936, Recon: 0.6902, KL: 48.0028, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7077, Val Loss: 48.6920, Recon: 0.7015, KL: 48.0062, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6941, Val Loss: 48.6899, Recon: 0.6892, KL: 48.0050, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6923, Val Loss: 48.6892, Recon: 0.6908, KL: 48.0016, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6918, Val Loss: 48.6912, Recon: 0.6857, KL: 48.0061, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6921, Val Loss: 48.6836, Recon: 0.6874, KL: 48.0048, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6843, Val Loss: 48.6814, Recon: 0.6823, KL: 48.0020, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6805, Val Loss: 48.6798, Recon: 0.6795, KL: 48.0011, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7036, Val Loss: 48.6885, Recon: 0.6992, KL: 48.0045, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6900, Val Loss: 48.6919, Recon: 0.6859, KL: 48.0041, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6878, Val Loss: 48.6928, Recon: 0.6839, KL: 48.0039, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6904, Val Loss: 48.6849, Recon: 0.6858, KL: 48.0046, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6885, Val Loss: 48.6847, Recon: 0.6812, KL: 48.0073, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7226, Val Loss: 48.6821, Recon: 0.6841, KL: 48.0385, KL_weight: 4.8000
Saved model 3 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_2.pt
Training bootstrap model 4/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9369, Val Loss: 0.6907, Recon: 0.9369, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7220, Val Loss: 4.6468, Recon: 0.8358, KL: 3.8862, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4977, Val Loss: 9.4561, Recon: 0.8269, KL: 8.6707, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2611, Val Loss: 14.2371, Recon: 0.7957, KL: 13.4654, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0579, Val Loss: 19.0273, Recon: 0.7989, KL: 18.2590, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8424, Val Loss: 23.8234, Recon: 0.7866, KL: 23.0558, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6176, Val Loss: 28.5772, Recon: 0.7680, KL: 27.8496, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4128, Val Loss: 33.4134, Recon: 0.7637, KL: 32.6491, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2231, Val Loss: 38.1931, Recon: 0.7700, KL: 37.4531, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9936, Val Loss: 42.9837, Recon: 0.7458, KL: 42.2478, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8189, Val Loss: 47.7621, Recon: 0.7712, KL: 47.0477, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7852, Val Loss: 48.7370, Recon: 0.7692, KL: 48.0160, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7534, Val Loss: 48.7212, Recon: 0.7451, KL: 48.0083, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7553, Val Loss: 48.7373, Recon: 0.7501, KL: 48.0053, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7444, Val Loss: 48.7043, Recon: 0.7402, KL: 48.0043, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7574, Val Loss: 48.7221, Recon: 0.7460, KL: 48.0114, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7507, Val Loss: 48.7306, Recon: 0.7440, KL: 48.0067, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7555, Val Loss: 48.7297, Recon: 0.7469, KL: 48.0086, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7341, Val Loss: 48.7112, Recon: 0.7302, KL: 48.0040, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7499, Val Loss: 48.7210, Recon: 0.7396, KL: 48.0103, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7556, Val Loss: 48.7388, Recon: 0.7502, KL: 48.0054, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7320, Val Loss: 48.7091, Recon: 0.7283, KL: 48.0037, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7224, Val Loss: 48.7126, Recon: 0.7154, KL: 48.0070, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7424, Val Loss: 48.7114, Recon: 0.7298, KL: 48.0126, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7533, Val Loss: 48.7139, Recon: 0.7460, KL: 48.0072, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7644, Val Loss: 48.7445, Recon: 0.7377, KL: 48.0267, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7345, Val Loss: 48.7269, Recon: 0.7254, KL: 48.0091, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7195, Val Loss: 48.7237, Recon: 0.7169, KL: 48.0026, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7217, Val Loss: 48.7349, Recon: 0.7147, KL: 48.0069, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7189, Val Loss: 48.7017, Recon: 0.7148, KL: 48.0041, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7132, Val Loss: 48.6997, Recon: 0.7111, KL: 48.0022, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7100, Val Loss: 48.6921, Recon: 0.7071, KL: 48.0028, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7218, Val Loss: 48.6904, Recon: 0.7157, KL: 48.0061, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7101, Val Loss: 48.6938, Recon: 0.7046, KL: 48.0055, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7170, Val Loss: 48.6935, Recon: 0.7140, KL: 48.0030, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7113, Val Loss: 48.6957, Recon: 0.7058, KL: 48.0055, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7067, Val Loss: 48.6861, Recon: 0.7027, KL: 48.0040, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7006, Val Loss: 48.6852, Recon: 0.6978, KL: 48.0028, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7392, Val Loss: 48.6955, Recon: 0.7093, KL: 48.0299, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7081, Val Loss: 48.6870, Recon: 0.7041, KL: 48.0039, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7070, Val Loss: 48.6858, Recon: 0.7032, KL: 48.0037, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7036, Val Loss: 48.6841, Recon: 0.7002, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7109, Val Loss: 48.6898, Recon: 0.7060, KL: 48.0049, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6960, Val Loss: 48.6741, Recon: 0.6914, KL: 48.0046, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6936, Val Loss: 48.6875, Recon: 0.6898, KL: 48.0038, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7012, Val Loss: 48.6798, Recon: 0.6976, KL: 48.0036, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7182, Val Loss: 48.6879, Recon: 0.6976, KL: 48.0205, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7431, Val Loss: 48.6908, Recon: 0.6916, KL: 48.0515, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7067, Val Loss: 48.6902, Recon: 0.7016, KL: 48.0050, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6947, Val Loss: 48.6862, Recon: 0.6927, KL: 48.0020, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6999, Val Loss: 48.6827, Recon: 0.6957, KL: 48.0043, KL_weight: 4.8000
Saved model 4 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_3.pt
Training bootstrap model 5/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8773, Val Loss: 0.6776, Recon: 0.8773, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6870, Val Loss: 4.6340, Recon: 0.8114, KL: 3.8756, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4604, Val Loss: 9.4306, Recon: 0.7954, KL: 8.6650, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2295, Val Loss: 14.2021, Recon: 0.7755, KL: 13.4540, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0239, Val Loss: 19.0016, Recon: 0.7673, KL: 18.2567, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8227, Val Loss: 23.8069, Recon: 0.7664, KL: 23.0563, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5963, Val Loss: 28.5744, Recon: 0.7481, KL: 27.8482, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3965, Val Loss: 33.3623, Recon: 0.7483, KL: 32.6482, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1955, Val Loss: 38.1824, Recon: 0.7445, KL: 37.4510, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0070, Val Loss: 42.9613, Recon: 0.7545, KL: 42.2524, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7769, Val Loss: 47.7617, Recon: 0.7309, KL: 47.0460, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7517, Val Loss: 48.7231, Recon: 0.7403, KL: 48.0114, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7291, Val Loss: 48.7282, Recon: 0.7237, KL: 48.0054, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7326, Val Loss: 48.7043, Recon: 0.7277, KL: 48.0050, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7365, Val Loss: 48.7355, Recon: 0.7243, KL: 48.0122, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7213, Val Loss: 48.6944, Recon: 0.7187, KL: 48.0026, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7637, Val Loss: 48.7336, Recon: 0.7399, KL: 48.0238, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7247, Val Loss: 48.7063, Recon: 0.7192, KL: 48.0055, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7084, Val Loss: 48.6957, Recon: 0.7031, KL: 48.0053, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7115, Val Loss: 48.7009, Recon: 0.7077, KL: 48.0038, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7058, Val Loss: 48.7017, Recon: 0.7033, KL: 48.0025, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7161, Val Loss: 48.6939, Recon: 0.7126, KL: 48.0035, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7003, Val Loss: 48.6933, Recon: 0.6977, KL: 48.0026, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7257, Val Loss: 48.6980, Recon: 0.7201, KL: 48.0056, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7212, Val Loss: 48.6943, Recon: 0.7093, KL: 48.0119, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7122, Val Loss: 48.6855, Recon: 0.7067, KL: 48.0055, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7051, Val Loss: 48.6938, Recon: 0.7010, KL: 48.0040, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7024, Val Loss: 48.6979, Recon: 0.6991, KL: 48.0033, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6917, Val Loss: 48.6884, Recon: 0.6890, KL: 48.0027, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7131, Val Loss: 48.6931, Recon: 0.7047, KL: 48.0084, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6957, Val Loss: 48.6818, Recon: 0.6941, KL: 48.0016, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7007, Val Loss: 48.7028, Recon: 0.6920, KL: 48.0087, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7035, Val Loss: 48.6820, Recon: 0.7002, KL: 48.0034, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6968, Val Loss: 48.6871, Recon: 0.6915, KL: 48.0053, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7058, Val Loss: 48.6986, Recon: 0.7022, KL: 48.0036, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6935, Val Loss: 48.7133, Recon: 0.6889, KL: 48.0046, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6921, Val Loss: 48.6795, Recon: 0.6896, KL: 48.0025, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6995, Val Loss: 48.6843, Recon: 0.6943, KL: 48.0051, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6912, Val Loss: 48.6719, Recon: 0.6871, KL: 48.0041, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6963, Val Loss: 48.6804, Recon: 0.6940, KL: 48.0023, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6941, Val Loss: 48.6873, Recon: 0.6909, KL: 48.0032, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6899, Val Loss: 48.6845, Recon: 0.6868, KL: 48.0031, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6988, Val Loss: 48.6934, Recon: 0.6942, KL: 48.0046, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6912, Val Loss: 48.6781, Recon: 0.6874, KL: 48.0038, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6956, Val Loss: 48.6837, Recon: 0.6932, KL: 48.0024, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6985, Val Loss: 48.6805, Recon: 0.6946, KL: 48.0039, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6931, Val Loss: 48.6862, Recon: 0.6869, KL: 48.0062, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6928, Val Loss: 48.6909, Recon: 0.6886, KL: 48.0042, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6813, Val Loss: 48.6703, Recon: 0.6790, KL: 48.0023, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6920, Val Loss: 48.6816, Recon: 0.6855, KL: 48.0064, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6836, Val Loss: 48.6819, Recon: 0.6811, KL: 48.0025, KL_weight: 4.8000
Saved model 5 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_4.pt
Training bootstrap model 6/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9201, Val Loss: 0.6857, Recon: 0.9201, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6940, Val Loss: 4.6419, Recon: 0.8193, KL: 3.8748, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4511, Val Loss: 9.4521, Recon: 0.7871, KL: 8.6639, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2469, Val Loss: 14.2201, Recon: 0.7855, KL: 13.4614, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0169, Val Loss: 18.9813, Recon: 0.7676, KL: 18.2494, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8308, Val Loss: 23.7923, Recon: 0.7712, KL: 23.0595, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5975, Val Loss: 28.5778, Recon: 0.7506, KL: 27.8469, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4144, Val Loss: 33.3732, Recon: 0.7622, KL: 32.6523, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2251, Val Loss: 38.1687, Recon: 0.7673, KL: 37.4577, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0040, Val Loss: 42.9934, Recon: 0.7489, KL: 42.2551, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7958, Val Loss: 47.7762, Recon: 0.7485, KL: 47.0473, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7401, Val Loss: 48.7162, Recon: 0.7347, KL: 48.0054, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7436, Val Loss: 48.7242, Recon: 0.7363, KL: 48.0073, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7368, Val Loss: 48.7334, Recon: 0.7289, KL: 48.0079, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7376, Val Loss: 48.7338, Recon: 0.7297, KL: 48.0079, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7311, Val Loss: 48.7382, Recon: 0.7225, KL: 48.0086, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7252, Val Loss: 48.7123, Recon: 0.7199, KL: 48.0053, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7297, Val Loss: 48.7328, Recon: 0.7235, KL: 48.0062, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7201, Val Loss: 48.7121, Recon: 0.7143, KL: 48.0058, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7101, Val Loss: 48.6892, Recon: 0.7075, KL: 48.0026, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7110, Val Loss: 48.7015, Recon: 0.7066, KL: 48.0043, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7074, Val Loss: 48.7073, Recon: 0.7031, KL: 48.0043, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7111, Val Loss: 48.6933, Recon: 0.7060, KL: 48.0051, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7067, Val Loss: 48.7087, Recon: 0.7043, KL: 48.0023, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.6950, Val Loss: 48.6943, Recon: 0.6930, KL: 48.0020, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7409, Val Loss: 48.7027, Recon: 0.7199, KL: 48.0210, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7032, Val Loss: 48.6930, Recon: 0.6999, KL: 48.0033, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7526, Val Loss: 48.7550, Recon: 0.7304, KL: 48.0222, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7129, Val Loss: 48.7055, Recon: 0.7044, KL: 48.0084, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7027, Val Loss: 48.6889, Recon: 0.6979, KL: 48.0048, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7010, Val Loss: 48.7101, Recon: 0.6971, KL: 48.0040, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7096, Val Loss: 48.6914, Recon: 0.6967, KL: 48.0128, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6944, Val Loss: 48.6943, Recon: 0.6902, KL: 48.0042, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7014, Val Loss: 48.7030, Recon: 0.6933, KL: 48.0081, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6958, Val Loss: 48.6855, Recon: 0.6901, KL: 48.0057, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6919, Val Loss: 48.6796, Recon: 0.6879, KL: 48.0040, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6947, Val Loss: 48.6858, Recon: 0.6914, KL: 48.0033, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6821, Val Loss: 48.6789, Recon: 0.6805, KL: 48.0016, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6982, Val Loss: 48.6880, Recon: 0.6917, KL: 48.0064, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6869, Val Loss: 48.6771, Recon: 0.6827, KL: 48.0042, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6885, Val Loss: 48.6806, Recon: 0.6856, KL: 48.0029, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6824, Val Loss: 48.6750, Recon: 0.6802, KL: 48.0022, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6827, Val Loss: 48.6790, Recon: 0.6794, KL: 48.0033, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6838, Val Loss: 48.6771, Recon: 0.6817, KL: 48.0021, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6900, Val Loss: 48.6738, Recon: 0.6879, KL: 48.0020, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7008, Val Loss: 48.6844, Recon: 0.6926, KL: 48.0082, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6839, Val Loss: 48.6779, Recon: 0.6801, KL: 48.0038, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6829, Val Loss: 48.6756, Recon: 0.6795, KL: 48.0034, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6856, Val Loss: 48.6831, Recon: 0.6831, KL: 48.0025, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6858, Val Loss: 48.6755, Recon: 0.6833, KL: 48.0025, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6847, Val Loss: 48.6766, Recon: 0.6803, KL: 48.0045, KL_weight: 4.8000
Saved model 6 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_5.pt
Training bootstrap model 7/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8390, Val Loss: 0.6725, Recon: 0.8390, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6909, Val Loss: 4.6333, Recon: 0.8099, KL: 3.8810, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4423, Val Loss: 9.4253, Recon: 0.7836, KL: 8.6588, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.1966, Val Loss: 14.1848, Recon: 0.7467, KL: 13.4499, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0069, Val Loss: 18.9846, Recon: 0.7521, KL: 18.2548, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7880, Val Loss: 23.7825, Recon: 0.7398, KL: 23.0481, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6154, Val Loss: 28.5800, Recon: 0.7425, KL: 27.8729, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3736, Val Loss: 33.3506, Recon: 0.7258, KL: 32.6478, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1983, Val Loss: 38.1809, Recon: 0.7355, KL: 37.4628, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9970, Val Loss: 42.9804, Recon: 0.7478, KL: 42.2492, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7868, Val Loss: 47.7851, Recon: 0.7343, KL: 47.0526, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7216, Val Loss: 48.7180, Recon: 0.7156, KL: 48.0061, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7303, Val Loss: 48.7195, Recon: 0.7228, KL: 48.0075, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7137, Val Loss: 48.7252, Recon: 0.7075, KL: 48.0063, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7191, Val Loss: 48.7179, Recon: 0.7157, KL: 48.0034, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7095, Val Loss: 48.7186, Recon: 0.7079, KL: 48.0016, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7103, Val Loss: 48.7344, Recon: 0.7073, KL: 48.0030, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7100, Val Loss: 48.7072, Recon: 0.7059, KL: 48.0041, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7035, Val Loss: 48.7096, Recon: 0.6981, KL: 48.0054, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7081, Val Loss: 48.7015, Recon: 0.7015, KL: 48.0066, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.6953, Val Loss: 48.6968, Recon: 0.6921, KL: 48.0032, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.6973, Val Loss: 48.6993, Recon: 0.6946, KL: 48.0027, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.6976, Val Loss: 48.7034, Recon: 0.6908, KL: 48.0068, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.6940, Val Loss: 48.7087, Recon: 0.6909, KL: 48.0031, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.6990, Val Loss: 48.7090, Recon: 0.6943, KL: 48.0047, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.6953, Val Loss: 48.7027, Recon: 0.6925, KL: 48.0028, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6979, Val Loss: 48.6998, Recon: 0.6958, KL: 48.0021, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6892, Val Loss: 48.6866, Recon: 0.6863, KL: 48.0029, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6882, Val Loss: 48.6893, Recon: 0.6860, KL: 48.0022, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7333, Val Loss: 48.7234, Recon: 0.7240, KL: 48.0093, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6959, Val Loss: 48.6969, Recon: 0.6887, KL: 48.0071, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6920, Val Loss: 48.6954, Recon: 0.6902, KL: 48.0017, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6862, Val Loss: 48.6827, Recon: 0.6842, KL: 48.0021, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6936, Val Loss: 48.6956, Recon: 0.6900, KL: 48.0036, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6894, Val Loss: 48.6828, Recon: 0.6857, KL: 48.0037, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7163, Val Loss: 48.6928, Recon: 0.7063, KL: 48.0100, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6826, Val Loss: 48.6805, Recon: 0.6810, KL: 48.0016, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6917, Val Loss: 48.6936, Recon: 0.6899, KL: 48.0018, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6941, Val Loss: 48.6984, Recon: 0.6876, KL: 48.0065, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6932, Val Loss: 48.6820, Recon: 0.6891, KL: 48.0041, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6796, Val Loss: 48.6890, Recon: 0.6781, KL: 48.0015, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6791, Val Loss: 48.6822, Recon: 0.6769, KL: 48.0022, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6805, Val Loss: 48.6784, Recon: 0.6773, KL: 48.0031, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6874, Val Loss: 48.6905, Recon: 0.6837, KL: 48.0037, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6780, Val Loss: 48.6800, Recon: 0.6764, KL: 48.0016, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6826, Val Loss: 48.6764, Recon: 0.6793, KL: 48.0033, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6881, Val Loss: 48.6932, Recon: 0.6839, KL: 48.0042, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6743, Val Loss: 48.6811, Recon: 0.6717, KL: 48.0026, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6714, Val Loss: 48.6717, Recon: 0.6692, KL: 48.0023, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6755, Val Loss: 48.6792, Recon: 0.6743, KL: 48.0012, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6740, Val Loss: 48.6744, Recon: 0.6722, KL: 48.0018, KL_weight: 4.8000
Saved model 7 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_6.pt
Training bootstrap model 8/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8576, Val Loss: 0.6723, Recon: 0.8576, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7001, Val Loss: 4.6475, Recon: 0.8149, KL: 3.8852, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4536, Val Loss: 9.4582, Recon: 0.7915, KL: 8.6621, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2507, Val Loss: 14.2063, Recon: 0.7879, KL: 13.4628, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0327, Val Loss: 18.9939, Recon: 0.7696, KL: 18.2632, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7996, Val Loss: 23.7800, Recon: 0.7501, KL: 23.0495, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6088, Val Loss: 28.5978, Recon: 0.7546, KL: 27.8542, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4108, Val Loss: 33.3819, Recon: 0.7609, KL: 32.6499, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1863, Val Loss: 38.1774, Recon: 0.7382, KL: 37.4482, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0096, Val Loss: 43.0131, Recon: 0.7455, KL: 42.2642, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7896, Val Loss: 47.7617, Recon: 0.7408, KL: 47.0488, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7411, Val Loss: 48.7346, Recon: 0.7341, KL: 48.0070, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7364, Val Loss: 48.7184, Recon: 0.7302, KL: 48.0062, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7303, Val Loss: 48.7153, Recon: 0.7221, KL: 48.0082, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7337, Val Loss: 48.7159, Recon: 0.7279, KL: 48.0058, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7214, Val Loss: 48.7150, Recon: 0.7185, KL: 48.0029, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7225, Val Loss: 48.7033, Recon: 0.7168, KL: 48.0058, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7156, Val Loss: 48.7123, Recon: 0.7108, KL: 48.0048, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7164, Val Loss: 48.7151, Recon: 0.7120, KL: 48.0044, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.8453, Val Loss: 48.7297, Recon: 0.7512, KL: 48.0941, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7196, Val Loss: 48.7067, Recon: 0.7133, KL: 48.0062, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7079, Val Loss: 48.7000, Recon: 0.7052, KL: 48.0027, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.8208, Val Loss: 48.7425, Recon: 0.7813, KL: 48.0395, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7509, Val Loss: 48.7210, Recon: 0.7420, KL: 48.0089, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7626, Val Loss: 48.7448, Recon: 0.7272, KL: 48.0353, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7158, Val Loss: 48.7098, Recon: 0.7097, KL: 48.0061, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7117, Val Loss: 48.7082, Recon: 0.7053, KL: 48.0064, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7070, Val Loss: 48.7021, Recon: 0.7019, KL: 48.0051, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6994, Val Loss: 48.6969, Recon: 0.6946, KL: 48.0048, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7072, Val Loss: 48.6970, Recon: 0.7046, KL: 48.0026, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7023, Val Loss: 48.7050, Recon: 0.6999, KL: 48.0023, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7184, Val Loss: 48.6994, Recon: 0.7015, KL: 48.0169, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7029, Val Loss: 48.7094, Recon: 0.6964, KL: 48.0065, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7051, Val Loss: 48.6953, Recon: 0.7009, KL: 48.0042, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7049, Val Loss: 48.7063, Recon: 0.6995, KL: 48.0054, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6994, Val Loss: 48.6936, Recon: 0.6943, KL: 48.0050, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6932, Val Loss: 48.6960, Recon: 0.6914, KL: 48.0018, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6946, Val Loss: 48.6944, Recon: 0.6915, KL: 48.0031, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6910, Val Loss: 48.7026, Recon: 0.6893, KL: 48.0017, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6887, Val Loss: 48.6976, Recon: 0.6868, KL: 48.0019, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6903, Val Loss: 48.6892, Recon: 0.6867, KL: 48.0036, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6884, Val Loss: 48.6786, Recon: 0.6862, KL: 48.0022, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6881, Val Loss: 48.6790, Recon: 0.6853, KL: 48.0028, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6923, Val Loss: 48.6797, Recon: 0.6859, KL: 48.0063, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6882, Val Loss: 48.6867, Recon: 0.6854, KL: 48.0028, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6988, Val Loss: 48.6945, Recon: 0.6959, KL: 48.0029, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6877, Val Loss: 48.6837, Recon: 0.6849, KL: 48.0028, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6796, Val Loss: 48.6796, Recon: 0.6784, KL: 48.0012, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6980, Val Loss: 48.6890, Recon: 0.6928, KL: 48.0052, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6879, Val Loss: 48.6811, Recon: 0.6853, KL: 48.0025, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6783, Val Loss: 48.6799, Recon: 0.6760, KL: 48.0023, KL_weight: 4.8000
Saved model 8 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_7.pt
Training bootstrap model 9/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9440, Val Loss: 0.7087, Recon: 0.9440, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7184, Val Loss: 4.6363, Recon: 0.8299, KL: 3.8885, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4680, Val Loss: 9.4399, Recon: 0.8015, KL: 8.6665, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2333, Val Loss: 14.2029, Recon: 0.7750, KL: 13.4583, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0289, Val Loss: 18.9960, Recon: 0.7729, KL: 18.2560, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7947, Val Loss: 23.7736, Recon: 0.7468, KL: 23.0478, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6329, Val Loss: 28.5995, Recon: 0.7723, KL: 27.8606, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4058, Val Loss: 33.3837, Recon: 0.7562, KL: 32.6496, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2049, Val Loss: 38.1846, Recon: 0.7522, KL: 37.4527, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9897, Val Loss: 42.9745, Recon: 0.7420, KL: 42.2477, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7982, Val Loss: 47.7677, Recon: 0.7451, KL: 47.0530, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7469, Val Loss: 48.7294, Recon: 0.7417, KL: 48.0052, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7371, Val Loss: 48.7367, Recon: 0.7275, KL: 48.0096, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7333, Val Loss: 48.7251, Recon: 0.7292, KL: 48.0041, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7275, Val Loss: 48.7208, Recon: 0.7234, KL: 48.0041, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7267, Val Loss: 48.7097, Recon: 0.7208, KL: 48.0059, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7197, Val Loss: 48.7272, Recon: 0.7173, KL: 48.0024, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7324, Val Loss: 48.7154, Recon: 0.7204, KL: 48.0120, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7406, Val Loss: 48.7187, Recon: 0.7315, KL: 48.0091, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7231, Val Loss: 48.6995, Recon: 0.7189, KL: 48.0041, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7366, Val Loss: 48.7056, Recon: 0.7289, KL: 48.0077, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.8572, Val Loss: 48.7764, Recon: 0.7673, KL: 48.0899, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7311, Val Loss: 48.7161, Recon: 0.7253, KL: 48.0058, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7288, Val Loss: 48.7089, Recon: 0.7227, KL: 48.0061, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7166, Val Loss: 48.7055, Recon: 0.7125, KL: 48.0041, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7247, Val Loss: 48.7066, Recon: 0.7106, KL: 48.0141, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7241, Val Loss: 48.7096, Recon: 0.7190, KL: 48.0051, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7142, Val Loss: 48.7046, Recon: 0.7097, KL: 48.0046, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7091, Val Loss: 48.7074, Recon: 0.7058, KL: 48.0033, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6994, Val Loss: 48.6960, Recon: 0.6972, KL: 48.0022, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7058, Val Loss: 48.6968, Recon: 0.7028, KL: 48.0030, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7046, Val Loss: 48.6895, Recon: 0.6991, KL: 48.0055, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7071, Val Loss: 48.7014, Recon: 0.7014, KL: 48.0056, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6987, Val Loss: 48.7008, Recon: 0.6948, KL: 48.0039, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7043, Val Loss: 48.6960, Recon: 0.6984, KL: 48.0060, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7880, Val Loss: 48.7068, Recon: 0.7238, KL: 48.0642, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7021, Val Loss: 48.7087, Recon: 0.6983, KL: 48.0038, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7029, Val Loss: 48.6823, Recon: 0.6963, KL: 48.0066, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6981, Val Loss: 48.6817, Recon: 0.6952, KL: 48.0029, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7511, Val Loss: 48.7214, Recon: 0.6953, KL: 48.0558, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7098, Val Loss: 48.6869, Recon: 0.7063, KL: 48.0035, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7086, Val Loss: 48.6999, Recon: 0.7017, KL: 48.0069, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6944, Val Loss: 48.6815, Recon: 0.6908, KL: 48.0036, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7026, Val Loss: 48.6898, Recon: 0.7011, KL: 48.0015, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6970, Val Loss: 48.6898, Recon: 0.6946, KL: 48.0024, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7005, Val Loss: 48.6856, Recon: 0.6964, KL: 48.0041, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6997, Val Loss: 48.6991, Recon: 0.6913, KL: 48.0084, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7083, Val Loss: 48.6976, Recon: 0.7055, KL: 48.0029, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6873, Val Loss: 48.7073, Recon: 0.6837, KL: 48.0036, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6887, Val Loss: 48.6887, Recon: 0.6844, KL: 48.0043, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6883, Val Loss: 48.6819, Recon: 0.6856, KL: 48.0028, KL_weight: 4.8000
Saved model 9 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_8.pt
Training bootstrap model 10/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9155, Val Loss: 0.6784, Recon: 0.9155, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7032, Val Loss: 4.6381, Recon: 0.8255, KL: 3.8778, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4771, Val Loss: 9.4329, Recon: 0.8101, KL: 8.6670, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2462, Val Loss: 14.1802, Recon: 0.7885, KL: 13.4577, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0335, Val Loss: 18.9922, Recon: 0.7791, KL: 18.2545, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8211, Val Loss: 23.8058, Recon: 0.7720, KL: 23.0490, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5998, Val Loss: 28.5891, Recon: 0.7527, KL: 27.8470, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4162, Val Loss: 33.3761, Recon: 0.7689, KL: 32.6473, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1943, Val Loss: 38.1829, Recon: 0.7427, KL: 37.4516, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0029, Val Loss: 42.9896, Recon: 0.7512, KL: 42.2517, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8085, Val Loss: 47.7592, Recon: 0.7542, KL: 47.0543, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7428, Val Loss: 48.7279, Recon: 0.7387, KL: 48.0041, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7506, Val Loss: 48.7028, Recon: 0.7429, KL: 48.0077, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7486, Val Loss: 48.7042, Recon: 0.7455, KL: 48.0032, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7383, Val Loss: 48.7068, Recon: 0.7308, KL: 48.0075, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7462, Val Loss: 48.7271, Recon: 0.7360, KL: 48.0102, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7316, Val Loss: 48.7164, Recon: 0.7271, KL: 48.0045, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7575, Val Loss: 48.7227, Recon: 0.7375, KL: 48.0200, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7323, Val Loss: 48.7071, Recon: 0.7236, KL: 48.0088, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7346, Val Loss: 48.6945, Recon: 0.7307, KL: 48.0039, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7315, Val Loss: 48.7073, Recon: 0.7248, KL: 48.0067, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7249, Val Loss: 48.6927, Recon: 0.7204, KL: 48.0044, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7221, Val Loss: 48.7004, Recon: 0.7177, KL: 48.0043, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7426, Val Loss: 48.7273, Recon: 0.7340, KL: 48.0086, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7264, Val Loss: 48.7014, Recon: 0.7233, KL: 48.0031, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7246, Val Loss: 48.6951, Recon: 0.7203, KL: 48.0043, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7225, Val Loss: 48.7197, Recon: 0.7154, KL: 48.0071, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7563, Val Loss: 48.7283, Recon: 0.7263, KL: 48.0300, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7204, Val Loss: 48.6920, Recon: 0.7161, KL: 48.0043, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7177, Val Loss: 48.6942, Recon: 0.7122, KL: 48.0055, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7257, Val Loss: 48.6973, Recon: 0.7198, KL: 48.0059, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7106, Val Loss: 48.6867, Recon: 0.7085, KL: 48.0021, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7283, Val Loss: 48.6907, Recon: 0.7230, KL: 48.0054, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7225, Val Loss: 48.6896, Recon: 0.7186, KL: 48.0039, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7112, Val Loss: 48.6845, Recon: 0.7082, KL: 48.0030, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7052, Val Loss: 48.7028, Recon: 0.7025, KL: 48.0027, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7090, Val Loss: 48.6919, Recon: 0.7067, KL: 48.0022, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7068, Val Loss: 48.6836, Recon: 0.7040, KL: 48.0028, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7074, Val Loss: 48.6839, Recon: 0.7054, KL: 48.0020, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7022, Val Loss: 48.6915, Recon: 0.7000, KL: 48.0022, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7081, Val Loss: 48.6850, Recon: 0.7057, KL: 48.0024, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7009, Val Loss: 48.7015, Recon: 0.6968, KL: 48.0041, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7026, Val Loss: 48.6954, Recon: 0.6992, KL: 48.0034, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7144, Val Loss: 48.6923, Recon: 0.7071, KL: 48.0073, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7491, Val Loss: 48.7003, Recon: 0.7113, KL: 48.0378, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7072, Val Loss: 48.6891, Recon: 0.7045, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7044, Val Loss: 48.6821, Recon: 0.7029, KL: 48.0014, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7023, Val Loss: 48.6923, Recon: 0.6965, KL: 48.0059, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7104, Val Loss: 48.6951, Recon: 0.7085, KL: 48.0019, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7138, Val Loss: 48.6941, Recon: 0.7066, KL: 48.0072, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7006, Val Loss: 48.6844, Recon: 0.6986, KL: 48.0020, KL_weight: 4.8000
Saved model 10 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_9.pt
Training bootstrap model 11/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8681, Val Loss: 0.6756, Recon: 0.8681, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6684, Val Loss: 4.6419, Recon: 0.7938, KL: 3.8746, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4468, Val Loss: 9.4305, Recon: 0.7846, KL: 8.6622, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2176, Val Loss: 14.1990, Recon: 0.7641, KL: 13.4534, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0140, Val Loss: 18.9899, Recon: 0.7561, KL: 18.2580, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8004, Val Loss: 23.8091, Recon: 0.7521, KL: 23.0484, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5907, Val Loss: 28.5838, Recon: 0.7393, KL: 27.8514, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3980, Val Loss: 33.3802, Recon: 0.7498, KL: 32.6482, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1877, Val Loss: 38.1709, Recon: 0.7360, KL: 37.4517, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9741, Val Loss: 42.9488, Recon: 0.7294, KL: 42.2447, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7951, Val Loss: 47.7928, Recon: 0.7404, KL: 47.0547, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7398, Val Loss: 48.7372, Recon: 0.7290, KL: 48.0108, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7351, Val Loss: 48.7482, Recon: 0.7265, KL: 48.0085, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7620, Val Loss: 48.7314, Recon: 0.7380, KL: 48.0240, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7206, Val Loss: 48.7127, Recon: 0.7178, KL: 48.0028, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7210, Val Loss: 48.7069, Recon: 0.7164, KL: 48.0046, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7573, Val Loss: 48.7255, Recon: 0.7405, KL: 48.0168, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7681, Val Loss: 48.7282, Recon: 0.7347, KL: 48.0334, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7229, Val Loss: 48.7062, Recon: 0.7186, KL: 48.0043, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7202, Val Loss: 48.7068, Recon: 0.7158, KL: 48.0044, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7153, Val Loss: 48.6982, Recon: 0.7117, KL: 48.0036, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7080, Val Loss: 48.6986, Recon: 0.7046, KL: 48.0034, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7124, Val Loss: 48.7071, Recon: 0.7101, KL: 48.0024, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7317, Val Loss: 48.7023, Recon: 0.7230, KL: 48.0087, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7130, Val Loss: 48.7042, Recon: 0.7083, KL: 48.0047, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7150, Val Loss: 48.7319, Recon: 0.7096, KL: 48.0055, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7120, Val Loss: 48.7126, Recon: 0.7058, KL: 48.0062, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7089, Val Loss: 48.6954, Recon: 0.7043, KL: 48.0046, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7004, Val Loss: 48.6970, Recon: 0.6989, KL: 48.0015, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6999, Val Loss: 48.6988, Recon: 0.6962, KL: 48.0038, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7063, Val Loss: 48.7040, Recon: 0.7025, KL: 48.0038, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6997, Val Loss: 48.6845, Recon: 0.6959, KL: 48.0037, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6975, Val Loss: 48.6850, Recon: 0.6945, KL: 48.0030, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6954, Val Loss: 48.6903, Recon: 0.6920, KL: 48.0035, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6910, Val Loss: 48.6842, Recon: 0.6885, KL: 48.0026, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7030, Val Loss: 48.6986, Recon: 0.7000, KL: 48.0030, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6930, Val Loss: 48.6947, Recon: 0.6908, KL: 48.0023, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6964, Val Loss: 48.6823, Recon: 0.6923, KL: 48.0041, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7031, Val Loss: 48.6863, Recon: 0.7001, KL: 48.0029, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6990, Val Loss: 48.6823, Recon: 0.6894, KL: 48.0096, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6938, Val Loss: 48.6938, Recon: 0.6908, KL: 48.0030, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6883, Val Loss: 48.6794, Recon: 0.6860, KL: 48.0022, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6891, Val Loss: 48.6863, Recon: 0.6866, KL: 48.0024, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6836, Val Loss: 48.6915, Recon: 0.6822, KL: 48.0013, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7054, Val Loss: 48.7178, Recon: 0.6928, KL: 48.0126, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6950, Val Loss: 48.6830, Recon: 0.6924, KL: 48.0026, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6864, Val Loss: 48.6958, Recon: 0.6830, KL: 48.0034, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6908, Val Loss: 48.6886, Recon: 0.6801, KL: 48.0107, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6897, Val Loss: 48.6815, Recon: 0.6848, KL: 48.0049, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6852, Val Loss: 48.6846, Recon: 0.6778, KL: 48.0074, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6783, Val Loss: 48.6735, Recon: 0.6768, KL: 48.0015, KL_weight: 4.8000
Saved model 11 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_10.pt
Training bootstrap model 12/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9219, Val Loss: 0.6797, Recon: 0.9219, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6921, Val Loss: 4.6331, Recon: 0.8180, KL: 3.8741, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4576, Val Loss: 9.4004, Recon: 0.7985, KL: 8.6590, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2380, Val Loss: 14.2026, Recon: 0.7784, KL: 13.4595, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0223, Val Loss: 18.9879, Recon: 0.7706, KL: 18.2517, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8231, Val Loss: 23.7693, Recon: 0.7685, KL: 23.0546, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6082, Val Loss: 28.5704, Recon: 0.7554, KL: 27.8527, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4130, Val Loss: 33.3799, Recon: 0.7555, KL: 32.6575, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2087, Val Loss: 38.1918, Recon: 0.7580, KL: 37.4507, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9990, Val Loss: 42.9619, Recon: 0.7491, KL: 42.2498, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8062, Val Loss: 47.7704, Recon: 0.7575, KL: 47.0487, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7631, Val Loss: 48.7297, Recon: 0.7544, KL: 48.0087, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7462, Val Loss: 48.7125, Recon: 0.7406, KL: 48.0056, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7455, Val Loss: 48.7134, Recon: 0.7410, KL: 48.0045, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7505, Val Loss: 48.7163, Recon: 0.7425, KL: 48.0080, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7369, Val Loss: 48.7010, Recon: 0.7321, KL: 48.0048, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7412, Val Loss: 48.6986, Recon: 0.7352, KL: 48.0060, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7489, Val Loss: 48.7094, Recon: 0.7416, KL: 48.0074, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7308, Val Loss: 48.7058, Recon: 0.7224, KL: 48.0084, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7477, Val Loss: 48.7048, Recon: 0.7354, KL: 48.0122, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7247, Val Loss: 48.7004, Recon: 0.7199, KL: 48.0048, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7262, Val Loss: 48.6997, Recon: 0.7219, KL: 48.0043, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7272, Val Loss: 48.6988, Recon: 0.7215, KL: 48.0058, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7197, Val Loss: 48.6946, Recon: 0.7166, KL: 48.0031, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7180, Val Loss: 48.6930, Recon: 0.7124, KL: 48.0057, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7242, Val Loss: 48.6896, Recon: 0.7206, KL: 48.0036, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7142, Val Loss: 48.6935, Recon: 0.7115, KL: 48.0028, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7162, Val Loss: 48.6867, Recon: 0.7131, KL: 48.0031, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7147, Val Loss: 48.6917, Recon: 0.7118, KL: 48.0029, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7098, Val Loss: 48.6872, Recon: 0.7080, KL: 48.0019, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7096, Val Loss: 48.6941, Recon: 0.7040, KL: 48.0057, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7242, Val Loss: 48.7013, Recon: 0.7192, KL: 48.0050, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7107, Val Loss: 48.6898, Recon: 0.7078, KL: 48.0029, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7113, Val Loss: 48.6920, Recon: 0.7077, KL: 48.0036, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7046, Val Loss: 48.6826, Recon: 0.7030, KL: 48.0016, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7146, Val Loss: 48.6970, Recon: 0.7105, KL: 48.0042, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7071, Val Loss: 48.6875, Recon: 0.7048, KL: 48.0023, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7143, Val Loss: 48.6884, Recon: 0.7103, KL: 48.0040, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7086, Val Loss: 48.6837, Recon: 0.7017, KL: 48.0069, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7098, Val Loss: 48.6963, Recon: 0.7082, KL: 48.0016, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7041, Val Loss: 48.6895, Recon: 0.7023, KL: 48.0019, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7123, Val Loss: 48.7165, Recon: 0.7021, KL: 48.0101, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7266, Val Loss: 48.6864, Recon: 0.7198, KL: 48.0067, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7059, Val Loss: 48.7017, Recon: 0.7040, KL: 48.0019, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7015, Val Loss: 48.6810, Recon: 0.6990, KL: 48.0025, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7059, Val Loss: 48.6787, Recon: 0.7028, KL: 48.0032, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7011, Val Loss: 48.6821, Recon: 0.6987, KL: 48.0024, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7144, Val Loss: 48.7078, Recon: 0.7070, KL: 48.0074, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7033, Val Loss: 48.6838, Recon: 0.7011, KL: 48.0022, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7034, Val Loss: 48.6844, Recon: 0.6998, KL: 48.0036, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6963, Val Loss: 48.6796, Recon: 0.6943, KL: 48.0020, KL_weight: 4.8000
Saved model 12 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_11.pt
Training bootstrap model 13/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9203, Val Loss: 0.6848, Recon: 0.9203, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6944, Val Loss: 4.6384, Recon: 0.8205, KL: 3.8739, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4619, Val Loss: 9.4117, Recon: 0.7941, KL: 8.6678, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2346, Val Loss: 14.1969, Recon: 0.7776, KL: 13.4570, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0338, Val Loss: 18.9995, Recon: 0.7773, KL: 18.2565, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8306, Val Loss: 23.7925, Recon: 0.7669, KL: 23.0637, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6046, Val Loss: 28.5857, Recon: 0.7550, KL: 27.8496, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3956, Val Loss: 33.3831, Recon: 0.7475, KL: 32.6481, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1862, Val Loss: 38.1704, Recon: 0.7405, KL: 37.4457, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9810, Val Loss: 42.9515, Recon: 0.7332, KL: 42.2478, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7721, Val Loss: 47.7603, Recon: 0.7268, KL: 47.0453, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7365, Val Loss: 48.7118, Recon: 0.7322, KL: 48.0043, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7261, Val Loss: 48.7058, Recon: 0.7223, KL: 48.0037, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7338, Val Loss: 48.7385, Recon: 0.7269, KL: 48.0069, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7629, Val Loss: 48.7258, Recon: 0.7400, KL: 48.0229, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7242, Val Loss: 48.7149, Recon: 0.7204, KL: 48.0038, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7274, Val Loss: 48.7052, Recon: 0.7224, KL: 48.0050, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7216, Val Loss: 48.6961, Recon: 0.7117, KL: 48.0099, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7161, Val Loss: 48.6994, Recon: 0.7097, KL: 48.0064, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7095, Val Loss: 48.6909, Recon: 0.7062, KL: 48.0033, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7009, Val Loss: 48.7023, Recon: 0.6984, KL: 48.0025, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.6992, Val Loss: 48.6951, Recon: 0.6968, KL: 48.0024, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7770, Val Loss: 48.7233, Recon: 0.7265, KL: 48.0504, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7088, Val Loss: 48.6928, Recon: 0.7060, KL: 48.0028, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7151, Val Loss: 48.7034, Recon: 0.7071, KL: 48.0080, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7002, Val Loss: 48.6825, Recon: 0.6971, KL: 48.0030, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6993, Val Loss: 48.6857, Recon: 0.6956, KL: 48.0036, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6938, Val Loss: 48.6932, Recon: 0.6907, KL: 48.0031, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6882, Val Loss: 48.6951, Recon: 0.6851, KL: 48.0032, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7257, Val Loss: 48.7058, Recon: 0.7120, KL: 48.0137, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6976, Val Loss: 48.6853, Recon: 0.6937, KL: 48.0039, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6932, Val Loss: 48.6803, Recon: 0.6917, KL: 48.0015, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6986, Val Loss: 48.6960, Recon: 0.6955, KL: 48.0030, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6929, Val Loss: 48.6845, Recon: 0.6896, KL: 48.0033, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6865, Val Loss: 48.6807, Recon: 0.6847, KL: 48.0017, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6866, Val Loss: 48.7022, Recon: 0.6835, KL: 48.0030, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6936, Val Loss: 48.6929, Recon: 0.6898, KL: 48.0038, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6924, Val Loss: 48.6922, Recon: 0.6896, KL: 48.0027, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7103, Val Loss: 48.7169, Recon: 0.7013, KL: 48.0089, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7101, Val Loss: 48.6976, Recon: 0.6958, KL: 48.0143, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6997, Val Loss: 48.6820, Recon: 0.6973, KL: 48.0024, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6877, Val Loss: 48.6959, Recon: 0.6853, KL: 48.0024, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6856, Val Loss: 48.6907, Recon: 0.6826, KL: 48.0031, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6822, Val Loss: 48.6775, Recon: 0.6805, KL: 48.0016, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6810, Val Loss: 48.6847, Recon: 0.6775, KL: 48.0035, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6876, Val Loss: 48.6751, Recon: 0.6848, KL: 48.0029, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6819, Val Loss: 48.6808, Recon: 0.6792, KL: 48.0028, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6793, Val Loss: 48.6763, Recon: 0.6775, KL: 48.0018, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6816, Val Loss: 48.6780, Recon: 0.6795, KL: 48.0021, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7108, Val Loss: 48.6917, Recon: 0.7033, KL: 48.0075, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6925, Val Loss: 48.6846, Recon: 0.6894, KL: 48.0031, KL_weight: 4.8000
Saved model 13 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_12.pt
Training bootstrap model 14/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 1.0506, Val Loss: 0.7180, Recon: 1.0506, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7212, Val Loss: 4.6430, Recon: 0.8377, KL: 3.8835, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4580, Val Loss: 9.4138, Recon: 0.7980, KL: 8.6600, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2288, Val Loss: 14.2006, Recon: 0.7769, KL: 13.4519, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0517, Val Loss: 19.0002, Recon: 0.7886, KL: 18.2631, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8288, Val Loss: 23.7902, Recon: 0.7744, KL: 23.0544, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6302, Val Loss: 28.6256, Recon: 0.7768, KL: 27.8535, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4004, Val Loss: 33.3743, Recon: 0.7532, KL: 32.6472, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2010, Val Loss: 38.1600, Recon: 0.7505, KL: 37.4505, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0181, Val Loss: 42.9769, Recon: 0.7627, KL: 42.2554, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8096, Val Loss: 47.7985, Recon: 0.7575, KL: 47.0521, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7989, Val Loss: 48.7562, Recon: 0.7711, KL: 48.0279, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7662, Val Loss: 48.7222, Recon: 0.7519, KL: 48.0143, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7480, Val Loss: 48.7151, Recon: 0.7406, KL: 48.0074, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7434, Val Loss: 48.7126, Recon: 0.7381, KL: 48.0053, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7325, Val Loss: 48.7108, Recon: 0.7277, KL: 48.0048, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7447, Val Loss: 48.7225, Recon: 0.7345, KL: 48.0102, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7332, Val Loss: 48.7253, Recon: 0.7264, KL: 48.0068, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7387, Val Loss: 48.7024, Recon: 0.7288, KL: 48.0099, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7228, Val Loss: 48.7134, Recon: 0.7170, KL: 48.0058, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7278, Val Loss: 48.7001, Recon: 0.7233, KL: 48.0044, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7402, Val Loss: 48.7463, Recon: 0.7281, KL: 48.0121, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7214, Val Loss: 48.6898, Recon: 0.7189, KL: 48.0025, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7206, Val Loss: 48.7243, Recon: 0.7105, KL: 48.0101, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7115, Val Loss: 48.6924, Recon: 0.7091, KL: 48.0024, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7203, Val Loss: 48.6992, Recon: 0.7129, KL: 48.0074, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7189, Val Loss: 48.7014, Recon: 0.7056, KL: 48.0134, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7096, Val Loss: 48.6875, Recon: 0.7058, KL: 48.0038, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7233, Val Loss: 48.7032, Recon: 0.7157, KL: 48.0076, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7087, Val Loss: 48.6915, Recon: 0.7051, KL: 48.0036, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7138, Val Loss: 48.6934, Recon: 0.7095, KL: 48.0043, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7016, Val Loss: 48.7086, Recon: 0.6992, KL: 48.0025, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6952, Val Loss: 48.6892, Recon: 0.6926, KL: 48.0026, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7216, Val Loss: 48.6960, Recon: 0.7089, KL: 48.0127, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7120, Val Loss: 48.6831, Recon: 0.7058, KL: 48.0062, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7084, Val Loss: 48.6825, Recon: 0.6981, KL: 48.0103, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7126, Val Loss: 48.6964, Recon: 0.7037, KL: 48.0089, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7005, Val Loss: 48.6891, Recon: 0.6988, KL: 48.0017, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6999, Val Loss: 48.6910, Recon: 0.6955, KL: 48.0044, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7000, Val Loss: 48.6817, Recon: 0.6977, KL: 48.0023, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6956, Val Loss: 48.6812, Recon: 0.6906, KL: 48.0049, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6935, Val Loss: 48.6874, Recon: 0.6906, KL: 48.0029, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6952, Val Loss: 48.6821, Recon: 0.6903, KL: 48.0049, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7087, Val Loss: 48.6850, Recon: 0.7000, KL: 48.0086, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6999, Val Loss: 48.6808, Recon: 0.6961, KL: 48.0038, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6999, Val Loss: 48.6896, Recon: 0.6926, KL: 48.0073, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6924, Val Loss: 48.6815, Recon: 0.6884, KL: 48.0040, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6882, Val Loss: 48.6767, Recon: 0.6860, KL: 48.0022, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6838, Val Loss: 48.6736, Recon: 0.6817, KL: 48.0020, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6818, Val Loss: 48.6780, Recon: 0.6808, KL: 48.0010, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6907, Val Loss: 48.6813, Recon: 0.6875, KL: 48.0032, KL_weight: 4.8000
Saved model 14 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_13.pt
Training bootstrap model 15/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8913, Val Loss: 0.6720, Recon: 0.8913, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6937, Val Loss: 4.6219, Recon: 0.8139, KL: 3.8798, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4329, Val Loss: 9.4051, Recon: 0.7762, KL: 8.6567, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2363, Val Loss: 14.1747, Recon: 0.7773, KL: 13.4590, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0086, Val Loss: 18.9795, Recon: 0.7559, KL: 18.2528, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7834, Val Loss: 23.7594, Recon: 0.7372, KL: 23.0462, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5915, Val Loss: 28.5704, Recon: 0.7399, KL: 27.8516, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3979, Val Loss: 33.3805, Recon: 0.7419, KL: 32.6560, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1901, Val Loss: 38.1692, Recon: 0.7363, KL: 37.4538, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9694, Val Loss: 42.9649, Recon: 0.7234, KL: 42.2459, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7715, Val Loss: 47.7523, Recon: 0.7280, KL: 47.0435, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7385, Val Loss: 48.7179, Recon: 0.7330, KL: 48.0055, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7248, Val Loss: 48.7245, Recon: 0.7198, KL: 48.0051, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7382, Val Loss: 48.7321, Recon: 0.7258, KL: 48.0124, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7307, Val Loss: 48.7225, Recon: 0.7237, KL: 48.0069, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7217, Val Loss: 48.7110, Recon: 0.7157, KL: 48.0061, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7396, Val Loss: 48.7116, Recon: 0.7342, KL: 48.0054, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7196, Val Loss: 48.7154, Recon: 0.7152, KL: 48.0044, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7148, Val Loss: 48.6948, Recon: 0.7105, KL: 48.0043, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7053, Val Loss: 48.7057, Recon: 0.7022, KL: 48.0031, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7264, Val Loss: 48.7261, Recon: 0.7176, KL: 48.0088, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7027, Val Loss: 48.6929, Recon: 0.7003, KL: 48.0024, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7053, Val Loss: 48.7006, Recon: 0.7019, KL: 48.0034, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7088, Val Loss: 48.7154, Recon: 0.7046, KL: 48.0042, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7047, Val Loss: 48.6946, Recon: 0.7028, KL: 48.0019, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7181, Val Loss: 48.7100, Recon: 0.7121, KL: 48.0060, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6999, Val Loss: 48.6958, Recon: 0.6974, KL: 48.0025, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7026, Val Loss: 48.7060, Recon: 0.6985, KL: 48.0041, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7116, Val Loss: 48.6953, Recon: 0.7089, KL: 48.0027, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7052, Val Loss: 48.6982, Recon: 0.7009, KL: 48.0044, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7023, Val Loss: 48.6951, Recon: 0.6997, KL: 48.0026, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7022, Val Loss: 48.7091, Recon: 0.6981, KL: 48.0042, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7014, Val Loss: 48.6939, Recon: 0.6970, KL: 48.0044, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6865, Val Loss: 48.6807, Recon: 0.6843, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6945, Val Loss: 48.6887, Recon: 0.6920, KL: 48.0025, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7098, Val Loss: 48.6939, Recon: 0.7016, KL: 48.0082, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7075, Val Loss: 48.6921, Recon: 0.7017, KL: 48.0058, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6951, Val Loss: 48.6881, Recon: 0.6932, KL: 48.0020, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6913, Val Loss: 48.6944, Recon: 0.6896, KL: 48.0018, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6879, Val Loss: 48.6966, Recon: 0.6864, KL: 48.0016, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6853, Val Loss: 48.6814, Recon: 0.6834, KL: 48.0019, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7010, Val Loss: 48.6959, Recon: 0.6936, KL: 48.0074, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6909, Val Loss: 48.6839, Recon: 0.6887, KL: 48.0022, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6863, Val Loss: 48.6798, Recon: 0.6852, KL: 48.0010, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6896, Val Loss: 48.6831, Recon: 0.6876, KL: 48.0020, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6922, Val Loss: 48.6940, Recon: 0.6897, KL: 48.0025, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6934, Val Loss: 48.6822, Recon: 0.6896, KL: 48.0038, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6917, Val Loss: 48.6847, Recon: 0.6878, KL: 48.0039, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6846, Val Loss: 48.6758, Recon: 0.6820, KL: 48.0026, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6962, Val Loss: 48.6829, Recon: 0.6860, KL: 48.0102, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6881, Val Loss: 48.6849, Recon: 0.6855, KL: 48.0026, KL_weight: 4.8000
Saved model 15 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_14.pt
Training bootstrap model 16/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9148, Val Loss: 0.6958, Recon: 0.9148, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6891, Val Loss: 4.5985, Recon: 0.8134, KL: 3.8756, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4435, Val Loss: 9.4067, Recon: 0.7879, KL: 8.6557, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2282, Val Loss: 14.2051, Recon: 0.7729, KL: 13.4553, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0217, Val Loss: 18.9934, Recon: 0.7668, KL: 18.2549, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8032, Val Loss: 23.7848, Recon: 0.7561, KL: 23.0470, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5971, Val Loss: 28.5612, Recon: 0.7484, KL: 27.8487, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4099, Val Loss: 33.3763, Recon: 0.7543, KL: 32.6556, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1889, Val Loss: 38.1835, Recon: 0.7397, KL: 37.4492, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9879, Val Loss: 42.9807, Recon: 0.7433, KL: 42.2446, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7911, Val Loss: 47.7703, Recon: 0.7422, KL: 47.0489, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7778, Val Loss: 48.7491, Recon: 0.7421, KL: 48.0357, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7560, Val Loss: 48.7451, Recon: 0.7471, KL: 48.0089, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7623, Val Loss: 48.7245, Recon: 0.7402, KL: 48.0220, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7450, Val Loss: 48.7271, Recon: 0.7343, KL: 48.0107, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7522, Val Loss: 48.7264, Recon: 0.7354, KL: 48.0168, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7370, Val Loss: 48.7344, Recon: 0.7305, KL: 48.0065, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7193, Val Loss: 48.7097, Recon: 0.7156, KL: 48.0037, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7264, Val Loss: 48.7114, Recon: 0.7208, KL: 48.0055, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7127, Val Loss: 48.7079, Recon: 0.7096, KL: 48.0032, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.8301, Val Loss: 48.7494, Recon: 0.7596, KL: 48.0706, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7317, Val Loss: 48.7248, Recon: 0.7171, KL: 48.0146, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7185, Val Loss: 48.7055, Recon: 0.7130, KL: 48.0055, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7131, Val Loss: 48.7109, Recon: 0.7089, KL: 48.0042, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7128, Val Loss: 48.7004, Recon: 0.7095, KL: 48.0033, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7169, Val Loss: 48.7066, Recon: 0.7121, KL: 48.0048, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7088, Val Loss: 48.7063, Recon: 0.7052, KL: 48.0036, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7043, Val Loss: 48.6999, Recon: 0.7007, KL: 48.0036, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7090, Val Loss: 48.7001, Recon: 0.7063, KL: 48.0027, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7074, Val Loss: 48.6941, Recon: 0.7041, KL: 48.0034, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.8841, Val Loss: 48.7824, Recon: 0.8210, KL: 48.0631, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7587, Val Loss: 48.7218, Recon: 0.7480, KL: 48.0107, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7686, Val Loss: 48.7297, Recon: 0.7431, KL: 48.0254, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7405, Val Loss: 48.7030, Recon: 0.7315, KL: 48.0090, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7181, Val Loss: 48.7040, Recon: 0.7118, KL: 48.0063, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7226, Val Loss: 48.6960, Recon: 0.7186, KL: 48.0039, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7238, Val Loss: 48.6907, Recon: 0.7181, KL: 48.0057, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7106, Val Loss: 48.6974, Recon: 0.7056, KL: 48.0050, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7211, Val Loss: 48.7050, Recon: 0.7156, KL: 48.0055, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7035, Val Loss: 48.6960, Recon: 0.7012, KL: 48.0024, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6994, Val Loss: 48.7022, Recon: 0.6973, KL: 48.0021, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6971, Val Loss: 48.6938, Recon: 0.6950, KL: 48.0022, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7099, Val Loss: 48.6932, Recon: 0.7069, KL: 48.0030, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7055, Val Loss: 48.6900, Recon: 0.7013, KL: 48.0042, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7041, Val Loss: 48.6856, Recon: 0.6994, KL: 48.0047, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6978, Val Loss: 48.7062, Recon: 0.6950, KL: 48.0028, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6917, Val Loss: 48.6931, Recon: 0.6892, KL: 48.0025, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7090, Val Loss: 48.7146, Recon: 0.7012, KL: 48.0078, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7048, Val Loss: 48.6874, Recon: 0.7025, KL: 48.0023, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7076, Val Loss: 48.6898, Recon: 0.7044, KL: 48.0032, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6961, Val Loss: 48.6984, Recon: 0.6930, KL: 48.0031, KL_weight: 4.8000
Saved model 16 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_15.pt
Training bootstrap model 17/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9000, Val Loss: 0.6868, Recon: 0.9000, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6795, Val Loss: 4.6265, Recon: 0.8076, KL: 3.8720, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4451, Val Loss: 9.3923, Recon: 0.7835, KL: 8.6616, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2424, Val Loss: 14.2057, Recon: 0.7839, KL: 13.4585, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0212, Val Loss: 18.9917, Recon: 0.7728, KL: 18.2484, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8378, Val Loss: 23.7978, Recon: 0.7704, KL: 23.0674, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6057, Val Loss: 28.6037, Recon: 0.7544, KL: 27.8513, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4022, Val Loss: 33.4110, Recon: 0.7496, KL: 32.6526, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2308, Val Loss: 38.1887, Recon: 0.7544, KL: 37.4764, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9913, Val Loss: 42.9629, Recon: 0.7430, KL: 42.2483, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7815, Val Loss: 47.7638, Recon: 0.7353, KL: 47.0462, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7456, Val Loss: 48.7178, Recon: 0.7407, KL: 48.0050, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7455, Val Loss: 48.7105, Recon: 0.7382, KL: 48.0073, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7244, Val Loss: 48.7058, Recon: 0.7197, KL: 48.0046, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7564, Val Loss: 48.7189, Recon: 0.7337, KL: 48.0227, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7282, Val Loss: 48.7104, Recon: 0.7227, KL: 48.0055, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7288, Val Loss: 48.7116, Recon: 0.7249, KL: 48.0039, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7176, Val Loss: 48.7024, Recon: 0.7104, KL: 48.0071, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7155, Val Loss: 48.6983, Recon: 0.7110, KL: 48.0045, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7160, Val Loss: 48.6992, Recon: 0.7128, KL: 48.0032, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7193, Val Loss: 48.7094, Recon: 0.7132, KL: 48.0061, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7045, Val Loss: 48.7040, Recon: 0.7035, KL: 48.0010, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7055, Val Loss: 48.6949, Recon: 0.7036, KL: 48.0019, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7152, Val Loss: 48.6941, Recon: 0.7108, KL: 48.0044, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7276, Val Loss: 48.7004, Recon: 0.7224, KL: 48.0053, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7096, Val Loss: 48.6942, Recon: 0.7039, KL: 48.0057, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7086, Val Loss: 48.7065, Recon: 0.7034, KL: 48.0052, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7184, Val Loss: 48.6939, Recon: 0.7094, KL: 48.0090, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7140, Val Loss: 48.6984, Recon: 0.7048, KL: 48.0093, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6959, Val Loss: 48.6930, Recon: 0.6941, KL: 48.0018, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6997, Val Loss: 48.6860, Recon: 0.6947, KL: 48.0050, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7032, Val Loss: 48.6952, Recon: 0.7005, KL: 48.0026, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7005, Val Loss: 48.6985, Recon: 0.6962, KL: 48.0043, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7078, Val Loss: 48.6998, Recon: 0.7018, KL: 48.0060, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7003, Val Loss: 48.6875, Recon: 0.6957, KL: 48.0046, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7044, Val Loss: 48.7046, Recon: 0.6996, KL: 48.0048, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6934, Val Loss: 48.7327, Recon: 0.6912, KL: 48.0022, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6999, Val Loss: 48.6972, Recon: 0.6961, KL: 48.0038, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7030, Val Loss: 48.6860, Recon: 0.6971, KL: 48.0059, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6991, Val Loss: 48.6953, Recon: 0.6958, KL: 48.0034, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6927, Val Loss: 48.6848, Recon: 0.6912, KL: 48.0015, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6951, Val Loss: 48.6912, Recon: 0.6919, KL: 48.0032, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6965, Val Loss: 48.6899, Recon: 0.6952, KL: 48.0013, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6997, Val Loss: 48.6843, Recon: 0.6949, KL: 48.0048, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7031, Val Loss: 48.6881, Recon: 0.7005, KL: 48.0025, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7032, Val Loss: 48.6788, Recon: 0.6992, KL: 48.0040, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6987, Val Loss: 48.6958, Recon: 0.6966, KL: 48.0020, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6925, Val Loss: 48.6956, Recon: 0.6882, KL: 48.0043, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6932, Val Loss: 48.6856, Recon: 0.6919, KL: 48.0013, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7017, Val Loss: 48.6967, Recon: 0.6955, KL: 48.0062, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7113, Val Loss: 48.6895, Recon: 0.7060, KL: 48.0053, KL_weight: 4.8000
Saved model 17 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_16.pt
Training bootstrap model 18/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9176, Val Loss: 0.6795, Recon: 0.9176, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7139, Val Loss: 4.6523, Recon: 0.8332, KL: 3.8807, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4589, Val Loss: 9.4100, Recon: 0.7944, KL: 8.6645, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2402, Val Loss: 14.2078, Recon: 0.7804, KL: 13.4598, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0278, Val Loss: 18.9868, Recon: 0.7730, KL: 18.2548, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8212, Val Loss: 23.7981, Recon: 0.7677, KL: 23.0535, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6042, Val Loss: 28.5759, Recon: 0.7510, KL: 27.8532, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4093, Val Loss: 33.3695, Recon: 0.7525, KL: 32.6568, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1891, Val Loss: 38.1680, Recon: 0.7413, KL: 37.4478, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0310, Val Loss: 42.9645, Recon: 0.7671, KL: 42.2639, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7935, Val Loss: 47.7607, Recon: 0.7441, KL: 47.0494, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7505, Val Loss: 48.7212, Recon: 0.7438, KL: 48.0067, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7416, Val Loss: 48.7201, Recon: 0.7360, KL: 48.0056, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7371, Val Loss: 48.7001, Recon: 0.7332, KL: 48.0040, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7404, Val Loss: 48.7220, Recon: 0.7342, KL: 48.0063, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7355, Val Loss: 48.7192, Recon: 0.7310, KL: 48.0046, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7311, Val Loss: 48.7025, Recon: 0.7272, KL: 48.0039, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7350, Val Loss: 48.7069, Recon: 0.7254, KL: 48.0096, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7363, Val Loss: 48.7166, Recon: 0.7279, KL: 48.0084, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7202, Val Loss: 48.7009, Recon: 0.7156, KL: 48.0045, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7209, Val Loss: 48.7150, Recon: 0.7182, KL: 48.0027, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7217, Val Loss: 48.7056, Recon: 0.7178, KL: 48.0039, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7193, Val Loss: 48.6965, Recon: 0.7166, KL: 48.0027, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7510, Val Loss: 48.7430, Recon: 0.7294, KL: 48.0216, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7170, Val Loss: 48.6956, Recon: 0.7120, KL: 48.0050, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7200, Val Loss: 48.6913, Recon: 0.7173, KL: 48.0028, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7130, Val Loss: 48.6976, Recon: 0.7079, KL: 48.0051, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7146, Val Loss: 48.6954, Recon: 0.7108, KL: 48.0039, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7106, Val Loss: 48.6909, Recon: 0.7085, KL: 48.0022, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7061, Val Loss: 48.6846, Recon: 0.7051, KL: 48.0010, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7193, Val Loss: 48.6972, Recon: 0.7165, KL: 48.0028, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7050, Val Loss: 48.6973, Recon: 0.7020, KL: 48.0030, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7071, Val Loss: 48.6987, Recon: 0.7034, KL: 48.0037, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7230, Val Loss: 48.6997, Recon: 0.7130, KL: 48.0100, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7127, Val Loss: 48.7022, Recon: 0.7092, KL: 48.0035, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7347, Val Loss: 48.7038, Recon: 0.7155, KL: 48.0192, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7077, Val Loss: 48.6881, Recon: 0.7048, KL: 48.0030, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7018, Val Loss: 48.6904, Recon: 0.7006, KL: 48.0012, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7041, Val Loss: 48.6837, Recon: 0.7027, KL: 48.0014, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7017, Val Loss: 48.6940, Recon: 0.6995, KL: 48.0022, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6966, Val Loss: 48.6855, Recon: 0.6943, KL: 48.0023, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7149, Val Loss: 48.7075, Recon: 0.7089, KL: 48.0060, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6962, Val Loss: 48.6941, Recon: 0.6942, KL: 48.0020, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7002, Val Loss: 48.6791, Recon: 0.6982, KL: 48.0020, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7072, Val Loss: 48.6865, Recon: 0.7034, KL: 48.0038, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7021, Val Loss: 48.6844, Recon: 0.6944, KL: 48.0077, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7034, Val Loss: 48.6841, Recon: 0.7007, KL: 48.0026, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6993, Val Loss: 48.6892, Recon: 0.6961, KL: 48.0032, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7083, Val Loss: 48.6886, Recon: 0.7031, KL: 48.0052, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6996, Val Loss: 48.6857, Recon: 0.6978, KL: 48.0018, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7216, Val Loss: 48.6994, Recon: 0.7135, KL: 48.0081, KL_weight: 4.8000
Saved model 18 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_17.pt
Training bootstrap model 19/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8921, Val Loss: 0.6912, Recon: 0.8921, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6709, Val Loss: 4.6185, Recon: 0.8018, KL: 3.8691, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4397, Val Loss: 9.4168, Recon: 0.7823, KL: 8.6575, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2304, Val Loss: 14.1865, Recon: 0.7700, KL: 13.4604, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0148, Val Loss: 18.9877, Recon: 0.7600, KL: 18.2548, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7918, Val Loss: 23.7708, Recon: 0.7455, KL: 23.0463, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6081, Val Loss: 28.5675, Recon: 0.7505, KL: 27.8577, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3786, Val Loss: 33.3745, Recon: 0.7310, KL: 32.6476, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1865, Val Loss: 38.1756, Recon: 0.7354, KL: 37.4512, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9795, Val Loss: 42.9535, Recon: 0.7347, KL: 42.2447, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7871, Val Loss: 47.7629, Recon: 0.7350, KL: 47.0521, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7241, Val Loss: 48.7222, Recon: 0.7215, KL: 48.0026, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7494, Val Loss: 48.7345, Recon: 0.7345, KL: 48.0149, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7400, Val Loss: 48.7029, Recon: 0.7326, KL: 48.0074, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7206, Val Loss: 48.7022, Recon: 0.7166, KL: 48.0039, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7248, Val Loss: 48.7180, Recon: 0.7186, KL: 48.0062, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7147, Val Loss: 48.7049, Recon: 0.7119, KL: 48.0028, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7217, Val Loss: 48.7004, Recon: 0.7154, KL: 48.0063, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7758, Val Loss: 48.7473, Recon: 0.7271, KL: 48.0488, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7127, Val Loss: 48.7079, Recon: 0.7074, KL: 48.0054, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7058, Val Loss: 48.7110, Recon: 0.7021, KL: 48.0038, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7142, Val Loss: 48.7194, Recon: 0.7101, KL: 48.0041, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7027, Val Loss: 48.6962, Recon: 0.7002, KL: 48.0024, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7048, Val Loss: 48.7084, Recon: 0.6980, KL: 48.0068, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7185, Val Loss: 48.6931, Recon: 0.7135, KL: 48.0050, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7060, Val Loss: 48.7000, Recon: 0.7014, KL: 48.0046, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7052, Val Loss: 48.7335, Recon: 0.6984, KL: 48.0068, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6977, Val Loss: 48.6964, Recon: 0.6950, KL: 48.0028, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7039, Val Loss: 48.6997, Recon: 0.7011, KL: 48.0028, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6933, Val Loss: 48.6873, Recon: 0.6908, KL: 48.0024, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6944, Val Loss: 48.6870, Recon: 0.6924, KL: 48.0020, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7015, Val Loss: 48.6996, Recon: 0.6988, KL: 48.0027, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7030, Val Loss: 48.6848, Recon: 0.6974, KL: 48.0056, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6957, Val Loss: 48.6830, Recon: 0.6902, KL: 48.0055, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6959, Val Loss: 48.6913, Recon: 0.6920, KL: 48.0039, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6908, Val Loss: 48.6852, Recon: 0.6877, KL: 48.0031, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6840, Val Loss: 48.6877, Recon: 0.6818, KL: 48.0022, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6882, Val Loss: 48.6960, Recon: 0.6862, KL: 48.0019, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7047, Val Loss: 48.7010, Recon: 0.6973, KL: 48.0075, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7062, Val Loss: 48.6949, Recon: 0.6997, KL: 48.0064, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6929, Val Loss: 48.6948, Recon: 0.6875, KL: 48.0054, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6859, Val Loss: 48.6851, Recon: 0.6824, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6845, Val Loss: 48.6796, Recon: 0.6811, KL: 48.0034, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7939, Val Loss: 48.6893, Recon: 0.7029, KL: 48.0909, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6859, Val Loss: 48.6922, Recon: 0.6817, KL: 48.0042, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6872, Val Loss: 48.6971, Recon: 0.6825, KL: 48.0047, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6790, Val Loss: 48.6665, Recon: 0.6771, KL: 48.0018, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6851, Val Loss: 48.6750, Recon: 0.6807, KL: 48.0044, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6982, Val Loss: 48.6848, Recon: 0.6942, KL: 48.0040, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6822, Val Loss: 48.6806, Recon: 0.6785, KL: 48.0037, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6944, Val Loss: 48.6784, Recon: 0.6871, KL: 48.0073, KL_weight: 4.8000
Saved model 19 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_18.pt
Training bootstrap model 20/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9661, Val Loss: 0.6974, Recon: 0.9661, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7098, Val Loss: 4.6361, Recon: 0.8297, KL: 3.8801, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4676, Val Loss: 9.4212, Recon: 0.8027, KL: 8.6649, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2342, Val Loss: 14.1866, Recon: 0.7823, KL: 13.4519, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0532, Val Loss: 19.0120, Recon: 0.7891, KL: 18.2641, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8159, Val Loss: 23.7808, Recon: 0.7646, KL: 23.0513, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6597, Val Loss: 28.5975, Recon: 0.7997, KL: 27.8600, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4116, Val Loss: 33.3806, Recon: 0.7600, KL: 32.6515, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2116, Val Loss: 38.1866, Recon: 0.7539, KL: 37.4577, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9991, Val Loss: 42.9750, Recon: 0.7537, KL: 42.2454, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7975, Val Loss: 47.7637, Recon: 0.7506, KL: 47.0469, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7741, Val Loss: 48.7341, Recon: 0.7571, KL: 48.0170, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7460, Val Loss: 48.7243, Recon: 0.7370, KL: 48.0090, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7578, Val Loss: 48.7330, Recon: 0.7449, KL: 48.0129, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7424, Val Loss: 48.7222, Recon: 0.7387, KL: 48.0038, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7364, Val Loss: 48.7012, Recon: 0.7318, KL: 48.0046, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7288, Val Loss: 48.7069, Recon: 0.7241, KL: 48.0048, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7367, Val Loss: 48.7146, Recon: 0.7296, KL: 48.0071, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7389, Val Loss: 48.7061, Recon: 0.7365, KL: 48.0023, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7231, Val Loss: 48.7125, Recon: 0.7184, KL: 48.0047, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7394, Val Loss: 48.7086, Recon: 0.7318, KL: 48.0076, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7389, Val Loss: 48.7051, Recon: 0.7334, KL: 48.0055, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7203, Val Loss: 48.6933, Recon: 0.7153, KL: 48.0050, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7413, Val Loss: 48.7229, Recon: 0.7219, KL: 48.0194, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7253, Val Loss: 48.7022, Recon: 0.7190, KL: 48.0064, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7273, Val Loss: 48.6995, Recon: 0.7199, KL: 48.0074, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7261, Val Loss: 48.7004, Recon: 0.7221, KL: 48.0040, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7615, Val Loss: 48.7149, Recon: 0.7341, KL: 48.0274, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7371, Val Loss: 48.7022, Recon: 0.7325, KL: 48.0046, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7289, Val Loss: 48.7142, Recon: 0.7204, KL: 48.0085, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7186, Val Loss: 48.7008, Recon: 0.7118, KL: 48.0067, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7197, Val Loss: 48.7037, Recon: 0.7103, KL: 48.0095, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7188, Val Loss: 48.6996, Recon: 0.7101, KL: 48.0086, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7047, Val Loss: 48.6911, Recon: 0.7021, KL: 48.0026, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7139, Val Loss: 48.6870, Recon: 0.7078, KL: 48.0061, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7130, Val Loss: 48.7058, Recon: 0.7102, KL: 48.0028, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7087, Val Loss: 48.7065, Recon: 0.7051, KL: 48.0036, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7072, Val Loss: 48.6960, Recon: 0.7041, KL: 48.0031, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7004, Val Loss: 48.6912, Recon: 0.6980, KL: 48.0024, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7033, Val Loss: 48.6861, Recon: 0.7004, KL: 48.0029, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7112, Val Loss: 48.7006, Recon: 0.7075, KL: 48.0038, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7287, Val Loss: 48.6938, Recon: 0.7058, KL: 48.0230, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7205, Val Loss: 48.6897, Recon: 0.7167, KL: 48.0038, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7013, Val Loss: 48.6835, Recon: 0.6983, KL: 48.0030, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7116, Val Loss: 48.6836, Recon: 0.6978, KL: 48.0138, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7025, Val Loss: 48.6882, Recon: 0.6984, KL: 48.0041, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6969, Val Loss: 48.6826, Recon: 0.6951, KL: 48.0017, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7019, Val Loss: 48.6944, Recon: 0.6987, KL: 48.0032, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7030, Val Loss: 48.6827, Recon: 0.6983, KL: 48.0047, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6995, Val Loss: 48.6778, Recon: 0.6973, KL: 48.0022, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7017, Val Loss: 48.6843, Recon: 0.6981, KL: 48.0036, KL_weight: 4.8000
Saved model 20 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_19.pt
Training bootstrap model 21/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9432, Val Loss: 0.6899, Recon: 0.9432, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7017, Val Loss: 4.6236, Recon: 0.8241, KL: 3.8777, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4445, Val Loss: 9.4080, Recon: 0.7846, KL: 8.6599, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2426, Val Loss: 14.1956, Recon: 0.7827, KL: 13.4599, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0237, Val Loss: 18.9948, Recon: 0.7663, KL: 18.2574, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8359, Val Loss: 23.7739, Recon: 0.7786, KL: 23.0573, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6154, Val Loss: 28.5669, Recon: 0.7644, KL: 27.8510, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3978, Val Loss: 33.3701, Recon: 0.7465, KL: 32.6514, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2028, Val Loss: 38.1729, Recon: 0.7559, KL: 37.4470, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9812, Val Loss: 42.9664, Recon: 0.7346, KL: 42.2467, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8098, Val Loss: 47.7697, Recon: 0.7435, KL: 47.0663, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7467, Val Loss: 48.7172, Recon: 0.7383, KL: 48.0085, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7335, Val Loss: 48.7100, Recon: 0.7283, KL: 48.0053, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7503, Val Loss: 48.7241, Recon: 0.7408, KL: 48.0096, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7453, Val Loss: 48.7225, Recon: 0.7371, KL: 48.0082, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7208, Val Loss: 48.7168, Recon: 0.7160, KL: 48.0048, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7231, Val Loss: 48.6985, Recon: 0.7207, KL: 48.0024, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7184, Val Loss: 48.7090, Recon: 0.7145, KL: 48.0040, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7386, Val Loss: 48.7075, Recon: 0.7207, KL: 48.0179, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7155, Val Loss: 48.7083, Recon: 0.7130, KL: 48.0026, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7136, Val Loss: 48.7012, Recon: 0.7095, KL: 48.0041, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7114, Val Loss: 48.6878, Recon: 0.7094, KL: 48.0020, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7132, Val Loss: 48.6949, Recon: 0.7115, KL: 48.0017, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7172, Val Loss: 48.7057, Recon: 0.7126, KL: 48.0047, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7152, Val Loss: 48.6951, Recon: 0.7118, KL: 48.0034, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7077, Val Loss: 48.6893, Recon: 0.7057, KL: 48.0019, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7121, Val Loss: 48.6906, Recon: 0.7090, KL: 48.0031, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7146, Val Loss: 48.6986, Recon: 0.7054, KL: 48.0093, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7057, Val Loss: 48.6902, Recon: 0.7024, KL: 48.0033, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7143, Val Loss: 48.6955, Recon: 0.7111, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7042, Val Loss: 48.6938, Recon: 0.6992, KL: 48.0050, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7049, Val Loss: 48.6890, Recon: 0.7029, KL: 48.0019, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7026, Val Loss: 48.6931, Recon: 0.6988, KL: 48.0037, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6957, Val Loss: 48.6828, Recon: 0.6937, KL: 48.0020, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7000, Val Loss: 48.6895, Recon: 0.6961, KL: 48.0039, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7019, Val Loss: 48.6828, Recon: 0.6981, KL: 48.0038, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6939, Val Loss: 48.6833, Recon: 0.6917, KL: 48.0023, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6931, Val Loss: 48.7067, Recon: 0.6903, KL: 48.0028, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6959, Val Loss: 48.6822, Recon: 0.6938, KL: 48.0021, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6968, Val Loss: 48.6869, Recon: 0.6932, KL: 48.0036, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7107, Val Loss: 48.7137, Recon: 0.7045, KL: 48.0062, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6989, Val Loss: 48.6850, Recon: 0.6933, KL: 48.0057, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6904, Val Loss: 48.6782, Recon: 0.6880, KL: 48.0024, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7016, Val Loss: 48.7118, Recon: 0.6964, KL: 48.0052, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7086, Val Loss: 48.6999, Recon: 0.7017, KL: 48.0069, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6900, Val Loss: 48.6895, Recon: 0.6872, KL: 48.0028, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7006, Val Loss: 48.6810, Recon: 0.6966, KL: 48.0040, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6863, Val Loss: 48.6740, Recon: 0.6840, KL: 48.0023, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6894, Val Loss: 48.6760, Recon: 0.6865, KL: 48.0029, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6872, Val Loss: 48.6781, Recon: 0.6854, KL: 48.0018, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6821, Val Loss: 48.6754, Recon: 0.6801, KL: 48.0020, KL_weight: 4.8000
Saved model 21 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_20.pt
Training bootstrap model 22/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9499, Val Loss: 0.6861, Recon: 0.9499, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7437, Val Loss: 4.6522, Recon: 0.8530, KL: 3.8908, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4934, Val Loss: 9.4231, Recon: 0.8297, KL: 8.6636, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2631, Val Loss: 14.2058, Recon: 0.8006, KL: 13.4625, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0491, Val Loss: 18.9942, Recon: 0.7931, KL: 18.2560, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8489, Val Loss: 23.7924, Recon: 0.7920, KL: 23.0570, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6205, Val Loss: 28.5618, Recon: 0.7730, KL: 27.8474, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4276, Val Loss: 33.3809, Recon: 0.7735, KL: 32.6541, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2385, Val Loss: 38.1917, Recon: 0.7812, KL: 37.4573, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0179, Val Loss: 42.9796, Recon: 0.7693, KL: 42.2486, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8073, Val Loss: 47.7743, Recon: 0.7534, KL: 47.0539, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7774, Val Loss: 48.7493, Recon: 0.7655, KL: 48.0118, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7636, Val Loss: 48.7360, Recon: 0.7576, KL: 48.0060, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.8804, Val Loss: 48.7332, Recon: 0.7631, KL: 48.1174, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7763, Val Loss: 48.7352, Recon: 0.7589, KL: 48.0174, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7538, Val Loss: 48.7154, Recon: 0.7484, KL: 48.0054, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7533, Val Loss: 48.7215, Recon: 0.7464, KL: 48.0068, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7576, Val Loss: 48.7148, Recon: 0.7466, KL: 48.0109, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7592, Val Loss: 48.7132, Recon: 0.7484, KL: 48.0108, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7365, Val Loss: 48.7096, Recon: 0.7329, KL: 48.0037, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7412, Val Loss: 48.7149, Recon: 0.7354, KL: 48.0058, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7535, Val Loss: 48.7137, Recon: 0.7420, KL: 48.0115, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7290, Val Loss: 48.6980, Recon: 0.7247, KL: 48.0043, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7564, Val Loss: 48.7088, Recon: 0.7462, KL: 48.0102, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7395, Val Loss: 48.7094, Recon: 0.7304, KL: 48.0092, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7495, Val Loss: 48.7265, Recon: 0.7408, KL: 48.0086, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7307, Val Loss: 48.7049, Recon: 0.7246, KL: 48.0061, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7336, Val Loss: 48.7055, Recon: 0.7273, KL: 48.0062, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7315, Val Loss: 48.7095, Recon: 0.7272, KL: 48.0043, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7274, Val Loss: 48.7056, Recon: 0.7203, KL: 48.0071, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7205, Val Loss: 48.6930, Recon: 0.7177, KL: 48.0028, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7431, Val Loss: 48.7164, Recon: 0.7334, KL: 48.0097, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7273, Val Loss: 48.7103, Recon: 0.7222, KL: 48.0051, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7316, Val Loss: 48.7260, Recon: 0.7242, KL: 48.0074, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7261, Val Loss: 48.7022, Recon: 0.7233, KL: 48.0028, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7132, Val Loss: 48.7039, Recon: 0.7098, KL: 48.0034, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7131, Val Loss: 48.6953, Recon: 0.7084, KL: 48.0047, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7164, Val Loss: 48.6904, Recon: 0.7126, KL: 48.0039, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7119, Val Loss: 48.7001, Recon: 0.7069, KL: 48.0050, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7072, Val Loss: 48.7046, Recon: 0.7027, KL: 48.0046, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7114, Val Loss: 48.7011, Recon: 0.7092, KL: 48.0022, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7110, Val Loss: 48.6824, Recon: 0.7076, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7095, Val Loss: 48.6955, Recon: 0.7064, KL: 48.0031, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6995, Val Loss: 48.6873, Recon: 0.6977, KL: 48.0018, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7176, Val Loss: 48.6902, Recon: 0.7110, KL: 48.0066, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7069, Val Loss: 48.6935, Recon: 0.7045, KL: 48.0024, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7059, Val Loss: 48.6872, Recon: 0.7035, KL: 48.0025, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7043, Val Loss: 48.7031, Recon: 0.6973, KL: 48.0070, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7029, Val Loss: 48.6899, Recon: 0.7001, KL: 48.0028, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7082, Val Loss: 48.6991, Recon: 0.6976, KL: 48.0106, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7062, Val Loss: 48.7130, Recon: 0.7000, KL: 48.0063, KL_weight: 4.8000
Saved model 22 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_21.pt
Training bootstrap model 23/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9036, Val Loss: 0.6800, Recon: 0.9036, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7333, Val Loss: 4.6760, Recon: 0.8399, KL: 3.8934, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4865, Val Loss: 9.4508, Recon: 0.8173, KL: 8.6692, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2689, Val Loss: 14.2202, Recon: 0.8063, KL: 13.4626, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0225, Val Loss: 18.9978, Recon: 0.7744, KL: 18.2481, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8396, Val Loss: 23.8005, Recon: 0.7780, KL: 23.0616, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6200, Val Loss: 28.5975, Recon: 0.7681, KL: 27.8519, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4121, Val Loss: 33.3743, Recon: 0.7632, KL: 32.6489, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2172, Val Loss: 38.2052, Recon: 0.7624, KL: 37.4549, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9991, Val Loss: 42.9786, Recon: 0.7462, KL: 42.2529, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8145, Val Loss: 47.7864, Recon: 0.7671, KL: 47.0475, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7388, Val Loss: 48.7216, Recon: 0.7353, KL: 48.0035, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7338, Val Loss: 48.7213, Recon: 0.7293, KL: 48.0045, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7627, Val Loss: 48.7564, Recon: 0.7512, KL: 48.0115, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7298, Val Loss: 48.7210, Recon: 0.7245, KL: 48.0054, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7390, Val Loss: 48.7241, Recon: 0.7332, KL: 48.0058, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7341, Val Loss: 48.7339, Recon: 0.7254, KL: 48.0087, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7533, Val Loss: 48.7322, Recon: 0.7379, KL: 48.0154, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7294, Val Loss: 48.7116, Recon: 0.7247, KL: 48.0048, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7342, Val Loss: 48.7093, Recon: 0.7258, KL: 48.0083, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7262, Val Loss: 48.7078, Recon: 0.7189, KL: 48.0073, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7405, Val Loss: 48.7316, Recon: 0.7298, KL: 48.0107, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7348, Val Loss: 48.7188, Recon: 0.7237, KL: 48.0111, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7244, Val Loss: 48.7125, Recon: 0.7179, KL: 48.0065, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7155, Val Loss: 48.7036, Recon: 0.7101, KL: 48.0055, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7199, Val Loss: 48.7093, Recon: 0.7155, KL: 48.0043, KL_weight: 4.8000
Epoch 130/250, Train Loss: 49.0694, Val Loss: 48.8453, Recon: 0.7249, KL: 48.3445, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7457, Val Loss: 48.7246, Recon: 0.7385, KL: 48.0072, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7699, Val Loss: 48.7391, Recon: 0.7510, KL: 48.0190, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7336, Val Loss: 48.7230, Recon: 0.7267, KL: 48.0069, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7290, Val Loss: 48.7206, Recon: 0.7249, KL: 48.0041, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7251, Val Loss: 48.7175, Recon: 0.7197, KL: 48.0053, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7218, Val Loss: 48.7041, Recon: 0.7153, KL: 48.0066, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7253, Val Loss: 48.7122, Recon: 0.7184, KL: 48.0069, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7149, Val Loss: 48.7127, Recon: 0.7112, KL: 48.0037, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7158, Val Loss: 48.7036, Recon: 0.7106, KL: 48.0052, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7138, Val Loss: 48.6980, Recon: 0.7100, KL: 48.0038, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7209, Val Loss: 48.7095, Recon: 0.7134, KL: 48.0074, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7135, Val Loss: 48.7169, Recon: 0.7109, KL: 48.0026, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7083, Val Loss: 48.6957, Recon: 0.7043, KL: 48.0040, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7010, Val Loss: 48.7000, Recon: 0.6985, KL: 48.0026, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7217, Val Loss: 48.7025, Recon: 0.7070, KL: 48.0147, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7112, Val Loss: 48.6957, Recon: 0.7037, KL: 48.0075, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7051, Val Loss: 48.6845, Recon: 0.7010, KL: 48.0042, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7014, Val Loss: 48.7037, Recon: 0.6962, KL: 48.0053, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6973, Val Loss: 48.7031, Recon: 0.6949, KL: 48.0024, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6962, Val Loss: 48.6896, Recon: 0.6941, KL: 48.0020, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7157, Val Loss: 48.6905, Recon: 0.7103, KL: 48.0055, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7008, Val Loss: 48.6823, Recon: 0.6962, KL: 48.0047, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6956, Val Loss: 48.6822, Recon: 0.6909, KL: 48.0048, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6994, Val Loss: 48.6877, Recon: 0.6919, KL: 48.0075, KL_weight: 4.8000
Saved model 23 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_22.pt
Training bootstrap model 24/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9761, Val Loss: 0.7232, Recon: 0.9761, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6918, Val Loss: 4.6583, Recon: 0.8147, KL: 3.8771, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4612, Val Loss: 9.4059, Recon: 0.8012, KL: 8.6599, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2470, Val Loss: 14.2084, Recon: 0.7906, KL: 13.4564, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0338, Val Loss: 18.9986, Recon: 0.7779, KL: 18.2559, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8308, Val Loss: 23.7975, Recon: 0.7779, KL: 23.0530, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6097, Val Loss: 28.5929, Recon: 0.7603, KL: 27.8494, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4109, Val Loss: 33.3743, Recon: 0.7635, KL: 32.6474, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1986, Val Loss: 38.1708, Recon: 0.7504, KL: 37.4482, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9893, Val Loss: 42.9646, Recon: 0.7440, KL: 42.2453, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7797, Val Loss: 47.7580, Recon: 0.7360, KL: 47.0437, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7447, Val Loss: 48.7182, Recon: 0.7386, KL: 48.0060, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7391, Val Loss: 48.7316, Recon: 0.7336, KL: 48.0056, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7502, Val Loss: 48.7142, Recon: 0.7406, KL: 48.0095, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7280, Val Loss: 48.7169, Recon: 0.7237, KL: 48.0042, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7330, Val Loss: 48.7276, Recon: 0.7257, KL: 48.0073, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7285, Val Loss: 48.7161, Recon: 0.7254, KL: 48.0031, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7336, Val Loss: 48.7247, Recon: 0.7221, KL: 48.0114, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7237, Val Loss: 48.7089, Recon: 0.7165, KL: 48.0072, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7115, Val Loss: 48.7116, Recon: 0.7080, KL: 48.0035, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7191, Val Loss: 48.7082, Recon: 0.7167, KL: 48.0023, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7127, Val Loss: 48.6949, Recon: 0.7047, KL: 48.0080, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7182, Val Loss: 48.6990, Recon: 0.7143, KL: 48.0039, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7119, Val Loss: 48.7105, Recon: 0.7075, KL: 48.0045, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7124, Val Loss: 48.7011, Recon: 0.7100, KL: 48.0025, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7127, Val Loss: 48.6991, Recon: 0.7083, KL: 48.0044, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7480, Val Loss: 48.7177, Recon: 0.7152, KL: 48.0329, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7088, Val Loss: 48.6945, Recon: 0.7061, KL: 48.0027, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7133, Val Loss: 48.7096, Recon: 0.7103, KL: 48.0030, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7006, Val Loss: 48.6935, Recon: 0.6975, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7031, Val Loss: 48.7066, Recon: 0.6995, KL: 48.0036, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7046, Val Loss: 48.6868, Recon: 0.7023, KL: 48.0023, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.9115, Val Loss: 48.7455, Recon: 0.7878, KL: 48.1237, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7343, Val Loss: 48.7132, Recon: 0.7239, KL: 48.0103, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7614, Val Loss: 48.7225, Recon: 0.7315, KL: 48.0299, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7331, Val Loss: 48.6953, Recon: 0.7132, KL: 48.0199, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7374, Val Loss: 48.6978, Recon: 0.7117, KL: 48.0257, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7095, Val Loss: 48.6966, Recon: 0.7062, KL: 48.0033, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7081, Val Loss: 48.6951, Recon: 0.7034, KL: 48.0047, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7027, Val Loss: 48.6897, Recon: 0.7001, KL: 48.0026, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7038, Val Loss: 48.6911, Recon: 0.7010, KL: 48.0029, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6947, Val Loss: 48.6830, Recon: 0.6921, KL: 48.0026, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7404, Val Loss: 48.7332, Recon: 0.6976, KL: 48.0428, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7008, Val Loss: 48.6897, Recon: 0.6982, KL: 48.0025, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7002, Val Loss: 48.6846, Recon: 0.6966, KL: 48.0036, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6928, Val Loss: 48.6856, Recon: 0.6910, KL: 48.0018, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7029, Val Loss: 48.6919, Recon: 0.6992, KL: 48.0037, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6952, Val Loss: 48.6849, Recon: 0.6937, KL: 48.0015, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7042, Val Loss: 48.6960, Recon: 0.6997, KL: 48.0045, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6986, Val Loss: 48.6874, Recon: 0.6968, KL: 48.0017, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6983, Val Loss: 48.6863, Recon: 0.6961, KL: 48.0023, KL_weight: 4.8000
Saved model 24 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_23.pt
Training bootstrap model 25/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9427, Val Loss: 0.6962, Recon: 0.9427, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7036, Val Loss: 4.6428, Recon: 0.8235, KL: 3.8801, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4817, Val Loss: 9.4259, Recon: 0.8123, KL: 8.6694, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2617, Val Loss: 14.1997, Recon: 0.7956, KL: 13.4660, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0445, Val Loss: 18.9981, Recon: 0.7868, KL: 18.2576, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8353, Val Loss: 23.8018, Recon: 0.7755, KL: 23.0598, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6127, Val Loss: 28.5726, Recon: 0.7613, KL: 27.8515, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4094, Val Loss: 33.4163, Recon: 0.7568, KL: 32.6526, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2205, Val Loss: 38.1733, Recon: 0.7706, KL: 37.4499, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0365, Val Loss: 43.0085, Recon: 0.7754, KL: 42.2611, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8214, Val Loss: 47.7849, Recon: 0.7590, KL: 47.0624, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7783, Val Loss: 48.7429, Recon: 0.7645, KL: 48.0138, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7426, Val Loss: 48.7416, Recon: 0.7372, KL: 48.0054, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7338, Val Loss: 48.7261, Recon: 0.7295, KL: 48.0044, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7423, Val Loss: 48.7134, Recon: 0.7357, KL: 48.0066, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7274, Val Loss: 48.7075, Recon: 0.7222, KL: 48.0052, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7383, Val Loss: 48.7241, Recon: 0.7317, KL: 48.0066, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7280, Val Loss: 48.7062, Recon: 0.7241, KL: 48.0039, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7303, Val Loss: 48.7110, Recon: 0.7257, KL: 48.0047, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7345, Val Loss: 48.7144, Recon: 0.7236, KL: 48.0109, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7427, Val Loss: 48.7224, Recon: 0.7345, KL: 48.0082, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7230, Val Loss: 48.7220, Recon: 0.7197, KL: 48.0033, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7314, Val Loss: 48.7254, Recon: 0.7260, KL: 48.0054, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7360, Val Loss: 48.7257, Recon: 0.7264, KL: 48.0096, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7173, Val Loss: 48.7117, Recon: 0.7143, KL: 48.0030, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7264, Val Loss: 48.7028, Recon: 0.7170, KL: 48.0094, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7303, Val Loss: 48.7170, Recon: 0.7248, KL: 48.0055, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7439, Val Loss: 48.7093, Recon: 0.7294, KL: 48.0146, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7241, Val Loss: 48.7006, Recon: 0.7142, KL: 48.0100, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7195, Val Loss: 48.7122, Recon: 0.7125, KL: 48.0070, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7140, Val Loss: 48.6920, Recon: 0.7080, KL: 48.0061, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7134, Val Loss: 48.7015, Recon: 0.7089, KL: 48.0045, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7114, Val Loss: 48.6946, Recon: 0.7059, KL: 48.0055, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7057, Val Loss: 48.6947, Recon: 0.7026, KL: 48.0031, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7293, Val Loss: 48.7198, Recon: 0.7213, KL: 48.0080, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7195, Val Loss: 48.6934, Recon: 0.7157, KL: 48.0038, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7375, Val Loss: 48.6969, Recon: 0.7132, KL: 48.0243, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7236, Val Loss: 48.6923, Recon: 0.7127, KL: 48.0109, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7135, Val Loss: 48.6972, Recon: 0.7087, KL: 48.0048, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7115, Val Loss: 48.6898, Recon: 0.7063, KL: 48.0051, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7077, Val Loss: 48.6931, Recon: 0.7041, KL: 48.0037, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7066, Val Loss: 48.6991, Recon: 0.7038, KL: 48.0029, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7045, Val Loss: 48.6919, Recon: 0.7022, KL: 48.0023, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7136, Val Loss: 48.6918, Recon: 0.7016, KL: 48.0120, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6992, Val Loss: 48.6853, Recon: 0.6971, KL: 48.0021, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7030, Val Loss: 48.6827, Recon: 0.7000, KL: 48.0030, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7067, Val Loss: 48.6880, Recon: 0.7019, KL: 48.0048, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7043, Val Loss: 48.6818, Recon: 0.7017, KL: 48.0026, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7015, Val Loss: 48.6871, Recon: 0.6986, KL: 48.0030, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6971, Val Loss: 48.6832, Recon: 0.6946, KL: 48.0025, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7076, Val Loss: 48.6861, Recon: 0.7054, KL: 48.0022, KL_weight: 4.8000
Saved model 25 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_24.pt
Training bootstrap model 26/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8944, Val Loss: 0.6724, Recon: 0.8944, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7012, Val Loss: 4.6277, Recon: 0.8214, KL: 3.8799, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4503, Val Loss: 9.4070, Recon: 0.7914, KL: 8.6589, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2275, Val Loss: 14.1902, Recon: 0.7732, KL: 13.4544, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0187, Val Loss: 18.9841, Recon: 0.7638, KL: 18.2549, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8052, Val Loss: 23.7925, Recon: 0.7527, KL: 23.0525, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5992, Val Loss: 28.5763, Recon: 0.7459, KL: 27.8533, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4122, Val Loss: 33.3841, Recon: 0.7598, KL: 32.6524, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2039, Val Loss: 38.1880, Recon: 0.7477, KL: 37.4563, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9957, Val Loss: 42.9657, Recon: 0.7422, KL: 42.2535, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7761, Val Loss: 47.7577, Recon: 0.7311, KL: 47.0450, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7412, Val Loss: 48.7206, Recon: 0.7270, KL: 48.0142, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7202, Val Loss: 48.7200, Recon: 0.7152, KL: 48.0050, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7289, Val Loss: 48.7101, Recon: 0.7228, KL: 48.0062, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7130, Val Loss: 48.7076, Recon: 0.7101, KL: 48.0029, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7397, Val Loss: 48.7053, Recon: 0.7199, KL: 48.0199, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7266, Val Loss: 48.7119, Recon: 0.7132, KL: 48.0134, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7287, Val Loss: 48.7119, Recon: 0.7200, KL: 48.0087, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7104, Val Loss: 48.6981, Recon: 0.7070, KL: 48.0034, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7084, Val Loss: 48.6950, Recon: 0.7046, KL: 48.0038, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7588, Val Loss: 48.7041, Recon: 0.7185, KL: 48.0403, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.8007, Val Loss: 48.7187, Recon: 0.7563, KL: 48.0444, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7100, Val Loss: 48.6991, Recon: 0.7053, KL: 48.0047, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7027, Val Loss: 48.6937, Recon: 0.7003, KL: 48.0024, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7004, Val Loss: 48.6965, Recon: 0.6966, KL: 48.0038, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7026, Val Loss: 48.7000, Recon: 0.6996, KL: 48.0031, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6968, Val Loss: 48.6971, Recon: 0.6947, KL: 48.0022, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6981, Val Loss: 48.6907, Recon: 0.6949, KL: 48.0032, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6954, Val Loss: 48.6948, Recon: 0.6936, KL: 48.0018, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6957, Val Loss: 48.6920, Recon: 0.6931, KL: 48.0026, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6984, Val Loss: 48.7151, Recon: 0.6927, KL: 48.0058, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7035, Val Loss: 48.6877, Recon: 0.6988, KL: 48.0047, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7026, Val Loss: 48.7133, Recon: 0.6880, KL: 48.0145, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7461, Val Loss: 48.7075, Recon: 0.7217, KL: 48.0244, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7127, Val Loss: 48.6972, Recon: 0.7039, KL: 48.0088, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6962, Val Loss: 48.6814, Recon: 0.6944, KL: 48.0019, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6854, Val Loss: 48.6797, Recon: 0.6837, KL: 48.0017, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6943, Val Loss: 48.6875, Recon: 0.6894, KL: 48.0049, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6925, Val Loss: 48.6873, Recon: 0.6911, KL: 48.0014, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6964, Val Loss: 48.6963, Recon: 0.6915, KL: 48.0049, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6968, Val Loss: 48.6908, Recon: 0.6942, KL: 48.0026, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6921, Val Loss: 48.6857, Recon: 0.6876, KL: 48.0045, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6867, Val Loss: 48.6795, Recon: 0.6844, KL: 48.0023, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6819, Val Loss: 48.6798, Recon: 0.6803, KL: 48.0016, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6844, Val Loss: 48.6869, Recon: 0.6818, KL: 48.0026, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6856, Val Loss: 48.6819, Recon: 0.6815, KL: 48.0041, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6769, Val Loss: 48.6702, Recon: 0.6749, KL: 48.0020, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6778, Val Loss: 48.6763, Recon: 0.6756, KL: 48.0022, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6886, Val Loss: 48.6742, Recon: 0.6822, KL: 48.0064, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6884, Val Loss: 48.6877, Recon: 0.6866, KL: 48.0018, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6801, Val Loss: 48.6756, Recon: 0.6751, KL: 48.0050, KL_weight: 4.8000
Saved model 26 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_25.pt
Training bootstrap model 27/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9052, Val Loss: 0.6802, Recon: 0.9052, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7138, Val Loss: 4.6280, Recon: 0.8321, KL: 3.8817, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4728, Val Loss: 9.4388, Recon: 0.8037, KL: 8.6691, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2442, Val Loss: 14.1987, Recon: 0.7840, KL: 13.4602, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0255, Val Loss: 18.9974, Recon: 0.7678, KL: 18.2577, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8275, Val Loss: 23.8194, Recon: 0.7737, KL: 23.0538, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6054, Val Loss: 28.5660, Recon: 0.7548, KL: 27.8506, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4166, Val Loss: 33.3767, Recon: 0.7652, KL: 32.6515, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1848, Val Loss: 38.1634, Recon: 0.7363, KL: 37.4485, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9887, Val Loss: 42.9832, Recon: 0.7391, KL: 42.2496, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7908, Val Loss: 47.7767, Recon: 0.7428, KL: 47.0480, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7404, Val Loss: 48.7375, Recon: 0.7357, KL: 48.0046, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.8034, Val Loss: 48.8252, Recon: 0.7632, KL: 48.0402, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7658, Val Loss: 48.7385, Recon: 0.7351, KL: 48.0307, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7337, Val Loss: 48.7118, Recon: 0.7274, KL: 48.0063, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7288, Val Loss: 48.7177, Recon: 0.7229, KL: 48.0058, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7367, Val Loss: 48.7213, Recon: 0.7226, KL: 48.0140, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7252, Val Loss: 48.7203, Recon: 0.7192, KL: 48.0060, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7231, Val Loss: 48.7111, Recon: 0.7197, KL: 48.0034, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7209, Val Loss: 48.7086, Recon: 0.7150, KL: 48.0059, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7204, Val Loss: 48.7268, Recon: 0.7167, KL: 48.0037, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7234, Val Loss: 48.7153, Recon: 0.7115, KL: 48.0119, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7123, Val Loss: 48.6990, Recon: 0.7095, KL: 48.0029, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7095, Val Loss: 48.7053, Recon: 0.7041, KL: 48.0054, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7120, Val Loss: 48.6956, Recon: 0.7081, KL: 48.0039, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7002, Val Loss: 48.7238, Recon: 0.6976, KL: 48.0026, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7131, Val Loss: 48.7094, Recon: 0.7105, KL: 48.0026, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7063, Val Loss: 48.6988, Recon: 0.7015, KL: 48.0048, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7256, Val Loss: 48.7301, Recon: 0.7161, KL: 48.0096, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7185, Val Loss: 48.7013, Recon: 0.7127, KL: 48.0058, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7190, Val Loss: 48.7010, Recon: 0.7089, KL: 48.0101, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7033, Val Loss: 48.6930, Recon: 0.7021, KL: 48.0012, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7004, Val Loss: 48.6883, Recon: 0.6975, KL: 48.0029, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6916, Val Loss: 48.7035, Recon: 0.6886, KL: 48.0030, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6989, Val Loss: 48.6913, Recon: 0.6950, KL: 48.0039, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6958, Val Loss: 48.6884, Recon: 0.6921, KL: 48.0037, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7336, Val Loss: 48.7057, Recon: 0.7043, KL: 48.0293, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6942, Val Loss: 48.6906, Recon: 0.6906, KL: 48.0036, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7157, Val Loss: 48.6914, Recon: 0.7038, KL: 48.0120, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7019, Val Loss: 48.6949, Recon: 0.6959, KL: 48.0060, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7097, Val Loss: 48.7007, Recon: 0.7013, KL: 48.0084, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6975, Val Loss: 48.6828, Recon: 0.6938, KL: 48.0036, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6902, Val Loss: 48.6837, Recon: 0.6880, KL: 48.0022, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7018, Val Loss: 48.6922, Recon: 0.6987, KL: 48.0031, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6922, Val Loss: 48.6810, Recon: 0.6903, KL: 48.0020, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6927, Val Loss: 48.6838, Recon: 0.6907, KL: 48.0020, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7002, Val Loss: 48.6907, Recon: 0.6967, KL: 48.0035, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6960, Val Loss: 48.6865, Recon: 0.6925, KL: 48.0035, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6924, Val Loss: 48.6910, Recon: 0.6890, KL: 48.0034, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6923, Val Loss: 48.6827, Recon: 0.6899, KL: 48.0024, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6922, Val Loss: 48.6860, Recon: 0.6865, KL: 48.0057, KL_weight: 4.8000
Saved model 27 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_26.pt
Training bootstrap model 28/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9146, Val Loss: 0.6730, Recon: 0.9146, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7919, Val Loss: 4.6862, Recon: 0.8706, KL: 3.9214, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4847, Val Loss: 9.4220, Recon: 0.8165, KL: 8.6682, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2798, Val Loss: 14.2226, Recon: 0.8162, KL: 13.4635, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0528, Val Loss: 18.9819, Recon: 0.7935, KL: 18.2593, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8371, Val Loss: 23.8123, Recon: 0.7765, KL: 23.0606, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6294, Val Loss: 28.6040, Recon: 0.7782, KL: 27.8512, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4478, Val Loss: 33.3861, Recon: 0.7899, KL: 32.6579, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2275, Val Loss: 38.2042, Recon: 0.7772, KL: 37.4503, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0119, Val Loss: 42.9915, Recon: 0.7599, KL: 42.2519, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8157, Val Loss: 47.7932, Recon: 0.7618, KL: 47.0539, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7746, Val Loss: 48.7285, Recon: 0.7662, KL: 48.0085, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7651, Val Loss: 48.7309, Recon: 0.7607, KL: 48.0044, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7643, Val Loss: 48.7213, Recon: 0.7579, KL: 48.0064, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7637, Val Loss: 48.7405, Recon: 0.7513, KL: 48.0124, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.8633, Val Loss: 48.7470, Recon: 0.7695, KL: 48.0939, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7562, Val Loss: 48.7302, Recon: 0.7533, KL: 48.0029, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7750, Val Loss: 48.7298, Recon: 0.7687, KL: 48.0063, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7539, Val Loss: 48.7078, Recon: 0.7396, KL: 48.0143, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7607, Val Loss: 48.7242, Recon: 0.7549, KL: 48.0058, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7552, Val Loss: 48.7474, Recon: 0.7462, KL: 48.0090, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7393, Val Loss: 48.7120, Recon: 0.7350, KL: 48.0043, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7394, Val Loss: 48.7225, Recon: 0.7350, KL: 48.0044, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7679, Val Loss: 48.7219, Recon: 0.7488, KL: 48.0191, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7499, Val Loss: 48.7213, Recon: 0.7424, KL: 48.0075, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7454, Val Loss: 48.7155, Recon: 0.7326, KL: 48.0128, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7314, Val Loss: 48.6968, Recon: 0.7257, KL: 48.0057, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7294, Val Loss: 48.6995, Recon: 0.7274, KL: 48.0019, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7216, Val Loss: 48.7006, Recon: 0.7182, KL: 48.0034, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7237, Val Loss: 48.6964, Recon: 0.7166, KL: 48.0072, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7337, Val Loss: 48.6907, Recon: 0.7289, KL: 48.0048, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7411, Val Loss: 48.7190, Recon: 0.7253, KL: 48.0158, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7137, Val Loss: 48.6977, Recon: 0.7095, KL: 48.0042, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7183, Val Loss: 48.6841, Recon: 0.7109, KL: 48.0074, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7243, Val Loss: 48.7229, Recon: 0.7184, KL: 48.0059, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7139, Val Loss: 48.6831, Recon: 0.7089, KL: 48.0050, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6999, Val Loss: 48.6874, Recon: 0.6959, KL: 48.0040, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7811, Val Loss: 48.6865, Recon: 0.7230, KL: 48.0581, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7084, Val Loss: 48.6942, Recon: 0.7046, KL: 48.0038, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7350, Val Loss: 48.6934, Recon: 0.7123, KL: 48.0227, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7080, Val Loss: 48.6874, Recon: 0.7044, KL: 48.0036, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7292, Val Loss: 48.7034, Recon: 0.7175, KL: 48.0117, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7048, Val Loss: 48.6825, Recon: 0.7035, KL: 48.0012, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7035, Val Loss: 48.6992, Recon: 0.7019, KL: 48.0017, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6975, Val Loss: 48.6843, Recon: 0.6955, KL: 48.0020, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7055, Val Loss: 48.6902, Recon: 0.6995, KL: 48.0060, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7009, Val Loss: 48.6764, Recon: 0.6966, KL: 48.0043, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6985, Val Loss: 48.6811, Recon: 0.6970, KL: 48.0016, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7022, Val Loss: 48.6964, Recon: 0.7000, KL: 48.0021, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7064, Val Loss: 48.6833, Recon: 0.7024, KL: 48.0039, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6898, Val Loss: 48.6851, Recon: 0.6883, KL: 48.0014, KL_weight: 4.8000
Saved model 28 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_27.pt
Training bootstrap model 29/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8677, Val Loss: 0.6743, Recon: 0.8677, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6854, Val Loss: 4.6329, Recon: 0.8039, KL: 3.8816, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4488, Val Loss: 9.4100, Recon: 0.7878, KL: 8.6610, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2462, Val Loss: 14.1926, Recon: 0.7873, KL: 13.4589, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0108, Val Loss: 18.9876, Recon: 0.7573, KL: 18.2535, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8031, Val Loss: 23.7994, Recon: 0.7546, KL: 23.0485, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5952, Val Loss: 28.5792, Recon: 0.7458, KL: 27.8494, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3815, Val Loss: 33.3780, Recon: 0.7354, KL: 32.6461, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1820, Val Loss: 38.1715, Recon: 0.7374, KL: 37.4446, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0022, Val Loss: 42.9847, Recon: 0.7429, KL: 42.2592, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7738, Val Loss: 47.7604, Recon: 0.7291, KL: 47.0447, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7318, Val Loss: 48.7326, Recon: 0.7250, KL: 48.0068, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7214, Val Loss: 48.7162, Recon: 0.7173, KL: 48.0041, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7830, Val Loss: 48.7572, Recon: 0.7281, KL: 48.0549, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7231, Val Loss: 48.7173, Recon: 0.7144, KL: 48.0086, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7225, Val Loss: 48.7183, Recon: 0.7151, KL: 48.0073, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7180, Val Loss: 48.7166, Recon: 0.7132, KL: 48.0048, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7152, Val Loss: 48.7257, Recon: 0.7075, KL: 48.0077, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7108, Val Loss: 48.7044, Recon: 0.7055, KL: 48.0053, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7019, Val Loss: 48.6961, Recon: 0.6993, KL: 48.0026, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7064, Val Loss: 48.7002, Recon: 0.7021, KL: 48.0043, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7064, Val Loss: 48.7218, Recon: 0.7021, KL: 48.0044, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7089, Val Loss: 48.6894, Recon: 0.7068, KL: 48.0021, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.6935, Val Loss: 48.7002, Recon: 0.6912, KL: 48.0023, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7007, Val Loss: 48.6925, Recon: 0.6991, KL: 48.0016, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.6963, Val Loss: 48.7020, Recon: 0.6929, KL: 48.0034, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7009, Val Loss: 48.6900, Recon: 0.6965, KL: 48.0044, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6946, Val Loss: 48.6940, Recon: 0.6917, KL: 48.0030, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7115, Val Loss: 48.6943, Recon: 0.7056, KL: 48.0059, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7001, Val Loss: 48.6922, Recon: 0.6981, KL: 48.0020, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6941, Val Loss: 48.6856, Recon: 0.6905, KL: 48.0037, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6954, Val Loss: 48.6928, Recon: 0.6919, KL: 48.0035, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6919, Val Loss: 48.6929, Recon: 0.6881, KL: 48.0038, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6864, Val Loss: 48.6865, Recon: 0.6837, KL: 48.0027, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6968, Val Loss: 48.6948, Recon: 0.6929, KL: 48.0039, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6910, Val Loss: 48.6871, Recon: 0.6880, KL: 48.0031, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6826, Val Loss: 48.6899, Recon: 0.6798, KL: 48.0028, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7177, Val Loss: 48.6937, Recon: 0.6948, KL: 48.0229, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6902, Val Loss: 48.7078, Recon: 0.6866, KL: 48.0036, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6865, Val Loss: 48.6914, Recon: 0.6810, KL: 48.0054, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7009, Val Loss: 48.6904, Recon: 0.6960, KL: 48.0049, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6903, Val Loss: 48.6881, Recon: 0.6886, KL: 48.0017, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6824, Val Loss: 48.6926, Recon: 0.6802, KL: 48.0022, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6905, Val Loss: 48.6856, Recon: 0.6865, KL: 48.0041, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6835, Val Loss: 48.6760, Recon: 0.6813, KL: 48.0022, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6853, Val Loss: 48.6876, Recon: 0.6820, KL: 48.0033, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6964, Val Loss: 48.7026, Recon: 0.6896, KL: 48.0068, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6859, Val Loss: 48.6779, Recon: 0.6846, KL: 48.0014, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6884, Val Loss: 48.6816, Recon: 0.6862, KL: 48.0022, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6843, Val Loss: 48.6816, Recon: 0.6796, KL: 48.0047, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6911, Val Loss: 48.6914, Recon: 0.6871, KL: 48.0040, KL_weight: 4.8000
Saved model 29 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_28.pt
Training bootstrap model 30/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8678, Val Loss: 0.6765, Recon: 0.8678, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6556, Val Loss: 4.6233, Recon: 0.7880, KL: 3.8676, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4272, Val Loss: 9.3899, Recon: 0.7701, KL: 8.6571, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2282, Val Loss: 14.2155, Recon: 0.7720, KL: 13.4562, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0027, Val Loss: 18.9853, Recon: 0.7532, KL: 18.2496, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8024, Val Loss: 23.7906, Recon: 0.7501, KL: 23.0523, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5813, Val Loss: 28.5702, Recon: 0.7354, KL: 27.8459, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3867, Val Loss: 33.3602, Recon: 0.7396, KL: 32.6471, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2060, Val Loss: 38.1875, Recon: 0.7562, KL: 37.4499, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9799, Val Loss: 42.9800, Recon: 0.7340, KL: 42.2459, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7837, Val Loss: 47.7656, Recon: 0.7322, KL: 47.0514, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7408, Val Loss: 48.7432, Recon: 0.7283, KL: 48.0125, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7369, Val Loss: 48.7257, Recon: 0.7282, KL: 48.0087, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7255, Val Loss: 48.7136, Recon: 0.7206, KL: 48.0049, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7309, Val Loss: 48.7214, Recon: 0.7256, KL: 48.0052, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7283, Val Loss: 48.7173, Recon: 0.7186, KL: 48.0098, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7172, Val Loss: 48.7088, Recon: 0.7126, KL: 48.0046, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7126, Val Loss: 48.7026, Recon: 0.7096, KL: 48.0030, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7132, Val Loss: 48.7079, Recon: 0.7096, KL: 48.0036, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7145, Val Loss: 48.7049, Recon: 0.7046, KL: 48.0099, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7082, Val Loss: 48.7127, Recon: 0.7040, KL: 48.0042, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7111, Val Loss: 48.6935, Recon: 0.7065, KL: 48.0047, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7051, Val Loss: 48.7052, Recon: 0.6992, KL: 48.0058, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7037, Val Loss: 48.7033, Recon: 0.6991, KL: 48.0046, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.6998, Val Loss: 48.6941, Recon: 0.6964, KL: 48.0034, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.6932, Val Loss: 48.7031, Recon: 0.6919, KL: 48.0013, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6973, Val Loss: 48.6913, Recon: 0.6939, KL: 48.0034, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7044, Val Loss: 48.7066, Recon: 0.6948, KL: 48.0096, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6986, Val Loss: 48.6893, Recon: 0.6936, KL: 48.0050, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6950, Val Loss: 48.6928, Recon: 0.6894, KL: 48.0057, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7016, Val Loss: 48.6920, Recon: 0.6982, KL: 48.0033, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6939, Val Loss: 48.7221, Recon: 0.6902, KL: 48.0036, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6923, Val Loss: 48.6949, Recon: 0.6897, KL: 48.0027, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6988, Val Loss: 48.6983, Recon: 0.6945, KL: 48.0044, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6873, Val Loss: 48.6810, Recon: 0.6852, KL: 48.0020, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6992, Val Loss: 48.6969, Recon: 0.6958, KL: 48.0034, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7118, Val Loss: 48.6912, Recon: 0.7011, KL: 48.0107, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6884, Val Loss: 48.6841, Recon: 0.6852, KL: 48.0032, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6855, Val Loss: 48.6902, Recon: 0.6839, KL: 48.0017, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6952, Val Loss: 48.6875, Recon: 0.6923, KL: 48.0030, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6877, Val Loss: 48.6789, Recon: 0.6856, KL: 48.0021, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6975, Val Loss: 48.6869, Recon: 0.6928, KL: 48.0047, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6851, Val Loss: 48.6832, Recon: 0.6829, KL: 48.0022, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6857, Val Loss: 48.6821, Recon: 0.6834, KL: 48.0023, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6929, Val Loss: 48.6857, Recon: 0.6882, KL: 48.0048, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6852, Val Loss: 48.6990, Recon: 0.6825, KL: 48.0027, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6784, Val Loss: 48.6959, Recon: 0.6771, KL: 48.0013, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6812, Val Loss: 48.6975, Recon: 0.6800, KL: 48.0013, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6898, Val Loss: 48.6842, Recon: 0.6853, KL: 48.0045, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6851, Val Loss: 48.6897, Recon: 0.6806, KL: 48.0045, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6815, Val Loss: 48.6720, Recon: 0.6806, KL: 48.0009, KL_weight: 4.8000
Saved model 30 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_29.pt
Training bootstrap model 31/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8915, Val Loss: 0.6763, Recon: 0.8915, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6826, Val Loss: 4.6357, Recon: 0.8077, KL: 3.8749, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4501, Val Loss: 9.4122, Recon: 0.7854, KL: 8.6647, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2204, Val Loss: 14.2086, Recon: 0.7638, KL: 13.4567, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0303, Val Loss: 19.0119, Recon: 0.7720, KL: 18.2583, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8156, Val Loss: 23.7900, Recon: 0.7581, KL: 23.0576, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5930, Val Loss: 28.5820, Recon: 0.7441, KL: 27.8489, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4071, Val Loss: 33.3915, Recon: 0.7551, KL: 32.6520, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1850, Val Loss: 38.1737, Recon: 0.7370, KL: 37.4481, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9810, Val Loss: 42.9811, Recon: 0.7321, KL: 42.2489, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8400, Val Loss: 47.8025, Recon: 0.7643, KL: 47.0757, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7573, Val Loss: 48.7345, Recon: 0.7484, KL: 48.0089, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7414, Val Loss: 48.7192, Recon: 0.7304, KL: 48.0111, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7290, Val Loss: 48.7363, Recon: 0.7227, KL: 48.0064, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7371, Val Loss: 48.7255, Recon: 0.7287, KL: 48.0083, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7582, Val Loss: 48.7321, Recon: 0.7322, KL: 48.0260, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7259, Val Loss: 48.7124, Recon: 0.7210, KL: 48.0049, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7269, Val Loss: 48.7066, Recon: 0.7227, KL: 48.0042, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7161, Val Loss: 48.7105, Recon: 0.7137, KL: 48.0024, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7149, Val Loss: 48.6991, Recon: 0.7115, KL: 48.0034, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7192, Val Loss: 48.7231, Recon: 0.7135, KL: 48.0057, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7208, Val Loss: 48.7057, Recon: 0.7182, KL: 48.0027, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7097, Val Loss: 48.6984, Recon: 0.7051, KL: 48.0047, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7070, Val Loss: 48.7033, Recon: 0.7033, KL: 48.0036, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7054, Val Loss: 48.6937, Recon: 0.7028, KL: 48.0026, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7167, Val Loss: 48.7150, Recon: 0.7101, KL: 48.0066, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7045, Val Loss: 48.6974, Recon: 0.7008, KL: 48.0038, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7159, Val Loss: 48.7183, Recon: 0.7102, KL: 48.0057, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7039, Val Loss: 48.6916, Recon: 0.7004, KL: 48.0036, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6976, Val Loss: 48.7018, Recon: 0.6941, KL: 48.0034, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6987, Val Loss: 48.7059, Recon: 0.6949, KL: 48.0038, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7055, Val Loss: 48.7224, Recon: 0.7012, KL: 48.0043, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7017, Val Loss: 48.7021, Recon: 0.6985, KL: 48.0032, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7034, Val Loss: 48.6948, Recon: 0.6974, KL: 48.0060, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6977, Val Loss: 48.6882, Recon: 0.6951, KL: 48.0025, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7001, Val Loss: 48.6930, Recon: 0.6986, KL: 48.0015, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7002, Val Loss: 48.6988, Recon: 0.6966, KL: 48.0035, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7028, Val Loss: 48.7109, Recon: 0.7004, KL: 48.0024, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7077, Val Loss: 48.6939, Recon: 0.7040, KL: 48.0036, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7081, Val Loss: 48.7043, Recon: 0.7057, KL: 48.0024, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6932, Val Loss: 48.6838, Recon: 0.6921, KL: 48.0011, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6929, Val Loss: 48.6841, Recon: 0.6913, KL: 48.0017, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6889, Val Loss: 48.6771, Recon: 0.6848, KL: 48.0042, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7067, Val Loss: 48.6939, Recon: 0.7045, KL: 48.0022, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7007, Val Loss: 48.6988, Recon: 0.6911, KL: 48.0096, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6881, Val Loss: 48.6891, Recon: 0.6867, KL: 48.0014, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6931, Val Loss: 48.6831, Recon: 0.6904, KL: 48.0027, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6941, Val Loss: 48.6821, Recon: 0.6928, KL: 48.0012, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7089, Val Loss: 48.6923, Recon: 0.6952, KL: 48.0138, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7234, Val Loss: 48.7027, Recon: 0.7099, KL: 48.0135, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6976, Val Loss: 48.7059, Recon: 0.6927, KL: 48.0049, KL_weight: 4.8000
Saved model 31 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_30.pt
Training bootstrap model 32/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9123, Val Loss: 0.6784, Recon: 0.9123, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7027, Val Loss: 4.6571, Recon: 0.8252, KL: 3.8775, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4611, Val Loss: 9.4036, Recon: 0.8009, KL: 8.6603, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2377, Val Loss: 14.2052, Recon: 0.7842, KL: 13.4535, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0330, Val Loss: 18.9778, Recon: 0.7808, KL: 18.2522, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8088, Val Loss: 23.7669, Recon: 0.7618, KL: 23.0469, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6126, Val Loss: 28.6099, Recon: 0.7624, KL: 27.8502, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4083, Val Loss: 33.3748, Recon: 0.7569, KL: 32.6514, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2097, Val Loss: 38.1930, Recon: 0.7554, KL: 37.4543, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9927, Val Loss: 42.9741, Recon: 0.7459, KL: 42.2468, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8309, Val Loss: 47.7676, Recon: 0.7681, KL: 47.0628, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7614, Val Loss: 48.7311, Recon: 0.7502, KL: 48.0113, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7477, Val Loss: 48.7233, Recon: 0.7424, KL: 48.0053, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7541, Val Loss: 48.7232, Recon: 0.7439, KL: 48.0102, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7317, Val Loss: 48.7099, Recon: 0.7278, KL: 48.0039, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7307, Val Loss: 48.7128, Recon: 0.7259, KL: 48.0048, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7395, Val Loss: 48.7221, Recon: 0.7269, KL: 48.0126, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7288, Val Loss: 48.7201, Recon: 0.7218, KL: 48.0070, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7229, Val Loss: 48.7146, Recon: 0.7202, KL: 48.0027, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7234, Val Loss: 48.7004, Recon: 0.7200, KL: 48.0034, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7207, Val Loss: 48.7063, Recon: 0.7176, KL: 48.0031, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7266, Val Loss: 48.7076, Recon: 0.7221, KL: 48.0045, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7234, Val Loss: 48.7046, Recon: 0.7183, KL: 48.0051, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7180, Val Loss: 48.6955, Recon: 0.7145, KL: 48.0035, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7167, Val Loss: 48.6884, Recon: 0.7133, KL: 48.0034, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7306, Val Loss: 48.6975, Recon: 0.7252, KL: 48.0054, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7239, Val Loss: 48.7001, Recon: 0.7171, KL: 48.0067, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7137, Val Loss: 48.6975, Recon: 0.7108, KL: 48.0029, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7291, Val Loss: 48.6883, Recon: 0.7232, KL: 48.0060, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7133, Val Loss: 48.6916, Recon: 0.7108, KL: 48.0025, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7132, Val Loss: 48.6959, Recon: 0.7104, KL: 48.0028, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7116, Val Loss: 48.6970, Recon: 0.7090, KL: 48.0026, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7100, Val Loss: 48.6938, Recon: 0.7053, KL: 48.0047, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7173, Val Loss: 48.7155, Recon: 0.7096, KL: 48.0077, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7125, Val Loss: 48.6890, Recon: 0.7085, KL: 48.0040, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7211, Val Loss: 48.6966, Recon: 0.7166, KL: 48.0045, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7136, Val Loss: 48.7020, Recon: 0.7077, KL: 48.0060, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7079, Val Loss: 48.6883, Recon: 0.7044, KL: 48.0035, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7128, Val Loss: 48.6924, Recon: 0.7082, KL: 48.0046, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7015, Val Loss: 48.6814, Recon: 0.6990, KL: 48.0025, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7089, Val Loss: 48.6837, Recon: 0.7021, KL: 48.0068, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6999, Val Loss: 48.6859, Recon: 0.6965, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6944, Val Loss: 48.6902, Recon: 0.6929, KL: 48.0014, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6989, Val Loss: 48.6916, Recon: 0.6965, KL: 48.0024, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6962, Val Loss: 48.6845, Recon: 0.6945, KL: 48.0017, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7023, Val Loss: 48.6976, Recon: 0.6971, KL: 48.0052, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7006, Val Loss: 48.6797, Recon: 0.6979, KL: 48.0026, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7001, Val Loss: 48.6986, Recon: 0.6962, KL: 48.0039, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6928, Val Loss: 48.6865, Recon: 0.6919, KL: 48.0009, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6950, Val Loss: 48.6934, Recon: 0.6920, KL: 48.0030, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6982, Val Loss: 48.6870, Recon: 0.6948, KL: 48.0035, KL_weight: 4.8000
Saved model 32 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_31.pt
Training bootstrap model 33/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9072, Val Loss: 0.6772, Recon: 0.9072, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6906, Val Loss: 4.6143, Recon: 0.8149, KL: 3.8756, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4679, Val Loss: 9.4259, Recon: 0.8002, KL: 8.6678, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2379, Val Loss: 14.2072, Recon: 0.7794, KL: 13.4585, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0300, Val Loss: 18.9937, Recon: 0.7727, KL: 18.2573, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8254, Val Loss: 23.7870, Recon: 0.7679, KL: 23.0575, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6074, Val Loss: 28.5927, Recon: 0.7535, KL: 27.8539, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3962, Val Loss: 33.3698, Recon: 0.7470, KL: 32.6492, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1846, Val Loss: 38.1624, Recon: 0.7373, KL: 37.4472, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9984, Val Loss: 42.9629, Recon: 0.7495, KL: 42.2489, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7997, Val Loss: 47.7593, Recon: 0.7486, KL: 47.0512, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7487, Val Loss: 48.7250, Recon: 0.7354, KL: 48.0133, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7407, Val Loss: 48.7311, Recon: 0.7296, KL: 48.0111, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7388, Val Loss: 48.7195, Recon: 0.7266, KL: 48.0122, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7241, Val Loss: 48.7238, Recon: 0.7188, KL: 48.0053, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7402, Val Loss: 48.7182, Recon: 0.7306, KL: 48.0096, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7272, Val Loss: 48.7125, Recon: 0.7237, KL: 48.0034, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7145, Val Loss: 48.7074, Recon: 0.7107, KL: 48.0038, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7168, Val Loss: 48.7109, Recon: 0.7137, KL: 48.0031, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7259, Val Loss: 48.7143, Recon: 0.7062, KL: 48.0197, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7230, Val Loss: 48.7205, Recon: 0.7190, KL: 48.0041, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7127, Val Loss: 48.7020, Recon: 0.7088, KL: 48.0039, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7184, Val Loss: 48.7025, Recon: 0.7149, KL: 48.0035, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7242, Val Loss: 48.6974, Recon: 0.7143, KL: 48.0099, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7071, Val Loss: 48.6933, Recon: 0.7052, KL: 48.0018, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7165, Val Loss: 48.7080, Recon: 0.7107, KL: 48.0058, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7195, Val Loss: 48.7066, Recon: 0.7101, KL: 48.0094, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7075, Val Loss: 48.7056, Recon: 0.7012, KL: 48.0064, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7072, Val Loss: 48.6982, Recon: 0.7043, KL: 48.0029, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7286, Val Loss: 48.7157, Recon: 0.7242, KL: 48.0044, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7089, Val Loss: 48.7168, Recon: 0.7053, KL: 48.0037, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6966, Val Loss: 48.6936, Recon: 0.6914, KL: 48.0052, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7012, Val Loss: 48.6826, Recon: 0.6979, KL: 48.0033, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7082, Val Loss: 48.6859, Recon: 0.7038, KL: 48.0044, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6916, Val Loss: 48.6896, Recon: 0.6887, KL: 48.0029, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7057, Val Loss: 48.6941, Recon: 0.7018, KL: 48.0039, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7022, Val Loss: 48.6867, Recon: 0.6979, KL: 48.0043, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6961, Val Loss: 48.6855, Recon: 0.6932, KL: 48.0029, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6990, Val Loss: 48.6861, Recon: 0.6934, KL: 48.0056, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6917, Val Loss: 48.6795, Recon: 0.6899, KL: 48.0018, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6928, Val Loss: 48.6897, Recon: 0.6889, KL: 48.0038, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6916, Val Loss: 48.6850, Recon: 0.6902, KL: 48.0015, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6932, Val Loss: 48.6933, Recon: 0.6900, KL: 48.0032, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6908, Val Loss: 48.6826, Recon: 0.6858, KL: 48.0049, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6913, Val Loss: 48.6811, Recon: 0.6877, KL: 48.0036, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6874, Val Loss: 48.6844, Recon: 0.6855, KL: 48.0019, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7012, Val Loss: 48.6842, Recon: 0.6958, KL: 48.0055, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6915, Val Loss: 48.6853, Recon: 0.6898, KL: 48.0018, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6928, Val Loss: 48.6893, Recon: 0.6858, KL: 48.0071, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6911, Val Loss: 48.6885, Recon: 0.6883, KL: 48.0028, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6842, Val Loss: 48.6820, Recon: 0.6828, KL: 48.0014, KL_weight: 4.8000
Saved model 33 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_32.pt
Training bootstrap model 34/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9393, Val Loss: 0.6982, Recon: 0.9393, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6908, Val Loss: 4.6195, Recon: 0.8185, KL: 3.8723, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4587, Val Loss: 9.4403, Recon: 0.7926, KL: 8.6661, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2372, Val Loss: 14.2011, Recon: 0.7793, KL: 13.4579, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0459, Val Loss: 19.0179, Recon: 0.7730, KL: 18.2730, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8101, Val Loss: 23.7895, Recon: 0.7602, KL: 23.0499, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5999, Val Loss: 28.5820, Recon: 0.7505, KL: 27.8494, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4253, Val Loss: 33.3768, Recon: 0.7746, KL: 32.6507, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1990, Val Loss: 38.1704, Recon: 0.7536, KL: 37.4454, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9879, Val Loss: 42.9948, Recon: 0.7395, KL: 42.2484, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7869, Val Loss: 47.7839, Recon: 0.7412, KL: 47.0458, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7440, Val Loss: 48.7296, Recon: 0.7368, KL: 48.0072, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7422, Val Loss: 48.7166, Recon: 0.7383, KL: 48.0039, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7271, Val Loss: 48.7055, Recon: 0.7245, KL: 48.0026, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7364, Val Loss: 48.7124, Recon: 0.7291, KL: 48.0073, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7376, Val Loss: 48.7159, Recon: 0.7296, KL: 48.0080, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7292, Val Loss: 48.7236, Recon: 0.7261, KL: 48.0032, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7196, Val Loss: 48.7266, Recon: 0.7162, KL: 48.0034, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7361, Val Loss: 48.7171, Recon: 0.7295, KL: 48.0067, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7238, Val Loss: 48.7126, Recon: 0.7188, KL: 48.0050, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7145, Val Loss: 48.7074, Recon: 0.7121, KL: 48.0025, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7297, Val Loss: 48.7000, Recon: 0.7249, KL: 48.0048, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7257, Val Loss: 48.7018, Recon: 0.7216, KL: 48.0041, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7119, Val Loss: 48.6967, Recon: 0.7082, KL: 48.0037, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7148, Val Loss: 48.7035, Recon: 0.7126, KL: 48.0023, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7161, Val Loss: 48.6969, Recon: 0.7125, KL: 48.0035, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7157, Val Loss: 48.6950, Recon: 0.7115, KL: 48.0042, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7143, Val Loss: 48.6992, Recon: 0.7112, KL: 48.0031, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7160, Val Loss: 48.7053, Recon: 0.7120, KL: 48.0039, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7144, Val Loss: 48.6912, Recon: 0.7125, KL: 48.0020, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7159, Val Loss: 48.7148, Recon: 0.7101, KL: 48.0058, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7230, Val Loss: 48.7124, Recon: 0.7153, KL: 48.0077, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7146, Val Loss: 48.7040, Recon: 0.7080, KL: 48.0066, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7067, Val Loss: 48.6870, Recon: 0.7040, KL: 48.0028, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7187, Val Loss: 48.7013, Recon: 0.7097, KL: 48.0090, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7111, Val Loss: 48.6916, Recon: 0.7071, KL: 48.0040, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7022, Val Loss: 48.6832, Recon: 0.6995, KL: 48.0027, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7164, Val Loss: 48.6932, Recon: 0.7107, KL: 48.0056, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6991, Val Loss: 48.6892, Recon: 0.6974, KL: 48.0017, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7032, Val Loss: 48.6867, Recon: 0.7007, KL: 48.0025, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7002, Val Loss: 48.6872, Recon: 0.6968, KL: 48.0033, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7032, Val Loss: 48.6940, Recon: 0.6976, KL: 48.0055, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7021, Val Loss: 48.6882, Recon: 0.7004, KL: 48.0016, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7067, Val Loss: 48.6967, Recon: 0.7014, KL: 48.0053, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6958, Val Loss: 48.6911, Recon: 0.6928, KL: 48.0030, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7066, Val Loss: 48.6872, Recon: 0.7008, KL: 48.0058, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7001, Val Loss: 48.6877, Recon: 0.6961, KL: 48.0040, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6951, Val Loss: 48.6805, Recon: 0.6925, KL: 48.0026, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7238, Val Loss: 48.7124, Recon: 0.7152, KL: 48.0086, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6972, Val Loss: 48.6806, Recon: 0.6949, KL: 48.0024, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6961, Val Loss: 48.6888, Recon: 0.6932, KL: 48.0028, KL_weight: 4.8000
Saved model 34 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_33.pt
Training bootstrap model 35/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9303, Val Loss: 0.6886, Recon: 0.9303, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7083, Val Loss: 4.6709, Recon: 0.8283, KL: 3.8800, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4773, Val Loss: 9.4264, Recon: 0.8126, KL: 8.6647, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2554, Val Loss: 14.2695, Recon: 0.8007, KL: 13.4548, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0392, Val Loss: 18.9994, Recon: 0.7835, KL: 18.2558, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8357, Val Loss: 23.7999, Recon: 0.7802, KL: 23.0554, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6203, Val Loss: 28.5871, Recon: 0.7701, KL: 27.8502, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4255, Val Loss: 33.3730, Recon: 0.7731, KL: 32.6524, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2185, Val Loss: 38.1847, Recon: 0.7691, KL: 37.4494, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9897, Val Loss: 42.9710, Recon: 0.7435, KL: 42.2461, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7958, Val Loss: 47.7884, Recon: 0.7475, KL: 47.0483, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7500, Val Loss: 48.7475, Recon: 0.7410, KL: 48.0089, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7521, Val Loss: 48.7164, Recon: 0.7438, KL: 48.0083, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7422, Val Loss: 48.7200, Recon: 0.7389, KL: 48.0034, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7265, Val Loss: 48.7169, Recon: 0.7207, KL: 48.0059, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7333, Val Loss: 48.7118, Recon: 0.7298, KL: 48.0035, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7300, Val Loss: 48.7104, Recon: 0.7251, KL: 48.0048, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7485, Val Loss: 48.7117, Recon: 0.7415, KL: 48.0070, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7379, Val Loss: 48.7153, Recon: 0.7294, KL: 48.0085, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7298, Val Loss: 48.7185, Recon: 0.7232, KL: 48.0066, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7273, Val Loss: 48.7098, Recon: 0.7233, KL: 48.0040, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7156, Val Loss: 48.7004, Recon: 0.7134, KL: 48.0021, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7210, Val Loss: 48.7123, Recon: 0.7166, KL: 48.0044, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7165, Val Loss: 48.7072, Recon: 0.7119, KL: 48.0046, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7171, Val Loss: 48.7006, Recon: 0.7137, KL: 48.0035, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7172, Val Loss: 48.6946, Recon: 0.7057, KL: 48.0115, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7208, Val Loss: 48.7121, Recon: 0.7166, KL: 48.0042, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7106, Val Loss: 48.7080, Recon: 0.7068, KL: 48.0038, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7128, Val Loss: 48.7011, Recon: 0.7087, KL: 48.0041, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7038, Val Loss: 48.7177, Recon: 0.7003, KL: 48.0035, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7026, Val Loss: 48.6931, Recon: 0.7005, KL: 48.0021, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7060, Val Loss: 48.6904, Recon: 0.7005, KL: 48.0055, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7036, Val Loss: 48.6907, Recon: 0.7003, KL: 48.0033, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6961, Val Loss: 48.6922, Recon: 0.6931, KL: 48.0030, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6997, Val Loss: 48.6964, Recon: 0.6971, KL: 48.0026, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7136, Val Loss: 48.6957, Recon: 0.7083, KL: 48.0053, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6942, Val Loss: 48.6825, Recon: 0.6921, KL: 48.0022, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7097, Val Loss: 48.7108, Recon: 0.7044, KL: 48.0054, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6996, Val Loss: 48.6881, Recon: 0.6956, KL: 48.0039, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6946, Val Loss: 48.6971, Recon: 0.6923, KL: 48.0023, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6914, Val Loss: 48.6773, Recon: 0.6897, KL: 48.0017, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7216, Val Loss: 48.7030, Recon: 0.6991, KL: 48.0225, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7363, Val Loss: 48.7096, Recon: 0.7177, KL: 48.0187, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6921, Val Loss: 48.6959, Recon: 0.6892, KL: 48.0029, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7422, Val Loss: 48.7074, Recon: 0.7231, KL: 48.0191, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7252, Val Loss: 48.7069, Recon: 0.7158, KL: 48.0094, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7458, Val Loss: 48.7029, Recon: 0.7189, KL: 48.0269, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7027, Val Loss: 48.6994, Recon: 0.6986, KL: 48.0041, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7021, Val Loss: 48.6847, Recon: 0.6986, KL: 48.0034, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6987, Val Loss: 48.6842, Recon: 0.6958, KL: 48.0029, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6901, Val Loss: 48.6830, Recon: 0.6879, KL: 48.0022, KL_weight: 4.8000
Saved model 35 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_34.pt
Training bootstrap model 36/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8932, Val Loss: 0.6846, Recon: 0.8932, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6862, Val Loss: 4.6399, Recon: 0.8113, KL: 3.8749, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4579, Val Loss: 9.4120, Recon: 0.7954, KL: 8.6625, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2208, Val Loss: 14.1968, Recon: 0.7683, KL: 13.4525, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0085, Val Loss: 18.9796, Recon: 0.7561, KL: 18.2524, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8258, Val Loss: 23.7968, Recon: 0.7592, KL: 23.0666, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6448, Val Loss: 28.6028, Recon: 0.7651, KL: 27.8797, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3755, Val Loss: 33.3706, Recon: 0.7315, KL: 32.6440, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1942, Val Loss: 38.1651, Recon: 0.7418, KL: 37.4524, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9738, Val Loss: 42.9670, Recon: 0.7288, KL: 42.2450, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7736, Val Loss: 47.7628, Recon: 0.7275, KL: 47.0461, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7308, Val Loss: 48.7174, Recon: 0.7224, KL: 48.0084, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7497, Val Loss: 48.7113, Recon: 0.7313, KL: 48.0184, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7319, Val Loss: 48.7224, Recon: 0.7291, KL: 48.0028, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7228, Val Loss: 48.7250, Recon: 0.7163, KL: 48.0066, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7583, Val Loss: 48.7396, Recon: 0.7418, KL: 48.0165, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7467, Val Loss: 48.7284, Recon: 0.7354, KL: 48.0113, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7234, Val Loss: 48.7217, Recon: 0.7180, KL: 48.0054, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7203, Val Loss: 48.7213, Recon: 0.7156, KL: 48.0047, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7146, Val Loss: 48.7199, Recon: 0.7110, KL: 48.0036, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7108, Val Loss: 48.7173, Recon: 0.7087, KL: 48.0022, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7165, Val Loss: 48.7179, Recon: 0.7131, KL: 48.0035, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7273, Val Loss: 48.7083, Recon: 0.7129, KL: 48.0144, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7127, Val Loss: 48.7152, Recon: 0.7076, KL: 48.0051, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7296, Val Loss: 48.7142, Recon: 0.7205, KL: 48.0091, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7029, Val Loss: 48.7048, Recon: 0.7005, KL: 48.0024, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7107, Val Loss: 48.7118, Recon: 0.7068, KL: 48.0039, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7780, Val Loss: 48.7049, Recon: 0.7505, KL: 48.0275, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7262, Val Loss: 48.7149, Recon: 0.7102, KL: 48.0160, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7100, Val Loss: 48.7007, Recon: 0.7025, KL: 48.0075, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7053, Val Loss: 48.6936, Recon: 0.7006, KL: 48.0047, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7024, Val Loss: 48.6961, Recon: 0.6990, KL: 48.0034, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7022, Val Loss: 48.7041, Recon: 0.7000, KL: 48.0022, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6984, Val Loss: 48.6926, Recon: 0.6958, KL: 48.0026, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6968, Val Loss: 48.6908, Recon: 0.6943, KL: 48.0025, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7037, Val Loss: 48.6996, Recon: 0.6995, KL: 48.0042, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6949, Val Loss: 48.6910, Recon: 0.6934, KL: 48.0015, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7225, Val Loss: 48.7044, Recon: 0.7186, KL: 48.0039, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6999, Val Loss: 48.6984, Recon: 0.6961, KL: 48.0038, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6959, Val Loss: 48.6806, Recon: 0.6925, KL: 48.0034, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6899, Val Loss: 48.6905, Recon: 0.6875, KL: 48.0024, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6881, Val Loss: 48.6860, Recon: 0.6862, KL: 48.0019, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6925, Val Loss: 48.7004, Recon: 0.6903, KL: 48.0022, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6895, Val Loss: 48.6978, Recon: 0.6883, KL: 48.0011, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6945, Val Loss: 48.6881, Recon: 0.6914, KL: 48.0031, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7265, Val Loss: 48.7218, Recon: 0.7218, KL: 48.0048, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6984, Val Loss: 48.6960, Recon: 0.6958, KL: 48.0026, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6928, Val Loss: 48.7038, Recon: 0.6893, KL: 48.0035, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6977, Val Loss: 48.7004, Recon: 0.6928, KL: 48.0049, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6852, Val Loss: 48.6882, Recon: 0.6835, KL: 48.0017, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6892, Val Loss: 48.6926, Recon: 0.6867, KL: 48.0025, KL_weight: 4.8000
Saved model 36 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_35.pt
Training bootstrap model 37/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9124, Val Loss: 0.6788, Recon: 0.9124, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7003, Val Loss: 4.6264, Recon: 0.8284, KL: 3.8719, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4630, Val Loss: 9.4340, Recon: 0.7985, KL: 8.6644, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2290, Val Loss: 14.1832, Recon: 0.7755, KL: 13.4535, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0214, Val Loss: 19.0006, Recon: 0.7698, KL: 18.2516, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8440, Val Loss: 23.8029, Recon: 0.7779, KL: 23.0661, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5967, Val Loss: 28.6043, Recon: 0.7479, KL: 27.8488, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3878, Val Loss: 33.3702, Recon: 0.7417, KL: 32.6461, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1982, Val Loss: 38.1637, Recon: 0.7516, KL: 37.4467, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9885, Val Loss: 42.9733, Recon: 0.7432, KL: 42.2454, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7805, Val Loss: 47.7541, Recon: 0.7356, KL: 47.0449, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7457, Val Loss: 48.7190, Recon: 0.7285, KL: 48.0172, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7457, Val Loss: 48.7244, Recon: 0.7376, KL: 48.0081, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7237, Val Loss: 48.7118, Recon: 0.7198, KL: 48.0039, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7216, Val Loss: 48.7027, Recon: 0.7184, KL: 48.0032, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7314, Val Loss: 48.7227, Recon: 0.7231, KL: 48.0083, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7328, Val Loss: 48.7070, Recon: 0.7166, KL: 48.0162, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7278, Val Loss: 48.7019, Recon: 0.7240, KL: 48.0038, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7276, Val Loss: 48.7140, Recon: 0.7241, KL: 48.0035, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7196, Val Loss: 48.7037, Recon: 0.7154, KL: 48.0042, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7201, Val Loss: 48.6885, Recon: 0.7129, KL: 48.0072, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7079, Val Loss: 48.6944, Recon: 0.7058, KL: 48.0021, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7148, Val Loss: 48.7123, Recon: 0.7098, KL: 48.0051, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7146, Val Loss: 48.6907, Recon: 0.7114, KL: 48.0032, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7349, Val Loss: 48.7339, Recon: 0.7196, KL: 48.0153, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7176, Val Loss: 48.7042, Recon: 0.7121, KL: 48.0055, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7220, Val Loss: 48.7003, Recon: 0.7115, KL: 48.0105, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7075, Val Loss: 48.7002, Recon: 0.7041, KL: 48.0033, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7019, Val Loss: 48.6915, Recon: 0.7001, KL: 48.0018, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7063, Val Loss: 48.6897, Recon: 0.7030, KL: 48.0033, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7062, Val Loss: 48.6971, Recon: 0.7032, KL: 48.0030, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6985, Val Loss: 48.6910, Recon: 0.6958, KL: 48.0026, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7095, Val Loss: 48.6928, Recon: 0.7062, KL: 48.0032, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7029, Val Loss: 48.6947, Recon: 0.6990, KL: 48.0039, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7180, Val Loss: 48.7035, Recon: 0.7101, KL: 48.0079, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6957, Val Loss: 48.6819, Recon: 0.6936, KL: 48.0021, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6975, Val Loss: 48.6795, Recon: 0.6948, KL: 48.0027, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7307, Val Loss: 48.6936, Recon: 0.7157, KL: 48.0150, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7003, Val Loss: 48.6956, Recon: 0.6976, KL: 48.0027, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7036, Val Loss: 48.6974, Recon: 0.6893, KL: 48.0143, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7018, Val Loss: 48.6877, Recon: 0.6988, KL: 48.0029, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6970, Val Loss: 48.6807, Recon: 0.6930, KL: 48.0040, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7068, Val Loss: 48.6947, Recon: 0.7036, KL: 48.0032, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6982, Val Loss: 48.6830, Recon: 0.6951, KL: 48.0031, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7049, Val Loss: 48.7001, Recon: 0.7004, KL: 48.0044, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7057, Val Loss: 48.6977, Recon: 0.7042, KL: 48.0015, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6974, Val Loss: 48.6872, Recon: 0.6926, KL: 48.0048, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7033, Val Loss: 48.6852, Recon: 0.6992, KL: 48.0041, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6970, Val Loss: 48.6819, Recon: 0.6946, KL: 48.0025, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6980, Val Loss: 48.6841, Recon: 0.6950, KL: 48.0031, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6976, Val Loss: 48.6786, Recon: 0.6956, KL: 48.0021, KL_weight: 4.8000
Saved model 37 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_36.pt
Training bootstrap model 38/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9184, Val Loss: 0.6897, Recon: 0.9184, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7083, Val Loss: 4.6260, Recon: 0.8289, KL: 3.8794, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4817, Val Loss: 9.4191, Recon: 0.8121, KL: 8.6696, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2292, Val Loss: 14.1935, Recon: 0.7766, KL: 13.4526, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0207, Val Loss: 18.9989, Recon: 0.7657, KL: 18.2551, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8098, Val Loss: 23.7918, Recon: 0.7589, KL: 23.0509, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5960, Val Loss: 28.5705, Recon: 0.7491, KL: 27.8469, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3931, Val Loss: 33.3764, Recon: 0.7436, KL: 32.6495, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1918, Val Loss: 38.1700, Recon: 0.7434, KL: 37.4484, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9919, Val Loss: 42.9627, Recon: 0.7475, KL: 42.2444, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8089, Val Loss: 47.7975, Recon: 0.7504, KL: 47.0584, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7367, Val Loss: 48.7113, Recon: 0.7294, KL: 48.0073, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7420, Val Loss: 48.7168, Recon: 0.7333, KL: 48.0087, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7359, Val Loss: 48.7014, Recon: 0.7292, KL: 48.0067, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7248, Val Loss: 48.7051, Recon: 0.7199, KL: 48.0048, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7261, Val Loss: 48.7125, Recon: 0.7202, KL: 48.0059, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7302, Val Loss: 48.7103, Recon: 0.7231, KL: 48.0071, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7147, Val Loss: 48.6995, Recon: 0.7122, KL: 48.0024, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7348, Val Loss: 48.7101, Recon: 0.7278, KL: 48.0069, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7130, Val Loss: 48.6985, Recon: 0.7101, KL: 48.0029, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7181, Val Loss: 48.7031, Recon: 0.7092, KL: 48.0089, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7172, Val Loss: 48.7011, Recon: 0.7127, KL: 48.0045, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7136, Val Loss: 48.7102, Recon: 0.7070, KL: 48.0066, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7159, Val Loss: 48.6896, Recon: 0.7128, KL: 48.0031, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7114, Val Loss: 48.6840, Recon: 0.7070, KL: 48.0044, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7126, Val Loss: 48.7073, Recon: 0.7082, KL: 48.0044, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7169, Val Loss: 48.6829, Recon: 0.7100, KL: 48.0070, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7184, Val Loss: 48.6983, Recon: 0.7106, KL: 48.0078, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7230, Val Loss: 48.6985, Recon: 0.7162, KL: 48.0068, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7095, Val Loss: 48.6851, Recon: 0.7057, KL: 48.0038, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7010, Val Loss: 48.6813, Recon: 0.6979, KL: 48.0031, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7017, Val Loss: 48.6932, Recon: 0.6990, KL: 48.0027, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6976, Val Loss: 48.6892, Recon: 0.6960, KL: 48.0016, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6953, Val Loss: 48.7001, Recon: 0.6932, KL: 48.0022, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6986, Val Loss: 48.6916, Recon: 0.6973, KL: 48.0013, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6960, Val Loss: 48.6965, Recon: 0.6934, KL: 48.0026, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7125, Val Loss: 48.6958, Recon: 0.6990, KL: 48.0135, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7082, Val Loss: 48.6892, Recon: 0.7025, KL: 48.0057, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6998, Val Loss: 48.7189, Recon: 0.6950, KL: 48.0048, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6953, Val Loss: 48.6771, Recon: 0.6924, KL: 48.0029, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6962, Val Loss: 48.6910, Recon: 0.6919, KL: 48.0043, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6913, Val Loss: 48.6877, Recon: 0.6896, KL: 48.0017, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7313, Val Loss: 48.6960, Recon: 0.7116, KL: 48.0197, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7058, Val Loss: 48.6966, Recon: 0.7006, KL: 48.0052, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6912, Val Loss: 48.6801, Recon: 0.6892, KL: 48.0019, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6942, Val Loss: 48.6841, Recon: 0.6917, KL: 48.0025, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7035, Val Loss: 48.6841, Recon: 0.6931, KL: 48.0104, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6954, Val Loss: 48.6814, Recon: 0.6928, KL: 48.0025, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6888, Val Loss: 48.6784, Recon: 0.6850, KL: 48.0038, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6908, Val Loss: 48.6790, Recon: 0.6875, KL: 48.0033, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6914, Val Loss: 48.6892, Recon: 0.6897, KL: 48.0017, KL_weight: 4.8000
Saved model 38 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_37.pt
Training bootstrap model 39/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9152, Val Loss: 0.6967, Recon: 0.9152, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7010, Val Loss: 4.6383, Recon: 0.8186, KL: 3.8824, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4621, Val Loss: 9.4345, Recon: 0.7975, KL: 8.6646, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2477, Val Loss: 14.2067, Recon: 0.7825, KL: 13.4653, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0215, Val Loss: 18.9971, Recon: 0.7668, KL: 18.2547, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8189, Val Loss: 23.7804, Recon: 0.7647, KL: 23.0542, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6009, Val Loss: 28.5938, Recon: 0.7499, KL: 27.8510, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3935, Val Loss: 33.3958, Recon: 0.7428, KL: 32.6507, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1841, Val Loss: 38.1516, Recon: 0.7391, KL: 37.4450, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9964, Val Loss: 42.9737, Recon: 0.7466, KL: 42.2497, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7845, Val Loss: 47.7627, Recon: 0.7362, KL: 47.0483, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7500, Val Loss: 48.7415, Recon: 0.7418, KL: 48.0082, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7346, Val Loss: 48.7273, Recon: 0.7280, KL: 48.0066, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7312, Val Loss: 48.7157, Recon: 0.7261, KL: 48.0051, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7339, Val Loss: 48.7178, Recon: 0.7283, KL: 48.0056, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7310, Val Loss: 48.7221, Recon: 0.7272, KL: 48.0038, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7388, Val Loss: 48.7201, Recon: 0.7334, KL: 48.0054, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7286, Val Loss: 48.7379, Recon: 0.7219, KL: 48.0067, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7148, Val Loss: 48.7171, Recon: 0.7116, KL: 48.0032, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7114, Val Loss: 48.7104, Recon: 0.7078, KL: 48.0036, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7281, Val Loss: 48.7377, Recon: 0.7183, KL: 48.0099, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7313, Val Loss: 48.7063, Recon: 0.7228, KL: 48.0086, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7498, Val Loss: 48.7101, Recon: 0.7227, KL: 48.0272, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7175, Val Loss: 48.7054, Recon: 0.7113, KL: 48.0061, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7216, Val Loss: 48.7068, Recon: 0.7165, KL: 48.0051, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7126, Val Loss: 48.7194, Recon: 0.7086, KL: 48.0040, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7083, Val Loss: 48.7081, Recon: 0.7061, KL: 48.0023, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7182, Val Loss: 48.6969, Recon: 0.7168, KL: 48.0014, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7053, Val Loss: 48.6882, Recon: 0.7028, KL: 48.0024, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7044, Val Loss: 48.7065, Recon: 0.6999, KL: 48.0045, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7998, Val Loss: 48.7018, Recon: 0.7422, KL: 48.0576, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7150, Val Loss: 48.7046, Recon: 0.7125, KL: 48.0025, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7091, Val Loss: 48.6869, Recon: 0.7063, KL: 48.0028, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7103, Val Loss: 48.6985, Recon: 0.7066, KL: 48.0037, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7030, Val Loss: 48.6843, Recon: 0.6998, KL: 48.0032, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7056, Val Loss: 48.6842, Recon: 0.7022, KL: 48.0034, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7110, Val Loss: 48.6962, Recon: 0.7050, KL: 48.0060, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7002, Val Loss: 48.6819, Recon: 0.6970, KL: 48.0032, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7067, Val Loss: 48.7013, Recon: 0.7011, KL: 48.0056, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6860, Val Loss: 48.6909, Recon: 0.6849, KL: 48.0011, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6933, Val Loss: 48.6792, Recon: 0.6903, KL: 48.0030, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6906, Val Loss: 48.6754, Recon: 0.6891, KL: 48.0015, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6885, Val Loss: 48.6856, Recon: 0.6856, KL: 48.0029, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6869, Val Loss: 48.6768, Recon: 0.6837, KL: 48.0032, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6868, Val Loss: 48.6765, Recon: 0.6831, KL: 48.0037, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6951, Val Loss: 48.6929, Recon: 0.6913, KL: 48.0038, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6897, Val Loss: 48.6920, Recon: 0.6875, KL: 48.0022, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6836, Val Loss: 48.6856, Recon: 0.6809, KL: 48.0028, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6759, Val Loss: 48.6749, Recon: 0.6734, KL: 48.0025, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7067, Val Loss: 48.7088, Recon: 0.6779, KL: 48.0288, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6923, Val Loss: 48.6808, Recon: 0.6888, KL: 48.0035, KL_weight: 4.8000
Saved model 39 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_38.pt
Training bootstrap model 40/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9016, Val Loss: 0.6787, Recon: 0.9016, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6931, Val Loss: 4.6341, Recon: 0.8196, KL: 3.8735, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4557, Val Loss: 9.4207, Recon: 0.7985, KL: 8.6573, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2436, Val Loss: 14.1774, Recon: 0.7888, KL: 13.4547, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0548, Val Loss: 19.0080, Recon: 0.7902, KL: 18.2645, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8215, Val Loss: 23.7792, Recon: 0.7655, KL: 23.0559, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5940, Val Loss: 28.5779, Recon: 0.7380, KL: 27.8560, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3975, Val Loss: 33.3813, Recon: 0.7483, KL: 32.6492, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1854, Val Loss: 38.1746, Recon: 0.7381, KL: 37.4473, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9853, Val Loss: 42.9832, Recon: 0.7371, KL: 42.2482, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8802, Val Loss: 47.7866, Recon: 0.7533, KL: 47.1269, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7410, Val Loss: 48.7142, Recon: 0.7353, KL: 48.0057, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7385, Val Loss: 48.7220, Recon: 0.7329, KL: 48.0056, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7458, Val Loss: 48.7171, Recon: 0.7379, KL: 48.0079, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7252, Val Loss: 48.7214, Recon: 0.7230, KL: 48.0022, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7311, Val Loss: 48.7127, Recon: 0.7259, KL: 48.0052, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7375, Val Loss: 48.7149, Recon: 0.7293, KL: 48.0082, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7342, Val Loss: 48.7371, Recon: 0.7296, KL: 48.0046, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7455, Val Loss: 48.7350, Recon: 0.7351, KL: 48.0104, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7256, Val Loss: 48.7139, Recon: 0.7216, KL: 48.0040, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7220, Val Loss: 48.7135, Recon: 0.7173, KL: 48.0047, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7207, Val Loss: 48.7100, Recon: 0.7170, KL: 48.0037, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7205, Val Loss: 48.7151, Recon: 0.7159, KL: 48.0045, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7186, Val Loss: 48.7197, Recon: 0.7132, KL: 48.0054, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7150, Val Loss: 48.6959, Recon: 0.7119, KL: 48.0031, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7120, Val Loss: 48.7036, Recon: 0.7082, KL: 48.0038, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7101, Val Loss: 48.7167, Recon: 0.7076, KL: 48.0024, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7139, Val Loss: 48.7413, Recon: 0.7110, KL: 48.0029, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7334, Val Loss: 48.7264, Recon: 0.7164, KL: 48.0169, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7159, Val Loss: 48.7025, Recon: 0.7105, KL: 48.0054, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7161, Val Loss: 48.6994, Recon: 0.7127, KL: 48.0034, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7115, Val Loss: 48.6953, Recon: 0.7090, KL: 48.0025, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7134, Val Loss: 48.7151, Recon: 0.7082, KL: 48.0052, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7060, Val Loss: 48.7022, Recon: 0.7037, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7914, Val Loss: 48.7500, Recon: 0.7465, KL: 48.0449, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7496, Val Loss: 48.7187, Recon: 0.7391, KL: 48.0105, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7174, Val Loss: 48.7088, Recon: 0.7140, KL: 48.0034, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7199, Val Loss: 48.7190, Recon: 0.7144, KL: 48.0055, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7096, Val Loss: 48.6929, Recon: 0.7064, KL: 48.0033, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7078, Val Loss: 48.6880, Recon: 0.7036, KL: 48.0042, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7274, Val Loss: 48.7109, Recon: 0.7002, KL: 48.0272, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7196, Val Loss: 48.7001, Recon: 0.7146, KL: 48.0051, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7110, Val Loss: 48.6931, Recon: 0.7086, KL: 48.0024, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7110, Val Loss: 48.7067, Recon: 0.7050, KL: 48.0061, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6966, Val Loss: 48.6851, Recon: 0.6937, KL: 48.0029, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7380, Val Loss: 48.7064, Recon: 0.7250, KL: 48.0131, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7098, Val Loss: 48.6917, Recon: 0.7012, KL: 48.0086, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7042, Val Loss: 48.6947, Recon: 0.7014, KL: 48.0028, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6987, Val Loss: 48.6856, Recon: 0.6937, KL: 48.0050, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6972, Val Loss: 48.6827, Recon: 0.6957, KL: 48.0016, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6926, Val Loss: 48.6838, Recon: 0.6914, KL: 48.0012, KL_weight: 4.8000
Saved model 40 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_39.pt
Training bootstrap model 41/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8903, Val Loss: 0.6769, Recon: 0.8903, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6707, Val Loss: 4.6112, Recon: 0.8022, KL: 3.8685, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4650, Val Loss: 9.4270, Recon: 0.7993, KL: 8.6657, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2316, Val Loss: 14.2447, Recon: 0.7700, KL: 13.4616, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0314, Val Loss: 18.9977, Recon: 0.7692, KL: 18.2622, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7984, Val Loss: 23.7929, Recon: 0.7483, KL: 23.0501, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6007, Val Loss: 28.5846, Recon: 0.7494, KL: 27.8513, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3887, Val Loss: 33.3822, Recon: 0.7438, KL: 32.6449, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1940, Val Loss: 38.1807, Recon: 0.7403, KL: 37.4537, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9991, Val Loss: 42.9807, Recon: 0.7444, KL: 42.2547, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7874, Val Loss: 47.7550, Recon: 0.7413, KL: 47.0461, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7286, Val Loss: 48.7199, Recon: 0.7257, KL: 48.0029, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7332, Val Loss: 48.7125, Recon: 0.7247, KL: 48.0085, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7437, Val Loss: 48.7271, Recon: 0.7356, KL: 48.0080, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7313, Val Loss: 48.7313, Recon: 0.7193, KL: 48.0120, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7147, Val Loss: 48.7025, Recon: 0.7107, KL: 48.0040, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7257, Val Loss: 48.7020, Recon: 0.7186, KL: 48.0071, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7293, Val Loss: 48.7205, Recon: 0.7183, KL: 48.0109, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7139, Val Loss: 48.7053, Recon: 0.7108, KL: 48.0031, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7292, Val Loss: 48.7089, Recon: 0.7228, KL: 48.0064, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7393, Val Loss: 48.7609, Recon: 0.7286, KL: 48.0106, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7101, Val Loss: 48.6994, Recon: 0.7078, KL: 48.0023, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7647, Val Loss: 48.8062, Recon: 0.7507, KL: 48.0140, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7358, Val Loss: 48.7048, Recon: 0.7230, KL: 48.0128, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7327, Val Loss: 48.7047, Recon: 0.7265, KL: 48.0062, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7083, Val Loss: 48.6948, Recon: 0.7048, KL: 48.0035, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7085, Val Loss: 48.7038, Recon: 0.7037, KL: 48.0047, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7105, Val Loss: 48.6958, Recon: 0.7050, KL: 48.0054, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6986, Val Loss: 48.6878, Recon: 0.6959, KL: 48.0027, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7027, Val Loss: 48.7040, Recon: 0.6991, KL: 48.0036, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6966, Val Loss: 48.6973, Recon: 0.6931, KL: 48.0035, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7060, Val Loss: 48.6926, Recon: 0.6992, KL: 48.0068, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6966, Val Loss: 48.6810, Recon: 0.6946, KL: 48.0021, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6930, Val Loss: 48.6979, Recon: 0.6898, KL: 48.0031, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6885, Val Loss: 48.6938, Recon: 0.6866, KL: 48.0020, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6994, Val Loss: 48.6870, Recon: 0.6949, KL: 48.0045, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6985, Val Loss: 48.6919, Recon: 0.6968, KL: 48.0017, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6917, Val Loss: 48.6867, Recon: 0.6887, KL: 48.0030, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6877, Val Loss: 48.6827, Recon: 0.6863, KL: 48.0014, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7021, Val Loss: 48.6981, Recon: 0.6963, KL: 48.0058, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6984, Val Loss: 48.6925, Recon: 0.6938, KL: 48.0046, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6893, Val Loss: 48.6925, Recon: 0.6873, KL: 48.0021, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7232, Val Loss: 48.7047, Recon: 0.7159, KL: 48.0073, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7062, Val Loss: 48.6925, Recon: 0.7041, KL: 48.0021, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6979, Val Loss: 48.7008, Recon: 0.6938, KL: 48.0041, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7025, Val Loss: 48.6924, Recon: 0.6965, KL: 48.0061, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7024, Val Loss: 48.6829, Recon: 0.6987, KL: 48.0037, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7092, Val Loss: 48.7052, Recon: 0.6958, KL: 48.0134, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6979, Val Loss: 48.6827, Recon: 0.6923, KL: 48.0056, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6949, Val Loss: 48.6935, Recon: 0.6916, KL: 48.0033, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6982, Val Loss: 48.6905, Recon: 0.6889, KL: 48.0093, KL_weight: 4.8000
Saved model 41 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_40.pt
Training bootstrap model 42/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8921, Val Loss: 0.6817, Recon: 0.8921, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6964, Val Loss: 4.6646, Recon: 0.8245, KL: 3.8718, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4520, Val Loss: 9.4162, Recon: 0.7913, KL: 8.6606, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2238, Val Loss: 14.1984, Recon: 0.7713, KL: 13.4525, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0052, Val Loss: 18.9920, Recon: 0.7562, KL: 18.2490, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8098, Val Loss: 23.8034, Recon: 0.7578, KL: 23.0520, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.5981, Val Loss: 28.5820, Recon: 0.7498, KL: 27.8482, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4198, Val Loss: 33.3722, Recon: 0.7549, KL: 32.6649, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1908, Val Loss: 38.1770, Recon: 0.7444, KL: 37.4464, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9772, Val Loss: 42.9602, Recon: 0.7312, KL: 42.2460, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7873, Val Loss: 47.7568, Recon: 0.7406, KL: 47.0467, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7435, Val Loss: 48.7172, Recon: 0.7378, KL: 48.0058, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7309, Val Loss: 48.7161, Recon: 0.7239, KL: 48.0070, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7418, Val Loss: 48.7256, Recon: 0.7289, KL: 48.0129, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7267, Val Loss: 48.7178, Recon: 0.7202, KL: 48.0065, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7622, Val Loss: 48.7521, Recon: 0.7447, KL: 48.0175, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7239, Val Loss: 48.7062, Recon: 0.7194, KL: 48.0045, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7178, Val Loss: 48.7207, Recon: 0.7118, KL: 48.0060, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7230, Val Loss: 48.7175, Recon: 0.7195, KL: 48.0035, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7184, Val Loss: 48.7090, Recon: 0.7129, KL: 48.0054, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7175, Val Loss: 48.7034, Recon: 0.7137, KL: 48.0038, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7099, Val Loss: 48.6987, Recon: 0.7066, KL: 48.0032, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7237, Val Loss: 48.7243, Recon: 0.7195, KL: 48.0042, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7248, Val Loss: 48.7039, Recon: 0.7155, KL: 48.0093, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7103, Val Loss: 48.7008, Recon: 0.7045, KL: 48.0058, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7076, Val Loss: 48.7018, Recon: 0.7034, KL: 48.0042, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7075, Val Loss: 48.7030, Recon: 0.7036, KL: 48.0039, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7043, Val Loss: 48.7017, Recon: 0.6999, KL: 48.0045, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6973, Val Loss: 48.7088, Recon: 0.6954, KL: 48.0018, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7030, Val Loss: 48.6950, Recon: 0.6992, KL: 48.0038, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6978, Val Loss: 48.6985, Recon: 0.6956, KL: 48.0022, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6989, Val Loss: 48.6946, Recon: 0.6963, KL: 48.0026, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6991, Val Loss: 48.6956, Recon: 0.6964, KL: 48.0028, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7001, Val Loss: 48.6829, Recon: 0.6980, KL: 48.0021, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6917, Val Loss: 48.7096, Recon: 0.6873, KL: 48.0044, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7793, Val Loss: 48.7287, Recon: 0.7496, KL: 48.0297, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7222, Val Loss: 48.7113, Recon: 0.7172, KL: 48.0050, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7162, Val Loss: 48.7082, Recon: 0.7094, KL: 48.0068, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7060, Val Loss: 48.7148, Recon: 0.7016, KL: 48.0044, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6966, Val Loss: 48.7066, Recon: 0.6947, KL: 48.0019, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6932, Val Loss: 48.6820, Recon: 0.6916, KL: 48.0017, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6906, Val Loss: 48.6926, Recon: 0.6874, KL: 48.0031, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6878, Val Loss: 48.6946, Recon: 0.6859, KL: 48.0019, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7276, Val Loss: 48.7094, Recon: 0.7094, KL: 48.0182, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6967, Val Loss: 48.6878, Recon: 0.6922, KL: 48.0044, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6867, Val Loss: 48.6766, Recon: 0.6841, KL: 48.0026, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6916, Val Loss: 48.6903, Recon: 0.6881, KL: 48.0034, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7266, Val Loss: 48.7106, Recon: 0.7171, KL: 48.0095, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6906, Val Loss: 48.6853, Recon: 0.6862, KL: 48.0044, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6911, Val Loss: 48.6797, Recon: 0.6880, KL: 48.0031, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6936, Val Loss: 48.6957, Recon: 0.6838, KL: 48.0098, KL_weight: 4.8000
Saved model 42 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_41.pt
Training bootstrap model 43/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8901, Val Loss: 0.6856, Recon: 0.8901, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6738, Val Loss: 4.6286, Recon: 0.8024, KL: 3.8714, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4529, Val Loss: 9.4183, Recon: 0.7890, KL: 8.6639, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2313, Val Loss: 14.2061, Recon: 0.7757, KL: 13.4556, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0231, Val Loss: 18.9902, Recon: 0.7633, KL: 18.2598, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7932, Val Loss: 23.7847, Recon: 0.7451, KL: 23.0481, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6063, Val Loss: 28.5705, Recon: 0.7596, KL: 27.8467, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3769, Val Loss: 33.3622, Recon: 0.7312, KL: 32.6457, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2169, Val Loss: 38.1549, Recon: 0.7512, KL: 37.4657, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9813, Val Loss: 42.9624, Recon: 0.7358, KL: 42.2455, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7817, Val Loss: 47.7529, Recon: 0.7351, KL: 47.0466, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7438, Val Loss: 48.7157, Recon: 0.7286, KL: 48.0152, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7679, Val Loss: 48.7307, Recon: 0.7495, KL: 48.0185, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7420, Val Loss: 48.7233, Recon: 0.7304, KL: 48.0116, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7291, Val Loss: 48.7035, Recon: 0.7214, KL: 48.0078, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7184, Val Loss: 48.7072, Recon: 0.7154, KL: 48.0030, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7133, Val Loss: 48.6980, Recon: 0.7087, KL: 48.0047, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7232, Val Loss: 48.7038, Recon: 0.7191, KL: 48.0041, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7134, Val Loss: 48.6932, Recon: 0.7078, KL: 48.0056, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7101, Val Loss: 48.6956, Recon: 0.7038, KL: 48.0063, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7140, Val Loss: 48.6991, Recon: 0.7094, KL: 48.0046, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7054, Val Loss: 48.6924, Recon: 0.7014, KL: 48.0040, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7044, Val Loss: 48.6933, Recon: 0.7011, KL: 48.0033, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.6930, Val Loss: 48.6864, Recon: 0.6906, KL: 48.0024, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7192, Val Loss: 48.6980, Recon: 0.7110, KL: 48.0082, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.6991, Val Loss: 48.6966, Recon: 0.6954, KL: 48.0036, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7006, Val Loss: 48.6871, Recon: 0.6981, KL: 48.0025, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6924, Val Loss: 48.7106, Recon: 0.6901, KL: 48.0023, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6968, Val Loss: 48.6912, Recon: 0.6933, KL: 48.0035, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6916, Val Loss: 48.6925, Recon: 0.6898, KL: 48.0018, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6927, Val Loss: 48.6988, Recon: 0.6901, KL: 48.0026, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7063, Val Loss: 48.6989, Recon: 0.7026, KL: 48.0037, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7013, Val Loss: 48.7183, Recon: 0.6971, KL: 48.0041, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7358, Val Loss: 48.7163, Recon: 0.7120, KL: 48.0238, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7024, Val Loss: 48.6959, Recon: 0.6969, KL: 48.0055, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6923, Val Loss: 48.6837, Recon: 0.6895, KL: 48.0028, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7045, Val Loss: 48.6794, Recon: 0.6955, KL: 48.0091, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6951, Val Loss: 48.6825, Recon: 0.6921, KL: 48.0030, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6890, Val Loss: 48.6910, Recon: 0.6875, KL: 48.0016, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6888, Val Loss: 48.6838, Recon: 0.6851, KL: 48.0037, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6969, Val Loss: 48.6823, Recon: 0.6922, KL: 48.0048, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6927, Val Loss: 48.6864, Recon: 0.6893, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6866, Val Loss: 48.6881, Recon: 0.6839, KL: 48.0027, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6851, Val Loss: 48.6744, Recon: 0.6793, KL: 48.0058, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6855, Val Loss: 48.6914, Recon: 0.6837, KL: 48.0019, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6889, Val Loss: 48.6800, Recon: 0.6868, KL: 48.0020, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6866, Val Loss: 48.6859, Recon: 0.6836, KL: 48.0030, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7379, Val Loss: 48.7359, Recon: 0.7167, KL: 48.0212, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.7004, Val Loss: 48.6857, Recon: 0.6934, KL: 48.0070, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6992, Val Loss: 48.6995, Recon: 0.6906, KL: 48.0086, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6922, Val Loss: 48.6928, Recon: 0.6820, KL: 48.0102, KL_weight: 4.8000
Saved model 43 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_42.pt
Training bootstrap model 44/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9483, Val Loss: 0.7198, Recon: 0.9483, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7115, Val Loss: 4.6318, Recon: 0.8303, KL: 3.8812, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4582, Val Loss: 9.4280, Recon: 0.7939, KL: 8.6643, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2464, Val Loss: 14.2326, Recon: 0.7870, KL: 13.4593, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0302, Val Loss: 19.0118, Recon: 0.7799, KL: 18.2503, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8189, Val Loss: 23.7892, Recon: 0.7669, KL: 23.0520, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6279, Val Loss: 28.6097, Recon: 0.7710, KL: 27.8570, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3972, Val Loss: 33.3830, Recon: 0.7475, KL: 32.6497, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2117, Val Loss: 38.1774, Recon: 0.7572, KL: 37.4545, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9925, Val Loss: 42.9813, Recon: 0.7465, KL: 42.2459, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7984, Val Loss: 47.8249, Recon: 0.7480, KL: 47.0504, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7498, Val Loss: 48.7525, Recon: 0.7459, KL: 48.0038, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7615, Val Loss: 48.7348, Recon: 0.7538, KL: 48.0078, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7892, Val Loss: 48.8222, Recon: 0.7654, KL: 48.0237, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7363, Val Loss: 48.7457, Recon: 0.7294, KL: 48.0069, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7318, Val Loss: 48.7619, Recon: 0.7255, KL: 48.0063, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7331, Val Loss: 48.7397, Recon: 0.7287, KL: 48.0044, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7196, Val Loss: 48.7345, Recon: 0.7157, KL: 48.0039, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7238, Val Loss: 48.7399, Recon: 0.7169, KL: 48.0069, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7228, Val Loss: 48.7038, Recon: 0.7191, KL: 48.0037, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7378, Val Loss: 48.7334, Recon: 0.7181, KL: 48.0197, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7369, Val Loss: 48.7166, Recon: 0.7223, KL: 48.0146, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7247, Val Loss: 48.7123, Recon: 0.7184, KL: 48.0063, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7181, Val Loss: 48.7182, Recon: 0.7137, KL: 48.0044, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7200, Val Loss: 48.7139, Recon: 0.7127, KL: 48.0073, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7516, Val Loss: 48.7351, Recon: 0.7355, KL: 48.0161, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7140, Val Loss: 48.7062, Recon: 0.7076, KL: 48.0064, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7113, Val Loss: 48.7011, Recon: 0.7054, KL: 48.0060, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7093, Val Loss: 48.6984, Recon: 0.7035, KL: 48.0057, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7188, Val Loss: 48.7135, Recon: 0.7071, KL: 48.0117, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7186, Val Loss: 48.7122, Recon: 0.7044, KL: 48.0143, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7050, Val Loss: 48.7095, Recon: 0.7016, KL: 48.0034, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7067, Val Loss: 48.7004, Recon: 0.7020, KL: 48.0047, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7035, Val Loss: 48.7005, Recon: 0.6989, KL: 48.0046, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7283, Val Loss: 48.7117, Recon: 0.7227, KL: 48.0056, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7224, Val Loss: 48.7082, Recon: 0.7128, KL: 48.0095, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7201, Val Loss: 48.6973, Recon: 0.7110, KL: 48.0090, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7055, Val Loss: 48.6962, Recon: 0.6991, KL: 48.0065, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7034, Val Loss: 48.6958, Recon: 0.7000, KL: 48.0034, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7027, Val Loss: 48.6911, Recon: 0.6996, KL: 48.0031, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6985, Val Loss: 48.6995, Recon: 0.6960, KL: 48.0025, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7015, Val Loss: 48.7032, Recon: 0.6981, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6980, Val Loss: 48.7041, Recon: 0.6931, KL: 48.0048, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6927, Val Loss: 48.7044, Recon: 0.6907, KL: 48.0020, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7022, Val Loss: 48.6902, Recon: 0.6934, KL: 48.0089, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6934, Val Loss: 48.6888, Recon: 0.6906, KL: 48.0028, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6928, Val Loss: 48.6963, Recon: 0.6912, KL: 48.0016, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6910, Val Loss: 48.6861, Recon: 0.6898, KL: 48.0012, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6904, Val Loss: 48.6970, Recon: 0.6872, KL: 48.0032, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6906, Val Loss: 48.7019, Recon: 0.6890, KL: 48.0016, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6857, Val Loss: 48.6821, Recon: 0.6840, KL: 48.0017, KL_weight: 4.8000
Saved model 44 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_43.pt
Training bootstrap model 45/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9395, Val Loss: 0.6868, Recon: 0.9395, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7103, Val Loss: 4.6414, Recon: 0.8334, KL: 3.8769, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4644, Val Loss: 9.4306, Recon: 0.8033, KL: 8.6611, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2523, Val Loss: 14.1973, Recon: 0.7902, KL: 13.4621, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0368, Val Loss: 18.9831, Recon: 0.7835, KL: 18.2533, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8108, Val Loss: 23.7807, Recon: 0.7638, KL: 23.0470, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6107, Val Loss: 28.5689, Recon: 0.7611, KL: 27.8496, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4073, Val Loss: 33.3665, Recon: 0.7580, KL: 32.6493, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2049, Val Loss: 38.1771, Recon: 0.7485, KL: 37.4564, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0043, Val Loss: 42.9714, Recon: 0.7564, KL: 42.2480, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7970, Val Loss: 47.7663, Recon: 0.7494, KL: 47.0476, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7530, Val Loss: 48.7196, Recon: 0.7477, KL: 48.0053, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7361, Val Loss: 48.7158, Recon: 0.7328, KL: 48.0033, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7478, Val Loss: 48.7216, Recon: 0.7395, KL: 48.0083, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7398, Val Loss: 48.7329, Recon: 0.7336, KL: 48.0062, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7306, Val Loss: 48.7181, Recon: 0.7266, KL: 48.0040, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7287, Val Loss: 48.7040, Recon: 0.7250, KL: 48.0037, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7239, Val Loss: 48.7159, Recon: 0.7189, KL: 48.0050, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7921, Val Loss: 48.7332, Recon: 0.7541, KL: 48.0380, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7305, Val Loss: 48.7143, Recon: 0.7261, KL: 48.0043, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7320, Val Loss: 48.7133, Recon: 0.7272, KL: 48.0049, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7321, Val Loss: 48.7390, Recon: 0.7223, KL: 48.0098, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7370, Val Loss: 48.7112, Recon: 0.7286, KL: 48.0084, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7216, Val Loss: 48.6991, Recon: 0.7180, KL: 48.0036, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7248, Val Loss: 48.6990, Recon: 0.7210, KL: 48.0038, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7108, Val Loss: 48.7055, Recon: 0.7085, KL: 48.0023, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7195, Val Loss: 48.7169, Recon: 0.7145, KL: 48.0050, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7137, Val Loss: 48.7101, Recon: 0.7102, KL: 48.0035, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7099, Val Loss: 48.6908, Recon: 0.7071, KL: 48.0027, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7219, Val Loss: 48.6989, Recon: 0.7139, KL: 48.0080, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7131, Val Loss: 48.7051, Recon: 0.7064, KL: 48.0067, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7097, Val Loss: 48.7021, Recon: 0.7051, KL: 48.0046, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7069, Val Loss: 48.6850, Recon: 0.7033, KL: 48.0035, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7043, Val Loss: 48.6878, Recon: 0.7022, KL: 48.0021, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7065, Val Loss: 48.6960, Recon: 0.7021, KL: 48.0045, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7106, Val Loss: 48.6992, Recon: 0.6978, KL: 48.0127, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7091, Val Loss: 48.6970, Recon: 0.7059, KL: 48.0032, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7037, Val Loss: 48.6920, Recon: 0.6995, KL: 48.0041, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6987, Val Loss: 48.6862, Recon: 0.6961, KL: 48.0026, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6988, Val Loss: 48.6853, Recon: 0.6957, KL: 48.0031, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6939, Val Loss: 48.6879, Recon: 0.6900, KL: 48.0039, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6938, Val Loss: 48.6851, Recon: 0.6922, KL: 48.0016, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6971, Val Loss: 48.6817, Recon: 0.6931, KL: 48.0040, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.7073, Val Loss: 48.6806, Recon: 0.7006, KL: 48.0067, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6917, Val Loss: 48.6849, Recon: 0.6899, KL: 48.0018, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6858, Val Loss: 48.6801, Recon: 0.6833, KL: 48.0025, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6918, Val Loss: 48.6737, Recon: 0.6900, KL: 48.0019, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6898, Val Loss: 48.6728, Recon: 0.6872, KL: 48.0026, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6945, Val Loss: 48.6792, Recon: 0.6904, KL: 48.0041, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6916, Val Loss: 48.6757, Recon: 0.6887, KL: 48.0030, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7120, Val Loss: 48.6921, Recon: 0.7031, KL: 48.0090, KL_weight: 4.8000
Saved model 45 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_44.pt
Training bootstrap model 46/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8949, Val Loss: 0.6840, Recon: 0.8949, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7894, Val Loss: 4.6780, Recon: 0.8677, KL: 3.9217, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4875, Val Loss: 9.4656, Recon: 0.8187, KL: 8.6689, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2664, Val Loss: 14.2161, Recon: 0.8067, KL: 13.4597, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0546, Val Loss: 19.0263, Recon: 0.7919, KL: 18.2627, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8300, Val Loss: 23.8041, Recon: 0.7788, KL: 23.0512, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6137, Val Loss: 28.6112, Recon: 0.7618, KL: 27.8519, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4241, Val Loss: 33.3916, Recon: 0.7715, KL: 32.6526, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1926, Val Loss: 38.1832, Recon: 0.7474, KL: 37.4452, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9798, Val Loss: 42.9821, Recon: 0.7338, KL: 42.2460, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8054, Val Loss: 47.7807, Recon: 0.7588, KL: 47.0466, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7478, Val Loss: 48.7280, Recon: 0.7419, KL: 48.0058, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7528, Val Loss: 48.7377, Recon: 0.7466, KL: 48.0061, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7389, Val Loss: 48.7283, Recon: 0.7307, KL: 48.0082, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7609, Val Loss: 48.7206, Recon: 0.7513, KL: 48.0095, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7552, Val Loss: 48.7405, Recon: 0.7451, KL: 48.0101, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7343, Val Loss: 48.7248, Recon: 0.7287, KL: 48.0056, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7263, Val Loss: 48.7230, Recon: 0.7208, KL: 48.0055, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7402, Val Loss: 48.7439, Recon: 0.7315, KL: 48.0087, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7327, Val Loss: 48.7648, Recon: 0.7240, KL: 48.0087, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7264, Val Loss: 48.7128, Recon: 0.7226, KL: 48.0038, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7757, Val Loss: 48.7285, Recon: 0.7285, KL: 48.0472, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7574, Val Loss: 48.7357, Recon: 0.7395, KL: 48.0179, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7254, Val Loss: 48.7045, Recon: 0.7201, KL: 48.0053, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7204, Val Loss: 48.7143, Recon: 0.7126, KL: 48.0079, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7113, Val Loss: 48.7120, Recon: 0.7085, KL: 48.0028, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7179, Val Loss: 48.7036, Recon: 0.7155, KL: 48.0024, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7298, Val Loss: 48.7130, Recon: 0.7159, KL: 48.0140, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7430, Val Loss: 48.7352, Recon: 0.7253, KL: 48.0177, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7172, Val Loss: 48.7094, Recon: 0.7142, KL: 48.0030, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7209, Val Loss: 48.7053, Recon: 0.7139, KL: 48.0070, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7093, Val Loss: 48.6953, Recon: 0.7063, KL: 48.0030, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7038, Val Loss: 48.6931, Recon: 0.7003, KL: 48.0035, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7150, Val Loss: 48.7001, Recon: 0.7123, KL: 48.0026, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7024, Val Loss: 48.6988, Recon: 0.7005, KL: 48.0019, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7044, Val Loss: 48.6975, Recon: 0.7001, KL: 48.0043, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7213, Val Loss: 48.7000, Recon: 0.7083, KL: 48.0130, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7004, Val Loss: 48.6883, Recon: 0.6970, KL: 48.0034, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7029, Val Loss: 48.6941, Recon: 0.6941, KL: 48.0088, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7010, Val Loss: 48.7090, Recon: 0.6965, KL: 48.0045, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7061, Val Loss: 48.6944, Recon: 0.7000, KL: 48.0061, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6988, Val Loss: 48.6973, Recon: 0.6953, KL: 48.0035, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7084, Val Loss: 48.7000, Recon: 0.7047, KL: 48.0037, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6991, Val Loss: 48.6980, Recon: 0.6943, KL: 48.0048, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6951, Val Loss: 48.6981, Recon: 0.6932, KL: 48.0019, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6979, Val Loss: 48.6980, Recon: 0.6945, KL: 48.0034, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6977, Val Loss: 48.7012, Recon: 0.6952, KL: 48.0026, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.7191, Val Loss: 48.6970, Recon: 0.6943, KL: 48.0248, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6953, Val Loss: 48.6879, Recon: 0.6922, KL: 48.0032, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.7059, Val Loss: 48.6861, Recon: 0.7003, KL: 48.0056, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.7047, Val Loss: 48.6908, Recon: 0.7022, KL: 48.0025, KL_weight: 4.8000
Saved model 46 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_45.pt
Training bootstrap model 47/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9244, Val Loss: 0.6973, Recon: 0.9244, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.9451, Val Loss: 4.7763, Recon: 0.9546, KL: 3.9905, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.5627, Val Loss: 9.4984, Recon: 0.8691, KL: 8.6936, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.3021, Val Loss: 14.2655, Recon: 0.8301, KL: 13.4720, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0725, Val Loss: 19.0479, Recon: 0.8061, KL: 18.2664, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8365, Val Loss: 23.7974, Recon: 0.7856, KL: 23.0510, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6549, Val Loss: 28.6059, Recon: 0.7956, KL: 27.8594, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4308, Val Loss: 33.3928, Recon: 0.7798, KL: 32.6510, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2296, Val Loss: 38.2064, Recon: 0.7768, KL: 37.4528, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0206, Val Loss: 42.9888, Recon: 0.7658, KL: 42.2548, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.8068, Val Loss: 47.8019, Recon: 0.7587, KL: 47.0481, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7655, Val Loss: 48.7364, Recon: 0.7576, KL: 48.0079, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7525, Val Loss: 48.7441, Recon: 0.7434, KL: 48.0091, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7575, Val Loss: 48.7448, Recon: 0.7460, KL: 48.0115, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7580, Val Loss: 48.7449, Recon: 0.7471, KL: 48.0110, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7392, Val Loss: 48.7243, Recon: 0.7336, KL: 48.0056, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7461, Val Loss: 48.7226, Recon: 0.7391, KL: 48.0071, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7344, Val Loss: 48.7511, Recon: 0.7255, KL: 48.0089, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7552, Val Loss: 48.7224, Recon: 0.7342, KL: 48.0210, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7423, Val Loss: 48.7398, Recon: 0.7267, KL: 48.0156, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7408, Val Loss: 48.7317, Recon: 0.7333, KL: 48.0075, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7304, Val Loss: 48.7186, Recon: 0.7288, KL: 48.0016, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7280, Val Loss: 48.7199, Recon: 0.7240, KL: 48.0040, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7189, Val Loss: 48.7236, Recon: 0.7136, KL: 48.0053, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7332, Val Loss: 48.7130, Recon: 0.7253, KL: 48.0079, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7287, Val Loss: 48.7132, Recon: 0.7233, KL: 48.0054, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7211, Val Loss: 48.7081, Recon: 0.7174, KL: 48.0037, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7200, Val Loss: 48.7195, Recon: 0.7100, KL: 48.0100, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7164, Val Loss: 48.7171, Recon: 0.7131, KL: 48.0033, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7195, Val Loss: 48.7210, Recon: 0.7138, KL: 48.0057, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7061, Val Loss: 48.7146, Recon: 0.7032, KL: 48.0029, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7182, Val Loss: 48.6971, Recon: 0.7139, KL: 48.0043, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7066, Val Loss: 48.6954, Recon: 0.7027, KL: 48.0040, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7033, Val Loss: 48.7046, Recon: 0.6988, KL: 48.0045, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.7007, Val Loss: 48.7080, Recon: 0.6990, KL: 48.0017, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7299, Val Loss: 48.7083, Recon: 0.7169, KL: 48.0130, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7057, Val Loss: 48.6976, Recon: 0.7008, KL: 48.0049, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7052, Val Loss: 48.6901, Recon: 0.6998, KL: 48.0054, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6950, Val Loss: 48.7015, Recon: 0.6931, KL: 48.0019, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7006, Val Loss: 48.6929, Recon: 0.6984, KL: 48.0022, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6949, Val Loss: 48.7166, Recon: 0.6932, KL: 48.0017, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.7145, Val Loss: 48.7081, Recon: 0.7070, KL: 48.0075, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.7067, Val Loss: 48.6950, Recon: 0.7005, KL: 48.0062, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6952, Val Loss: 48.6868, Recon: 0.6902, KL: 48.0050, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7067, Val Loss: 48.6952, Recon: 0.7024, KL: 48.0043, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7024, Val Loss: 48.6988, Recon: 0.6988, KL: 48.0036, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.7077, Val Loss: 48.7033, Recon: 0.6964, KL: 48.0113, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6953, Val Loss: 48.6959, Recon: 0.6933, KL: 48.0020, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6942, Val Loss: 48.7033, Recon: 0.6908, KL: 48.0034, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6961, Val Loss: 48.6943, Recon: 0.6938, KL: 48.0023, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6894, Val Loss: 48.6855, Recon: 0.6864, KL: 48.0030, KL_weight: 4.8000
Saved model 47 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_46.pt
Training bootstrap model 48/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.8955, Val Loss: 0.6870, Recon: 0.8955, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6919, Val Loss: 4.6482, Recon: 0.8140, KL: 3.8779, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4582, Val Loss: 9.4544, Recon: 0.7891, KL: 8.6691, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2323, Val Loss: 14.2071, Recon: 0.7724, KL: 13.4599, KL_weight: 1.3440
Epoch 20/250, Train Loss: 18.9985, Val Loss: 18.9818, Recon: 0.7497, KL: 18.2487, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7938, Val Loss: 23.7773, Recon: 0.7462, KL: 23.0476, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6184, Val Loss: 28.5904, Recon: 0.7536, KL: 27.8648, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.3934, Val Loss: 33.3777, Recon: 0.7437, KL: 32.6497, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1934, Val Loss: 38.1683, Recon: 0.7412, KL: 37.4523, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9988, Val Loss: 42.9718, Recon: 0.7440, KL: 42.2549, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7767, Val Loss: 47.7752, Recon: 0.7303, KL: 47.0464, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7309, Val Loss: 48.7287, Recon: 0.7249, KL: 48.0060, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7665, Val Loss: 48.7609, Recon: 0.7526, KL: 48.0139, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7609, Val Loss: 48.7335, Recon: 0.7327, KL: 48.0282, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7203, Val Loss: 48.7208, Recon: 0.7137, KL: 48.0066, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7230, Val Loss: 48.7259, Recon: 0.7201, KL: 48.0029, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7162, Val Loss: 48.7129, Recon: 0.7140, KL: 48.0022, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7386, Val Loss: 48.7143, Recon: 0.7258, KL: 48.0128, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7126, Val Loss: 48.7096, Recon: 0.7059, KL: 48.0067, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7169, Val Loss: 48.7019, Recon: 0.7126, KL: 48.0043, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7198, Val Loss: 48.6981, Recon: 0.7170, KL: 48.0028, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7141, Val Loss: 48.7032, Recon: 0.7102, KL: 48.0039, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.6997, Val Loss: 48.6946, Recon: 0.6978, KL: 48.0019, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7065, Val Loss: 48.7024, Recon: 0.6988, KL: 48.0077, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7043, Val Loss: 48.6975, Recon: 0.7008, KL: 48.0036, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.6965, Val Loss: 48.6940, Recon: 0.6940, KL: 48.0025, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7358, Val Loss: 48.7063, Recon: 0.7004, KL: 48.0354, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7088, Val Loss: 48.7025, Recon: 0.7050, KL: 48.0038, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7085, Val Loss: 48.7037, Recon: 0.7031, KL: 48.0054, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6938, Val Loss: 48.6901, Recon: 0.6918, KL: 48.0021, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.6922, Val Loss: 48.6845, Recon: 0.6900, KL: 48.0022, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6883, Val Loss: 48.6989, Recon: 0.6864, KL: 48.0019, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7001, Val Loss: 48.6969, Recon: 0.6941, KL: 48.0060, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6882, Val Loss: 48.6888, Recon: 0.6859, KL: 48.0023, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6933, Val Loss: 48.6907, Recon: 0.6911, KL: 48.0022, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6895, Val Loss: 48.6875, Recon: 0.6877, KL: 48.0018, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6971, Val Loss: 48.6961, Recon: 0.6894, KL: 48.0077, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6865, Val Loss: 48.6883, Recon: 0.6843, KL: 48.0022, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7066, Val Loss: 48.7010, Recon: 0.7001, KL: 48.0065, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6906, Val Loss: 48.6924, Recon: 0.6887, KL: 48.0020, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6964, Val Loss: 48.7011, Recon: 0.6913, KL: 48.0051, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6829, Val Loss: 48.6926, Recon: 0.6803, KL: 48.0026, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6973, Val Loss: 48.6913, Recon: 0.6863, KL: 48.0110, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6909, Val Loss: 48.6859, Recon: 0.6893, KL: 48.0016, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.7016, Val Loss: 48.7017, Recon: 0.6956, KL: 48.0059, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6845, Val Loss: 48.6858, Recon: 0.6833, KL: 48.0012, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6803, Val Loss: 48.6875, Recon: 0.6790, KL: 48.0014, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6880, Val Loss: 48.6927, Recon: 0.6865, KL: 48.0015, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6969, Val Loss: 48.6943, Recon: 0.6882, KL: 48.0087, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6858, Val Loss: 48.6797, Recon: 0.6831, KL: 48.0027, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6817, Val Loss: 48.6751, Recon: 0.6790, KL: 48.0027, KL_weight: 4.8000
Saved model 48 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_47.pt
Training bootstrap model 49/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9120, Val Loss: 0.6780, Recon: 0.9120, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.7102, Val Loss: 4.6397, Recon: 0.8346, KL: 3.8756, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4723, Val Loss: 9.4232, Recon: 0.8034, KL: 8.6689, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2515, Val Loss: 14.2022, Recon: 0.7936, KL: 13.4579, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0388, Val Loss: 18.9899, Recon: 0.7794, KL: 18.2594, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.8284, Val Loss: 23.7706, Recon: 0.7778, KL: 23.0507, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6383, Val Loss: 28.5858, Recon: 0.7826, KL: 27.8557, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4114, Val Loss: 33.3653, Recon: 0.7615, KL: 32.6499, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.2171, Val Loss: 38.1920, Recon: 0.7642, KL: 37.4530, KL_weight: 3.7440
Epoch 45/250, Train Loss: 43.0019, Val Loss: 42.9750, Recon: 0.7529, KL: 42.2490, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7928, Val Loss: 47.7545, Recon: 0.7440, KL: 47.0488, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7581, Val Loss: 48.7181, Recon: 0.7499, KL: 48.0082, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7402, Val Loss: 48.7172, Recon: 0.7378, KL: 48.0024, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7363, Val Loss: 48.7091, Recon: 0.7298, KL: 48.0065, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7200, Val Loss: 48.7075, Recon: 0.7172, KL: 48.0028, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7264, Val Loss: 48.7041, Recon: 0.7214, KL: 48.0050, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7226, Val Loss: 48.7015, Recon: 0.7192, KL: 48.0034, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7352, Val Loss: 48.7073, Recon: 0.7280, KL: 48.0073, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7332, Val Loss: 48.7253, Recon: 0.7259, KL: 48.0073, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7283, Val Loss: 48.7206, Recon: 0.7246, KL: 48.0037, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7300, Val Loss: 48.7052, Recon: 0.7237, KL: 48.0063, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7399, Val Loss: 48.7023, Recon: 0.7311, KL: 48.0088, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7200, Val Loss: 48.7031, Recon: 0.7149, KL: 48.0051, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7252, Val Loss: 48.7051, Recon: 0.7203, KL: 48.0049, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7184, Val Loss: 48.7003, Recon: 0.7107, KL: 48.0077, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7116, Val Loss: 48.6882, Recon: 0.7084, KL: 48.0032, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.7126, Val Loss: 48.6907, Recon: 0.7083, KL: 48.0043, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.7143, Val Loss: 48.6973, Recon: 0.7030, KL: 48.0113, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.7021, Val Loss: 48.6893, Recon: 0.6995, KL: 48.0026, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.7087, Val Loss: 48.6817, Recon: 0.7055, KL: 48.0032, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7018, Val Loss: 48.6881, Recon: 0.7009, KL: 48.0010, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.6973, Val Loss: 48.6866, Recon: 0.6948, KL: 48.0025, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.7078, Val Loss: 48.6903, Recon: 0.7015, KL: 48.0063, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.7044, Val Loss: 48.6945, Recon: 0.7018, KL: 48.0026, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6948, Val Loss: 48.6813, Recon: 0.6924, KL: 48.0024, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.7051, Val Loss: 48.6831, Recon: 0.7028, KL: 48.0023, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.7010, Val Loss: 48.6869, Recon: 0.6978, KL: 48.0032, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.7013, Val Loss: 48.6827, Recon: 0.6974, KL: 48.0039, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.7074, Val Loss: 48.6827, Recon: 0.7035, KL: 48.0039, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.7060, Val Loss: 48.6837, Recon: 0.6973, KL: 48.0087, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.7187, Val Loss: 48.6923, Recon: 0.7141, KL: 48.0046, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6990, Val Loss: 48.6855, Recon: 0.6956, KL: 48.0034, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6927, Val Loss: 48.6767, Recon: 0.6895, KL: 48.0031, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6986, Val Loss: 48.6940, Recon: 0.6914, KL: 48.0072, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6881, Val Loss: 48.6731, Recon: 0.6857, KL: 48.0024, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.7045, Val Loss: 48.6854, Recon: 0.6920, KL: 48.0126, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6948, Val Loss: 48.6814, Recon: 0.6890, KL: 48.0058, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6921, Val Loss: 48.6779, Recon: 0.6903, KL: 48.0018, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6953, Val Loss: 48.6873, Recon: 0.6909, KL: 48.0043, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6879, Val Loss: 48.6808, Recon: 0.6852, KL: 48.0027, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6963, Val Loss: 48.6800, Recon: 0.6881, KL: 48.0082, KL_weight: 4.8000
Saved model 49 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_48.pt
Training bootstrap model 50/50
Training data shape IM MODEL: torch.Size([1560, 1194])
Epoch 1/250, Train Loss: 0.9179, Val Loss: 0.7132, Recon: 0.9179, KL: 0.0000, KL_weight: 0.0000
Epoch 5/250, Train Loss: 4.6788, Val Loss: 4.6163, Recon: 0.8053, KL: 3.8735, KL_weight: 0.3840
Epoch 10/250, Train Loss: 9.4494, Val Loss: 9.4082, Recon: 0.7859, KL: 8.6636, KL_weight: 0.8640
Epoch 15/250, Train Loss: 14.2364, Val Loss: 14.2133, Recon: 0.7782, KL: 13.4583, KL_weight: 1.3440
Epoch 20/250, Train Loss: 19.0206, Val Loss: 19.0225, Recon: 0.7644, KL: 18.2562, KL_weight: 1.8240
Epoch 25/250, Train Loss: 23.7915, Val Loss: 23.7742, Recon: 0.7432, KL: 23.0483, KL_weight: 2.3040
Epoch 30/250, Train Loss: 28.6235, Val Loss: 28.6277, Recon: 0.7686, KL: 27.8549, KL_weight: 2.7840
Epoch 35/250, Train Loss: 33.4018, Val Loss: 33.3737, Recon: 0.7527, KL: 32.6491, KL_weight: 3.2640
Epoch 40/250, Train Loss: 38.1835, Val Loss: 38.1645, Recon: 0.7370, KL: 37.4465, KL_weight: 3.7440
Epoch 45/250, Train Loss: 42.9775, Val Loss: 42.9576, Recon: 0.7278, KL: 42.2497, KL_weight: 4.2240
Epoch 50/250, Train Loss: 47.7685, Val Loss: 47.7608, Recon: 0.7236, KL: 47.0449, KL_weight: 4.7040
Epoch 55/250, Train Loss: 48.7283, Val Loss: 48.7221, Recon: 0.7180, KL: 48.0104, KL_weight: 4.8000
Epoch 60/250, Train Loss: 48.7317, Val Loss: 48.7556, Recon: 0.7199, KL: 48.0118, KL_weight: 4.8000
Epoch 65/250, Train Loss: 48.7140, Val Loss: 48.7149, Recon: 0.7112, KL: 48.0028, KL_weight: 4.8000
Epoch 70/250, Train Loss: 48.7169, Val Loss: 48.7211, Recon: 0.7142, KL: 48.0028, KL_weight: 4.8000
Epoch 75/250, Train Loss: 48.7192, Val Loss: 48.7088, Recon: 0.7142, KL: 48.0050, KL_weight: 4.8000
Epoch 80/250, Train Loss: 48.7171, Val Loss: 48.7236, Recon: 0.7126, KL: 48.0045, KL_weight: 4.8000
Epoch 85/250, Train Loss: 48.7188, Val Loss: 48.7179, Recon: 0.7114, KL: 48.0074, KL_weight: 4.8000
Epoch 90/250, Train Loss: 48.7422, Val Loss: 48.7225, Recon: 0.7315, KL: 48.0107, KL_weight: 4.8000
Epoch 95/250, Train Loss: 48.7130, Val Loss: 48.6986, Recon: 0.7101, KL: 48.0029, KL_weight: 4.8000
Epoch 100/250, Train Loss: 48.7486, Val Loss: 48.7429, Recon: 0.7312, KL: 48.0174, KL_weight: 4.8000
Epoch 105/250, Train Loss: 48.7283, Val Loss: 48.7100, Recon: 0.7219, KL: 48.0064, KL_weight: 4.8000
Epoch 110/250, Train Loss: 48.7115, Val Loss: 48.6999, Recon: 0.7085, KL: 48.0030, KL_weight: 4.8000
Epoch 115/250, Train Loss: 48.7031, Val Loss: 48.6895, Recon: 0.6995, KL: 48.0036, KL_weight: 4.8000
Epoch 120/250, Train Loss: 48.7047, Val Loss: 48.6979, Recon: 0.7015, KL: 48.0032, KL_weight: 4.8000
Epoch 125/250, Train Loss: 48.7006, Val Loss: 48.6974, Recon: 0.6973, KL: 48.0033, KL_weight: 4.8000
Epoch 130/250, Train Loss: 48.6942, Val Loss: 48.6928, Recon: 0.6909, KL: 48.0032, KL_weight: 4.8000
Epoch 135/250, Train Loss: 48.6972, Val Loss: 48.6905, Recon: 0.6928, KL: 48.0044, KL_weight: 4.8000
Epoch 140/250, Train Loss: 48.6936, Val Loss: 48.6949, Recon: 0.6917, KL: 48.0019, KL_weight: 4.8000
Epoch 145/250, Train Loss: 48.6856, Val Loss: 48.6897, Recon: 0.6828, KL: 48.0028, KL_weight: 4.8000
Epoch 150/250, Train Loss: 48.7290, Val Loss: 48.7021, Recon: 0.7053, KL: 48.0237, KL_weight: 4.8000
Epoch 155/250, Train Loss: 48.7071, Val Loss: 48.6906, Recon: 0.7017, KL: 48.0053, KL_weight: 4.8000
Epoch 160/250, Train Loss: 48.6947, Val Loss: 48.7000, Recon: 0.6916, KL: 48.0031, KL_weight: 4.8000
Epoch 165/250, Train Loss: 48.6946, Val Loss: 48.6807, Recon: 0.6898, KL: 48.0048, KL_weight: 4.8000
Epoch 170/250, Train Loss: 48.6903, Val Loss: 48.6885, Recon: 0.6869, KL: 48.0033, KL_weight: 4.8000
Epoch 175/250, Train Loss: 48.6937, Val Loss: 48.6831, Recon: 0.6885, KL: 48.0052, KL_weight: 4.8000
Epoch 180/250, Train Loss: 48.6963, Val Loss: 48.6836, Recon: 0.6944, KL: 48.0019, KL_weight: 4.8000
Epoch 185/250, Train Loss: 48.6854, Val Loss: 48.6885, Recon: 0.6827, KL: 48.0027, KL_weight: 4.8000
Epoch 190/250, Train Loss: 48.6915, Val Loss: 48.6855, Recon: 0.6902, KL: 48.0012, KL_weight: 4.8000
Epoch 195/250, Train Loss: 48.6874, Val Loss: 48.6848, Recon: 0.6854, KL: 48.0020, KL_weight: 4.8000
Epoch 200/250, Train Loss: 48.6862, Val Loss: 48.6785, Recon: 0.6842, KL: 48.0020, KL_weight: 4.8000
Epoch 205/250, Train Loss: 48.6809, Val Loss: 48.6798, Recon: 0.6783, KL: 48.0026, KL_weight: 4.8000
Epoch 210/250, Train Loss: 48.6807, Val Loss: 48.6868, Recon: 0.6788, KL: 48.0019, KL_weight: 4.8000
Epoch 215/250, Train Loss: 48.6808, Val Loss: 48.6774, Recon: 0.6785, KL: 48.0023, KL_weight: 4.8000
Epoch 220/250, Train Loss: 48.6792, Val Loss: 48.6825, Recon: 0.6760, KL: 48.0032, KL_weight: 4.8000
Epoch 225/250, Train Loss: 48.6925, Val Loss: 48.6874, Recon: 0.6830, KL: 48.0095, KL_weight: 4.8000
Epoch 230/250, Train Loss: 48.6884, Val Loss: 48.6909, Recon: 0.6801, KL: 48.0083, KL_weight: 4.8000
Epoch 235/250, Train Loss: 48.6757, Val Loss: 48.6789, Recon: 0.6738, KL: 48.0019, KL_weight: 4.8000
Epoch 240/250, Train Loss: 48.6810, Val Loss: 48.6683, Recon: 0.6783, KL: 48.0027, KL_weight: 4.8000
Epoch 245/250, Train Loss: 48.6819, Val Loss: 48.6697, Recon: 0.6801, KL: 48.0018, KL_weight: 4.8000
Epoch 250/250, Train Loss: 48.6854, Val Loss: 48.6839, Recon: 0.6830, KL: 48.0024, KL_weight: 4.8000
Saved model 50 to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/models/bootstrap_model_49.pt

================================================================================
RUNNING POST-TRAINING DIAGNOSTICS
================================================================================

================================================================================
STARTING VAE DIAGNOSTICS
================================================================================

============================================================
Extracting latents for: HC_train
============================================================

============================================================
Extracting latents for: HC_valid
============================================================

============================================================
POSTERIOR COLLAPSE CHECK - HC_train
============================================================

Latent Space Statistics (across 1560 samples):
  Mean variance per dimension: 0.128818
  Std variance per dimension: 0.016033
  Min variance: 0.094276
  Max variance: 0.150548

  Collapsed dimensions (var < 0.01): 0/20

  Mean logvar per dimension: -1.2790
  Std logvar per dimension: 0.0900

  Mean L2 norm (distance to origin): 1.6221
  Std L2 norm: 0.6125

  Active dimensions (var > 0.1): 18/20

============================================================
POSTERIOR COLLAPSE CHECK - HC_valid
============================================================

Latent Space Statistics (across 390 samples):
  Mean variance per dimension: 0.119210
  Std variance per dimension: 0.015766
  Min variance: 0.085262
  Max variance: 0.142445

  Collapsed dimensions (var < 0.01): 0/20

  Mean logvar per dimension: -1.2967
  Std logvar per dimension: 0.0925

  Mean L2 norm (distance to origin): 1.5809
  Std L2 norm: 0.5987

  Active dimensions (var > 0.1): 18/20

============================================================
GROUP COMPARISON: HC_train vs HC_valid
============================================================

KL Divergence Statistics:
  HC    - Mean: 7.3492, Std: 1.2515, Median: 7.5852
  HC_valid - Mean: 7.3783, Std: 1.2006, Median: 7.6750
  Difference: 0.0291
  T-test: t=-0.4141, p=0.6788

Reconstruction Error Statistics:
  HC    - Mean: 0.681071, Std: 0.190525
  HC_valid - Mean: 0.674854, Std: 0.152468
  Difference: -0.006217
  T-test: t=0.5980, p=0.5499

Latent Space Variance:
  HC    - Mean variance: 0.128818
  HC_valid - Mean variance: 0.119210
  Ratio: 0.9254
    HC_valid has LESS variance  easier to fit to prior!

Distance to Origin (L2 norm):
  HC    - Mean: 1.6221, Std: 0.6125
  HC_valid - Mean: 1.5809, Std: 0.5987

============================================================
MAHALANOBIS DISTANCE (proper anomaly detection)
============================================================
  HC    - Mean Mahalanobis: 3.7256, Std: 1.8766
  HC_valid - Mean Mahalanobis: 3.6397, Std: 1.8343
  T-test: t=0.8120, p=0.4169
    PROBLEM: HC_valid is closer to HC distribution

 Diagnostic plots saved to: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/diagnostics_post_training/vae_diagnostics.png
 Diagnostic report saved to: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/diagnostics_post_training/diagnostic_report.txt

================================================================================
DIAGNOSTICS COMPLETE
================================================================================
Outputs saved to: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/diagnostics_post_training
 Post-training diagnostics completed
  Diagnostic plots saved to: /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303/diagnostics_post_training/

Key Findings:
  HC Train KL: 7.3492
  HC Valid KL: 7.3783
  Collapsed dimensions: 0/20
Successfully trained 50 bootstrap models
Normative modeling training completed successfully!
Results saved to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303
Normative modeling complete. Results saved to /net/data.isilon/ag-cherrmann/lduttenhoefer/project/VAE_model/analysis/TRAINING/norm_results_HC_Vgm_G_T_all_columnwise_20251101_1303
